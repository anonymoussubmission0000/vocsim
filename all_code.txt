
import logging
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, List
from pathlib import Path 

logger = logging.getLogger(__name__)

class Benchmark(ABC):
    """Abstract Base Class for all benchmark evaluations."""

    def __init__(self, **kwargs):
        self._initialize(**kwargs)

    def _initialize(self, **kwargs):
        """Optional initialization for subclasses to process parameters."""
        pass

    @abstractmethod
    def evaluate(self, *, 
                 distance_matrix: Optional[Any] = None,
                 features: Optional[Any] = None, 
                 distance_matrix_path: Optional[Path] = None,
                 feature_hdf5_path: Optional[Path] = None, 
                 dataset: Optional[Any] = None,
                 labels: Optional[List[Any]] = None,
                 item_id_map: Optional[Dict[str, Dict]] = None,
                 feature_config: Optional[Dict[str, Any]] = None, 
                 **kwargs) -> Dict[str, Any]: 
        """
        Run the benchmark evaluation.

        Accepts either data directly (e.g., distance_matrix) OR paths
        to HDF5 files (feature_hdf5_path).
        Implementations should handle loading data from paths if direct data is None
        or implement chunked processing if feasible (like ClassificationBenchmark).
        """
        pass

    def __call__(self, **kwargs) -> Dict[str, Any]:
        """Allows calling the instance like a function."""
        return self.evaluate(**kwargs)import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Type
import torch
import numpy as np
import pandas as pd
from collections import Counter
from tqdm import tqdm
import h5py
import gc
import time
import itertools

from sklearn.metrics import make_scorer, top_k_accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics._scorer import _BaseScorer

from benchmarks.base import Benchmark
from utils.file_utils import read_hdf5_slice, read_hdf5_metadata

logger = logging.getLogger(__name__)
HDF5_FEATURE_DATASET_NAME = "features"
DEFAULT_CHUNK_SIZE = 1024


class ClassificationBenchmark(Benchmark):
    def _initialize(
        self,
        n_splits: int = 5,
        classifiers: Optional[List[str]] = None,
        classifier_params: Optional[Dict[str, Dict]] = None,
        random_state: int = 42,
        eval_metrics: Optional[List[str]] = None,
        top_k: Optional[int] = 5,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        label_source_key: Optional[str] = None,
        **kwargs,
    ):
        """
        Initializes the ClassificationBenchmark parameters and sets up classifier configurations.

        Args:
            n_splits (int): Number of splits for cross-validation.
            classifiers (Optional[List[str]]): List of classifier types to run
                ('knn', 'rf', 'mlp'). If None, defaults to all.
            classifier_params (Optional[Dict[str, Dict]]): Dictionary of parameter
                grids or values for each classifier type. Merged with defaults.
            random_state (int): Random state for reproducibility.
            eval_metrics (Optional[List[str]]): List of standard scikit-learn
                scoring metric names (e.g., 'accuracy', 'f1_macro').
                If None, defaults to ['accuracy'].
            top_k (Optional[int]): The k value for top-k accuracy scoring. If None
                or <= 1, top-k scoring is disabled.
            chunk_size (int): Size of chunks to read features (not directly used in
                current CV approach, but kept for potential future uses).
            label_source_key (Optional[str]): Key used by BenchmarkManager to
                retrieve the list of labels.
            **kwargs: Additional keyword arguments.
        """
        self.n_splits = n_splits
        self.random_state = random_state
        self.eval_metrics = eval_metrics or ["accuracy"]
        self.top_k = top_k if top_k and top_k > 1 else None
        self.chunk_size = chunk_size
        self.label_source_key = label_source_key
        default_classifiers = ["knn", "rf", "mlp"]
        self.classifiers_to_run = classifiers or default_classifiers
        default_params = {
            "knn": {"n_neighbors": [3, 10, 30], "n_jobs": [-1]},
            "rf": {"max_depth": [10, 20], "random_state": [self.random_state], "n_jobs": [-1]},
            "mlp": {
                "alpha": [0.1, 0.01, 0.001],
                "random_state": [self.random_state],
                "max_iter": [500],
                "solver": ["adam"],
                "activation": ["relu"],
                "hidden_layer_sizes": [(100,), [400], [200, 200]],
                "batch_size": ["auto", 256],
                "learning_rate_init": [0.001],
                "learning_rate": ["adaptive"],
                "tol": [1e-4],
                "early_stopping": [True],
            },
        }
        self.classifier_configs = {}
        user_params = classifier_params or {}
        config_counter = 0

        logger.debug("Generating classifier configurations...")
        for clf_type in self.classifiers_to_run:
            base_params = default_params.get(clf_type, {})
            user_type_params = user_params.get(clf_type, {})
            param_grid = {**base_params, **user_type_params}
            param_grid = {k: (v if isinstance(v, list) else [v]) for k, v in param_grid.items()}

            keys, values = zip(*param_grid.items())
            product_count = 0
            for bundle_values in itertools.product(*values):
                product_count += 1
                bundle = dict(zip(keys, bundle_values))

                def format_val(v):
                    if isinstance(v, tuple):
                        return f"({','.join(map(str, v))}{',' if len(v)==1 else ''})"
                    if isinstance(v, list):
                        return f"[{','.join(map(str, v))}]"
                    return repr(v)

                param_strs = []
                for k, v_loop in sorted(bundle.items()):
                    param_strs.append(f"{k}={format_val(v_loop)}")
                desc_string = f"{clf_type}(" + ", ".join(param_strs) + ")"
                config_key = f"{clf_type}_{config_counter}"
                config_counter += 1
                self.classifier_configs[config_key] = {
                    "type": clf_type,
                    "params": bundle,
                    "description": desc_string,
                }
            logger.debug(f"Generated {product_count} configurations for {clf_type}.")
        logger.info(
            f"Initialized ClassificationBenchmark. N_Splits={n_splits}, TopK={self.top_k}, LabelSourceKey='{self.label_source_key}'"
        )
        logger.info(
            f"Classifiers to run ({len(self.classifier_configs)} configs): {[v['description'] for v in self.classifier_configs.values()]}"
        )

    def _get_classifier_instance(self, clf_type: str, params: Dict) -> Optional[Any]:
        """
        Creates an instance of a scikit-learn classifier based on type and parameters.

        Args:
            clf_type (str): The type of classifier ('knn', 'rf', 'mlp').
            params (Dict): The parameters for the classifier constructor.

        Returns:
            Optional[Any]: An instantiated classifier object, or None if
                           instantiation fails (especially for MLP with bad params).
        """
        if clf_type == "knn":
            return KNeighborsClassifier(**params)
        elif clf_type == "rf":
            return RandomForestClassifier(**params)
        elif clf_type == "mlp":
            params_copy = params.copy()
            hls_param = params_copy.get("hidden_layer_sizes")
            final_hls_tuple = None
            if isinstance(hls_param, (list, tuple)):
                if hls_param and all(isinstance(i, int) and i > 0 for i in hls_param):
                    final_hls_tuple = tuple(hls_param)
                else:
                    logger.error(f"Invalid elements in hidden_layer_sizes list/tuple: {hls_param}")
                    return None
            elif isinstance(hls_param, int):
                if hls_param > 0:
                    final_hls_tuple = (hls_param,)
                else:
                    logger.error(f"Invalid non-positive integer for hidden_layer_sizes: {hls_param}")
                    return None
            else:
                logger.error(f"Invalid or missing 'hidden_layer_sizes'. Expected list/tuple/int, got {type(hls_param)}.")
                return None
            params_copy["hidden_layer_sizes"] = final_hls_tuple
            try:
                logger.debug(f"Instantiating MLPClassifier with final params: {params_copy}")
                return MLPClassifier(**params_copy)
            except Exception as mlp_init_err:
                logger.error(f"Error instantiating MLPClassifier: {mlp_init_err}. Params: {params_copy}", exc_info=True)
                return None
        else:
            raise ValueError(f"Unsupported classifier type: {clf_type}")

    def evaluate(
        self, feature_hdf5_path: Optional[Path] = None, labels: Optional[List[Any]] = None, **kwargs
    ) -> Dict[str, Any]:
        """
        Runs the classification benchmark.

        Loads features from the specified HDF5 file, prepares and encodes labels
        based on HDF5 metadata indices, and performs cross-validation using
        the configured classifiers and scoring metrics.

        Args:
            feature_hdf5_path (Optional[Path]): Path to the HDF5 file containing features.
            labels (Optional[List[Any]]): A list of labels provided by the
                BenchmarkManager, corresponding to the original item indices.
            **kwargs: Additional keyword arguments (not used by this method).

        Returns:
            Dict[str, Any]: A dictionary containing the results for each classifier
                           configuration. Each key is the configuration description,
                           and the value is a dictionary of metric results (mean and std)
                           or an error message.
        """
        start_eval_time = time.time()
        if not self.enabled:
            return {"error": "scikit-learn not installed"}
        if feature_hdf5_path is None:
            return {"error": "feature_hdf5_path is required for ClassificationBenchmark."}
        if not feature_hdf5_path.exists():
            return {"error": f"Feature HDF5 file not found: {feature_hdf5_path}"}
        if labels is None:
            logger.error(
                "ClassificationBenchmark requires 'labels' list. This list should be populated by BenchmarkManager using 'label_source_key'."
            )
            return {"error": "Labels missing"}

        overall_results = {}
        logger.info(f"Starting Classification Benchmark on: {feature_hdf5_path.name}")
        logger.info(f"Received labels list length: {len(labels) if labels else 'None'}")
        if labels and len(labels) > 5:
            logger.info(f"Sample of first 5 received labels: {labels[:5]}")

        try:
            feature_metadata = read_hdf5_metadata(feature_hdf5_path, HDF5_FEATURE_DATASET_NAME)
            if not feature_metadata:
                logger.error(f"Could not read feature metadata from {feature_hdf5_path} for dataset {HDF5_FEATURE_DATASET_NAME}")
                raise ValueError("Could not read feature metadata.")

            n_samples_hdf5 = feature_metadata.get("num_items_processed")
            if n_samples_hdf5 is None:
                n_samples_hdf5 = feature_metadata.get("num_items")
                attr_used = "'num_items' (fallback)"
            else:
                attr_used = "'num_items_processed'"
            if n_samples_hdf5 is None:
                logger.error(
                    f"Could not determine number of samples from HDF5 metadata ('num_items_processed' or 'num_items'). File: {feature_hdf5_path.name}"
                )
                raise ValueError("Could not determine number of samples from metadata.")
            n_samples_hdf5 = int(n_samples_hdf5)
            logger.info(f"HDF5 metadata indicates {n_samples_hdf5} samples (using {attr_used}).")
            if n_samples_hdf5 <= 0:
                return {"error": f"Feature file contains {n_samples_hdf5} samples."}

            original_indices = feature_metadata.get("original_indices")

            if not isinstance(labels, list):
                try:
                    labels = list(labels)
                except TypeError:
                    return {"error": f"Labels object (type {type(labels)}) is not list-convertible."}

            if original_indices is not None:
                logger.info(f"Found 'original_indices' in HDF5 metadata (length: {len(original_indices)}). Mapping labels...")
                if not isinstance(original_indices, (list, np.ndarray)):
                    raise TypeError("original_indices in metadata must be a list or numpy array.")
                if len(original_indices) != n_samples_hdf5:
                    logger.error(
                        f"Metadata mismatch: len(original_indices)={len(original_indices)} != n_samples_hdf5={n_samples_hdf5}. Aborting."
                    )
                    return {"error": "Metadata mismatch: indices vs sample count."}
                hdf5_idx_map = {int(orig_idx): hdf5_pos for hdf5_pos, orig_idx in enumerate(original_indices)}
                logger.debug(f"Built hdf5_idx_map from original_indices. Size: {len(hdf5_idx_map)}")
            else:
                logger.warning("No 'original_indices' in metadata. Assuming HDF5 data corresponds to first N labels.")
                if len(labels) < n_samples_hdf5:
                    logger.error(
                        f"Labels list length ({len(labels)}) is less than HDF5 sample count ({n_samples_hdf5}) and no 'original_indices' provided."
                    )
                    return {"error": f"Labels list length ({len(labels)}) < HDF5 sample count ({n_samples_hdf5}) and no indices provided."}
                hdf5_idx_map = {i: i for i in range(n_samples_hdf5)}
                logger.debug(f"Built hdf5_idx_map assuming direct correspondence. Size: {len(hdf5_idx_map)}")

            y_ordered = [None] * n_samples_hdf5
            valid_label_count = 0
            indices_in_hdf5_with_valid_label = []
            max_label_index = len(labels) - 1
            num_skipped_due_to_none_label = 0
            num_skipped_due_to_idx_out_of_bounds = 0

            for orig_idx_key, hdf5_pos_val in hdf5_idx_map.items():
                orig_idx = int(orig_idx_key)
                hdf5_pos = int(hdf5_pos_val)

                if 0 <= orig_idx <= max_label_index:
                    label_value = labels[orig_idx]
                    if label_value is not None:
                        y_ordered[hdf5_pos] = str(label_value)
                        valid_label_count += 1
                        indices_in_hdf5_with_valid_label.append(hdf5_pos)
                    else:
                        num_skipped_due_to_none_label += 1
                else:
                    num_skipped_due_to_idx_out_of_bounds += 1

            logger.debug(
                f"Label mapping: Skipped {num_skipped_due_to_none_label} (None label),"
                f" {num_skipped_due_to_idx_out_of_bounds} (idx OOB)."
            )

            if valid_label_count == 0:
                logger.error("No valid labels found corresponding to features in HDF5 file after mapping.")
                return {"error": "No valid labels found corresponding to features in HDF5 file."}

            y_str = np.array([y_ordered[hdf5_pos] for hdf5_pos in indices_in_hdf5_with_valid_label])
            valid_hdf5_indices = np.array(indices_in_hdf5_with_valid_label, dtype=int)

            if valid_hdf5_indices.size == 0 or y_str.size == 0 or valid_hdf5_indices.size != y_str.size:
                logger.error("Internal error: Mismatch between valid indices and labels after filtering.")
                return {"error": "Internal error: Label/index mismatch after filtering."}

            logger.info(f"Encoding {len(y_str)} string labels into numerical format...")
            le = LabelEncoder()
            try:
                y = le.fit_transform(y_str)
                encoded_classes = le.classes_
                logger.info(f"Label encoding successful. Found {len(encoded_classes)} unique classes.")
                if not np.issubdtype(y.dtype, np.integer):
                    logger.warning(f"LabelEncoder output dtype is {y.dtype}, ensuring integer.")
                    y = y.astype(int)
            except Exception as le_err:
                logger.error(f"LabelEncoder failed: {le_err}", exc_info=True)
                return {"error": f"Label encoding failed: {le_err}"}

            logger.info(f"Prepared {len(y)} numerical labels for classification.")
            if len(y) > 0:
                unique_y_numeric, counts_y_numeric = np.unique(y, return_counts=True)
                logger.info(f"Final numerical label distribution (count: {len(unique_y_numeric)}): {dict(zip(unique_y_numeric, counts_y_numeric))}")

            if len(y) < self.n_splits:
                logger.error(f"Insufficient valid labeled samples ({len(y)}) for {self.n_splits}-fold CV.")
                return {"error": f"Insufficient valid labeled samples ({len(y)}) for {self.n_splits}-fold CV"}

            label_counts = Counter(y)
            min_samples_per_class = min(label_counts.values()) if label_counts else 0
            logger.info(f"Min samples per class: {min_samples_per_class}, n_splits: {self.n_splits}")

            if min_samples_per_class < self.n_splits:
                error_msg = (
                    f"Smallest class ({min_samples_per_class}) < n_splits ({self.n_splits}). "
                    f"Distribution: {dict(label_counts)}"
                )
                logger.error(error_msg)
                return {"error": error_msg}

            skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)
            scorers = {metric: metric for metric in self.eval_metrics}
            unique_y_labels_numeric = np.unique(y)
            if self.top_k is not None and len(unique_y_labels_numeric) > 1:
                try:
                    scorers[f"top_{self.top_k}_accuracy"] = make_scorer(
                        score_func=top_k_accuracy_score,
                        response_method=("decision_function", "predict_proba"),
                        greater_is_better=True,
                        k=self.top_k,
                        labels=unique_y_labels_numeric,
                    )
                    logger.info(f"Successfully created top-{self.top_k} accuracy scorer.")
                except Exception as scorer_err:
                    logger.error(f"Failed to create top-{self.top_k} scorer: {scorer_err}", exc_info=True)
                    scorers.pop(f"top_{self.top_k}_accuracy", None)

        except Exception as e:
            logger.error(f"Data/Label preparation failed: {e}", exc_info=True)
            return {"error": f"Data prep/CV setup failed: {e}"}

        num_configs = len(self.classifier_configs)
        for config_idx, (config_key, config_data) in enumerate(self.classifier_configs.items()):
            config_start_time = time.time()
            clf_type = config_data["type"]
            params = config_data["params"]
            config_description = config_data["description"]

            logger.info(f"--- Evaluating Config {config_idx+1}/{num_configs}: {config_description} ---")
            clf_results = {}
            X = None

            if clf_type in ["knn", "rf", "mlp"]:
                logger.debug(f"Loading relevant feature data for {config_description}...")
                try:
                    if valid_hdf5_indices.size == 0:
                        raise ValueError("No valid indices to load features from.")
                    min_hdf5_idx = valid_hdf5_indices.min()
                    max_hdf5_idx = valid_hdf5_indices.max()

                    block_features = read_hdf5_slice(
                        feature_hdf5_path, int(min_hdf5_idx), int(max_hdf5_idx) + 1, HDF5_FEATURE_DATASET_NAME
                    )
                    if block_features is None:
                        raise IOError(f"Failed to read HDF5 feature block [{min_hdf5_idx}-{max_hdf5_idx+1}]")

                    relative_indices = valid_hdf5_indices - min_hdf5_idx
                    if np.any(relative_indices < 0) or np.any(relative_indices >= block_features.shape[0]):
                        raise IndexError(
                            "Calculated relative indices out of bounds for loaded block."
                            f" MinRel={relative_indices.min()}, MaxRel={relative_indices.max()}, BlockShape={block_features.shape[0]}"
                        )

                    X_raw = block_features[relative_indices]

                    feature_dim_orig = X_raw.shape[1:]
                    if X_raw.ndim > 2:
                        X = X_raw.reshape(X_raw.shape[0], -1)
                    elif X_raw.ndim == 1:
                        X = X_raw.reshape(-1, 1)
                    elif X_raw.ndim == 2:
                        X = X_raw
                    else:
                        raise ValueError(f"Invalid feature dimensionality {X_raw.ndim}")
                    feature_dim_flat = X.shape[1]

                    if not np.isfinite(X).all():
                        nan_count = np.sum(np.isnan(X))
                        inf_count = np.sum(np.isinf(X))
                        logger.warning(f"NaN/Inf found in features for CV (NaNs: {nan_count}, Infs: {inf_count}). Replacing with 0.")
                        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

                    if X.shape[0] != len(y):
                        raise RuntimeError(f"Feature samples ({X.shape[0]}) != label count ({len(y)}).")
                    logger.info(f"Loaded features for CV (orig dim: {feature_dim_orig}, flat dim: {feature_dim_flat}). Shape: {X.shape}")

                    classifier = self._get_classifier_instance(clf_type, params)
                    if classifier is None:
                        logger.error(f"Could not instantiate classifier for {config_description}. Skipping.")
                        clf_results = {"error": "Classifier instantiation failed (check logs for parameter issues)."}
                        overall_results[config_description] = clf_results
                        continue

                    pipeline = Pipeline([("scaler", StandardScaler()), ("classifier", classifier)])
                    current_scorers = scorers.copy()

                    clf_needs_proba = any(
                        isinstance(s, _BaseScorer)
                        and (s._response_method == "predict_proba" or s._response_method == ("decision_function", "predict_proba"))
                        for s in current_scorers.values()
                    )
                    clf_supports_proba = hasattr(classifier, "predict_proba")
                    clf_supports_decision = hasattr(classifier, "decision_function")

                    if clf_needs_proba and not (clf_supports_proba or clf_supports_decision):
                        logger.warning(
                            f"Classifier {config_description} doesn't support predict_proba or decision_function,"
                            f" removing scorers requiring them (e.g., top_k_accuracy)."
                        )
                        scorers_to_remove = [
                            name
                            for name, scorer in current_scorers.items()
                            if isinstance(scorer, _BaseScorer) and scorer._response_method != "predict"
                        ]
                        current_scorers = {k: v for k, v in current_scorers.items() if k not in scorers_to_remove}

                        if not current_scorers:
                            logger.warning(f"No scorers left for {config_description}. Skipping.")
                            clf_results = {"warning": "Skipped, no valid scorers."}
                            overall_results[config_description] = clf_results
                            continue

                    logger.debug(f"Running standard cross_validate for {config_description}...")
                    cv_results = cross_validate(pipeline, X, y, cv=skf, scoring=current_scorers, n_jobs=1, error_score="raise")

                    for metric in current_scorers.keys():
                        score_key = f"test_{metric}"
                        mean_score = np.nan
                        std_score = np.nan
                        if score_key in cv_results and cv_results[score_key] is not None:
                            scores_array = np.asarray(cv_results[score_key])
                            valid_scores = scores_array[~np.isnan(scores_array)]
                            if len(valid_scores) > 0:
                                mean_score = np.mean(valid_scores) * 100
                                std_score = np.std(valid_scores) * 100
                            else:
                                logger.warning(f"No valid scores found for metric '{metric}' for {config_description}.")
                        else:
                            logger.warning(
                                f"Metric '{metric}' (key: {score_key}) not found or None in cv_results for {config_description}. Keys: {list(cv_results.keys())}"
                            )

                        clf_results[f"{metric}_mean"] = mean_score
                        clf_results[f"{metric}_std"] = std_score
                        if not np.isnan(mean_score):
                            logger.info(f"  {metric}: {mean_score:.2f}% (+/- {std_score:.2f}%)")
                        else:
                            logger.info(f"  {metric}: NaN (Check logs for errors during CV)")

                except Exception as e:
                    logger.error(f"Failed CV Pipeline for {config_description}: {e}", exc_info=True)
                    clf_results = {f"{metric}_mean": np.nan for metric in scorers}
                    clf_results.update({f"{metric}_std": np.nan for metric in scorers})
                    clf_results["error"] = f"CV Fail: {str(e)}"
                finally:
                    if X is not None:
                        del X
                    if "pipeline" in locals():
                        del pipeline
                    if "classifier" in locals():
                        del classifier
                    if "block_features" in locals():
                        del block_features
                    if "X_raw" in locals():
                        del X_raw
                    gc.collect()
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.empty_cache()
                        except Exception:
                            pass

            else:
                logger.warning(f"Skipping unsupported classifier type: {clf_type}")
                clf_results = {"error": f"Classifier type '{clf_type}' not handled."}

            overall_results[config_description] = clf_results
            config_elapsed = time.time() - config_start_time
            logger.info(f"--- Config {config_description} finished in {config_elapsed:.2f}s ---")

        eval_elapsed = time.time() - start_eval_time
        logger.info(f"Classification benchmark finished in {eval_elapsed:.2f}s.")
        return overall_resultsimport logging
from typing import Any, Dict, List, Optional, Union
import numpy as np
from collections import Counter
import time
from pathlib import Path
import gc
from sklearn.cluster import HDBSCAN
from sklearn.metrics.cluster import contingency_matrix
import umap

from benchmarks.base import Benchmark
from utils.file_utils import load_hdf5, read_hdf5_metadata

logger = logging.getLogger(__name__)
HDF5_FEATURE_DATASET_NAME = "features"
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"

DEFAULT_CP_METRICS = {
    "weighted_purity": None,
    "num_clusters_found": None,
    "num_noise_points": None,
    "error": None,
}


class ClusteringPurity(Benchmark):
    """
    Evaluates feature separation using UMAP projection and HDBSCAN clustering.
    Loads required features or distance matrix from HDF5 file internally.
    """

    def _initialize(
        self,
        umap_n_components: int = 2,
        umap_metric: str = "cosine",
        umap_low_memory: bool = True,
        umap_random_state: int = 42,
        hdbscan_min_cluster_size: int = 5,
        hdbscan_min_samples: Optional[int] = None,
        hdbscan_metric: str = "euclidean",
        use_distance_matrix_for_umap: bool = False,
        **kwargs,
    ):
        """
        Initializes parameters for UMAP, HDBSCAN, and data loading.

        Args:
            umap_n_components (int): Number of dimensions for UMAP projection.
            umap_metric (str): Metric to use for UMAP ('cosine', 'euclidean', etc.).
            umap_low_memory (bool): Whether to use UMAP's low_memory mode.
            umap_random_state (int): Random state for UMAP.
            hdbscan_min_cluster_size (int): Minimum size of clusters for HDBSCAN.
            hdbscan_min_samples (Optional[int]): Minimum samples in neighborhood for HDBSCAN.
            hdbscan_metric (str): Metric for HDBSCAN.
            use_distance_matrix_for_umap (bool): If True, loads distance matrix; otherwise loads features.
            **kwargs: Additional keyword arguments.
        """
        self.use_dist_matrix = use_distance_matrix_for_umap
        _umap_metric = "precomputed" if self.use_dist_matrix else umap_metric
        self.umap_params = {
            "n_components": umap_n_components,
            "metric": _umap_metric,
            "low_memory": umap_low_memory,
            "random_state": umap_random_state,
            "verbose": False,
            "init": "random",
        }
        self.hdbscan_params = {
            "min_cluster_size": hdbscan_min_cluster_size,
            "min_samples": hdbscan_min_samples if hdbscan_min_samples else hdbscan_min_cluster_size,
            "metric": hdbscan_metric,
            "n_jobs": -1,
        }

    def evaluate(
        self,
        *,
        distance_matrix_path: Optional[Path] = None,
        feature_hdf5_path: Optional[Path] = None,
        distance_matrix: Optional[Any] = None,
        features: Optional[Any] = None,
        dataset: Optional[Any] = None,
        labels: Optional[List[Any]] = None,
        item_id_map: Optional[Dict[str, Dict]] = None,
        min_class_size_for_purity: int = 1,
        feature_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Performs UMAP dimensionality reduction, HDBSCAN clustering on the projection,
        and calculates the weighted purity of the clusters against true labels.

        Loads data (either features or a distance matrix) from HDF5 files or
        uses directly provided data. Filters data and labels to ensure correspondence
        and handle missing labels.

        Args:
            distance_matrix_path (Optional[Path]): Path to HDF5 file containing the distance matrix.
            feature_hdf5_path (Optional[Path]): Path to HDF5 file containing the features.
            distance_matrix (Optional[Any]): Directly provided distance matrix data.
            features (Optional[Any]): Directly provided features data.
            dataset (Optional[Any]): Contextual dataset object (not directly used).
            labels (Optional[List[Any]]): A list of labels corresponding to original item indices.
            item_id_map (Optional[Dict[str, Dict]]): Mapping from original item IDs (not directly used).
            min_class_size_for_purity (int): Minimum size a true class must have to be included
                in the purity calculation.
            feature_config (Optional[Dict[str, Any]]): Contextual feature configuration (not directly used).
            **kwargs: Additional keyword arguments (not used).

        Returns:
            Dict[str, Any]: A dictionary containing weighted purity, number of clusters found,
                            number of noise points, sample count after filtering, timing information,
                            and an error message if applicable.
        """
        input_data = None
        input_data_source = None
        n_samples = 0
        filtered_labels = []
        metadata = None
        data_load_success = False

        logger.info(f"ClusteringPurity: use_distance_matrix={self.use_dist_matrix}")
        if self.use_dist_matrix:
            if distance_matrix is not None:
                logger.debug("Using directly provided distance_matrix.")
                input_data = distance_matrix
                input_data_source = "direct_distance_matrix"
                data_load_success = True
            elif distance_matrix_path is not None and distance_matrix_path.exists():
                logger.debug(f"Loading distance matrix from HDF5: {distance_matrix_path}")
                input_data = load_hdf5(distance_matrix_path, HDF5_DISTANCE_DATASET_NAME, "Distance Matrix")
                input_data_source = str(distance_matrix_path)
                if input_data is not None:
                    metadata = read_hdf5_metadata(distance_matrix_path, HDF5_DISTANCE_DATASET_NAME)
                    data_load_success = True
            if not data_load_success:
                return {**DEFAULT_CP_METRICS, "error": "Distance matrix required but not provided/loaded"}
            try:
                if not isinstance(input_data, np.ndarray):
                    input_data = np.array(input_data, dtype=np.float32)
                if input_data.ndim != 2 or input_data.shape[0] != input_data.shape[1]:
                    raise ValueError(f"Loaded distance matrix invalid shape: {input_data.shape}")
                n_samples = input_data.shape[0]
            except Exception as e:
                return {**DEFAULT_CP_METRICS, "error": f"Invalid distance matrix format: {e}"}
            if self.umap_params.get("metric") != "precomputed":
                logger.warning("Using distance matrix but UMAP metric is not 'precomputed'. Forcing.")
                self.umap_params["metric"] = "precomputed"

        else:
            if features is not None:
                logger.debug("Using directly provided features.")
                input_data = features
                input_data_source = "direct_features"
                data_load_success = True
            elif feature_hdf5_path is not None and feature_hdf5_path.exists():
                logger.debug(f"Loading features from HDF5: {feature_hdf5_path}")
                input_data = load_hdf5(feature_hdf5_path, HDF5_FEATURE_DATASET_NAME, "Features")
                input_data_source = str(feature_hdf5_path)
                if input_data is not None:
                    metadata = read_hdf5_metadata(feature_hdf5_path, HDF5_FEATURE_DATASET_NAME)
                    data_load_success = True
            if not data_load_success:
                return {**DEFAULT_CP_METRICS, "error": "Features required but not provided/loaded"}
            if self.umap_params.get("metric") == "precomputed":
                logger.warning("Using features but UMAP metric is 'precomputed'. Changing to 'cosine'.")
                self.umap_params["metric"] = "cosine"

            try:
                if not isinstance(input_data, np.ndarray):
                    input_data = np.array(input_data, dtype=np.float32)
                n_samples = input_data.shape[0]
                if input_data.ndim > 2:
                    logger.info(f"Input features >2D ({input_data.shape}). Flattening non-sample dims for UMAP/HDBSCAN.")
                    input_data = input_data.reshape(n_samples, -1)
                elif input_data.ndim == 1:
                    logger.warning(f"Input features are 1D ({input_data.shape}). Reshaping to (N, 1).")
                    input_data = input_data.reshape(-1, 1)
                elif input_data.ndim == 0:
                    raise ValueError("Cannot use scalar features")
                elif input_data.ndim != 2:
                    raise ValueError(f"Unexpected feature dimensionality: {input_data.ndim}")
            except Exception as e:
                return {**DEFAULT_CP_METRICS, "error": f"Invalid feature format/flattening failed: {e}"}

        logger.info(f"Loaded data for ClusteringPurity. Shape: {input_data.shape}, Source: {input_data_source}")

        if labels is None:
            logger.error("ClusteringPurity requires 'labels'.")
            del input_data
            gc.collect()
            return {**DEFAULT_CP_METRICS, "error": "Labels missing"}
        if not isinstance(labels, list):
            try:
                labels = list(labels)
            except TypeError:
                del input_data
                gc.collect()
                return {**DEFAULT_CP_METRICS, "error": "Labels not list-convertible"}

        original_indices = None
        if metadata and "original_indices" in metadata:
            original_indices = metadata["original_indices"]
            if len(original_indices) != n_samples:
                logger.warning(
                    f"Metadata original_indices length ({len(original_indices)}) != data samples ({n_samples}). Trusting data shape."
                )
                original_indices = list(range(n_samples))
        else:
            logger.warning("No 'original_indices' in metadata. Assuming data corresponds to first N labels.")
            original_indices = list(range(n_samples))

        valid_indices_map = {}
        filtered_indices_in_input_data = []

        for i, orig_idx in enumerate(original_indices):
            if 0 <= orig_idx < len(labels) and labels[orig_idx] is not None:
                valid_indices_map[i] = str(labels[orig_idx])
                filtered_indices_in_input_data.append(i)

        if not valid_indices_map:
            logger.error("No valid labels found corresponding to the loaded data.")
            del input_data
            gc.collect()
            return {**DEFAULT_CP_METRICS, "error": "No valid labels for data"}

        if len(filtered_indices_in_input_data) != n_samples:
            logger.info(f"Filtering input data ({n_samples}) for {len(filtered_indices_in_input_data)} valid labels.")
            if self.use_dist_matrix:
                input_data = input_data[np.ix_(filtered_indices_in_input_data, filtered_indices_in_input_data)]
            else:
                input_data = input_data[filtered_indices_in_input_data]
            n_samples = input_data.shape[0]
            filtered_labels = [valid_indices_map[i] for i in filtered_indices_in_input_data]
            logger.info(f"Data filtered. New shape: {input_data.shape}")
        else:
            filtered_labels = [str(labels[orig_idx]) for orig_idx in original_indices]

        if n_samples != len(filtered_labels):
            logger.error(f"CRITICAL: Filter mismatch: Data samples ({n_samples}) != Filtered labels ({len(filtered_labels)}).")
            del input_data
            gc.collect()
            return {**DEFAULT_CP_METRICS, "error": "Internal data/label filter mismatch"}

        if n_samples < self.umap_params.get("n_components", 2) or n_samples < self.hdbscan_params.get("min_cluster_size", 2):
            logger.error(f"Not enough valid samples ({n_samples}) for UMAP/HDBSCAN. Skipping.")
            del input_data
            gc.collect()
            return {**DEFAULT_CP_METRICS, "error": "Insufficient valid samples after filtering"}

        if np.isnan(input_data).any() or np.isinf(input_data).any():
            logger.warning(
                f"NaN/Inf detected in filtered input data (shape {input_data.shape}) before UMAP/HDBSCAN. Replacing with finite min/max."
            )
            finite_mask = np.isfinite(input_data)
            max_finite = 0.0
            min_finite = 0.0
            if np.any(finite_mask):
                finite_values = input_data[finite_mask]
                if finite_values.size > 0:
                    max_finite = np.max(finite_values)
                    min_finite = np.min(finite_values)
                logger.debug(f"Replacing non-finite: Max finite = {max_finite}, Min finite = {min_finite}")
            else:
                logger.warning("Input data contains only NaN/Inf values after filtering! Replacing with 0.0.")
            input_data = np.where(np.isnan(input_data), max_finite, input_data)
            input_data = np.where(np.isposinf(input_data), max_finite, input_data)
            input_data = np.where(np.isneginf(input_data), min_finite, input_data)
            if np.isnan(input_data).any() or np.isinf(input_data).any():
                logger.error("CRITICAL: Non-finite values still present after replacement! Aborting.")
                return {**DEFAULT_CP_METRICS, "error": "Failed to replace non-finite values"}
            if self.use_dist_matrix:
                logger.debug("Re-zeroing diagonal of distance matrix after NaN replacement.")
                np.fill_diagonal(input_data, 0)

        embeddings = None
        umap_time = -1.0
        logger.info("Starting UMAP projection...")
        start_time = time.time()
        try:
            reducer = umap.UMAP(**self.umap_params)
            embeddings = reducer.fit_transform(input_data)
            umap_time = time.time() - start_time
            logger.info(f"UMAP done ({umap_time:.2f}s). Embedding Shape: {embeddings.shape}")
            if embeddings.shape[0] != n_samples:
                raise RuntimeError(f"UMAP output samples ({embeddings.shape[0]}) != expected ({n_samples})")
            if np.isnan(embeddings).any() or np.isinf(embeddings).any():
                raise ValueError("UMAP output contains NaN/Inf.")
        except Exception as e:
            logger.error(f"UMAP failed: {e}. Input shape: {input_data.shape}, UMAP params: {self.umap_params}", exc_info=True)
            if input_data.size < 100:
                logger.error(f"Sample of input data to UMAP:\n{input_data}")
            del input_data
            gc.collect()
            return {**DEFAULT_CP_METRICS, "error": f"UMAP fail: {e}"}

        cluster_labels = None
        num_clusters = 0
        num_noise = 0
        hdbscan_time = -1.0
        logger.info("Starting HDBSCAN clustering...")
        start_time = time.time()
        try:
            clusterer = HDBSCAN(**self.hdbscan_params)
            cluster_labels = clusterer.fit_predict(embeddings)
            hdbscan_time = time.time() - start_time
            num_clusters = len(set(cluster_labels) - {-1})
            num_noise = np.sum(cluster_labels == -1)
            logger.info(f"HDBSCAN done ({hdbscan_time:.2f}s). Found {num_clusters} clusters, {num_noise} noise points.")
            if len(cluster_labels) != n_samples:
                raise RuntimeError(f"HDBSCAN output labels ({len(cluster_labels)}) != expected ({n_samples})")
        except Exception as e:
            logger.error(f"HDBSCAN failed: {e}", exc_info=True)
            del input_data, embeddings
            gc.collect()
            return {
                **DEFAULT_CP_METRICS,
                "num_clusters_found": 0,
                "num_noise_points": n_samples,
                "error": f"HDBSCAN fail: {e}",
            }

        weighted_purity = None
        logger.info("Calculating weighted purity...")
        try:
            true_label_counts = Counter(filtered_labels)
            _min_class_size = max(1, min_class_size_for_purity)
            valid_class_mask = np.array([true_label_counts[lbl] >= _min_class_size for lbl in filtered_labels])
            if not np.any(valid_class_mask):
                logger.warning(f"No classes >= {_min_class_size} samples found. Purity is None.")
                weighted_purity = None
            else:
                labels_for_purity = np.array(filtered_labels)[valid_class_mask]
                preds_for_purity = cluster_labels[valid_class_mask]
                if len(labels_for_purity) != len(preds_for_purity):
                    raise RuntimeError(f"Purity length mismatch: Labels ({len(labels_for_purity)}) vs Preds ({len(preds_for_purity)}).")
                cont_mat = contingency_matrix(labels_for_purity, preds_for_purity)
                cluster_sizes = np.sum(cont_mat, axis=0)
                majority_counts = np.max(cont_mat, axis=0)
                unique_preds, cluster_indices = np.unique(preds_for_purity, return_inverse=True)
                actual_cluster_sizes = cluster_sizes
                actual_majority_counts = majority_counts
                cluster_purities = np.zeros(len(actual_cluster_sizes), dtype=float)
                valid_cluster_mask_purity = actual_cluster_sizes > 0
                if np.any(valid_cluster_mask_purity):
                    cluster_purities[valid_cluster_mask_purity] = actual_majority_counts[valid_cluster_mask_purity] / actual_cluster_sizes[valid_cluster_mask_purity]
                if np.any(valid_cluster_mask_purity):
                    total_weight = np.sum(actual_cluster_sizes[valid_cluster_mask_purity])
                    if total_weight > 0:
                        weighted_purity = np.average(cluster_purities[valid_cluster_mask_purity], weights=actual_cluster_sizes[valid_cluster_mask_purity])
                    else:
                        weighted_purity = 0.0
                else:
                    weighted_purity = 0.0
                logger.info(
                    "Weighted Purity calc details:"
                    f" Valid mask sum={np.sum(valid_class_mask)}, Labels={len(labels_for_purity)}, Preds={len(preds_for_purity)},"
                    f" ContMat Shape={cont_mat.shape}, Unique Preds={len(unique_preds)}"
                )
                logger.info(f"Weighted Purity: {weighted_purity:.4f}")
        except Exception as e:
            logger.error(f"Purity calculation failed: {e}", exc_info=True)
            weighted_purity = None

        del input_data, embeddings, cluster_labels, filtered_labels
        if "labels_for_purity" in locals():
            del labels_for_purity
        if "preds_for_purity" in locals():
            del preds_for_purity
        gc.collect()

        return {
            "weighted_purity": weighted_purity,
            "num_clusters_found": num_clusters,
            "num_noise_points": num_noise,
            "samples_after_filtering": n_samples,
            "umap_time_seconds": round(umap_time, 2) if umap_time >= 0 else None,
            "hdbscan_time_seconds": round(hdbscan_time, 2) if hdbscan_time >= 0 else None,
            "error": None,
        }import logging
from typing import Any, Dict, List, Optional
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import gc

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)


class CSCFBenchmark(Benchmark):
    """
    Computes a Class Separation Confusion Fraction (PCCF).
    """

    def _initialize(self, **kwargs):
        """
        Initializes the benchmark.
        """
        pass

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        distance_matrix_path: Optional[Path] = None,
        labels: Optional[List[Any]] = None,
        min_class_size: int = 2,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Evaluates the Class Separation Confusion Fraction (PCCF).

        Args:
            distance_matrix (Optional[Any]): Directly provided distance matrix data.
            distance_matrix_path (Optional[Path]): Path to HDF5 file containing the distance matrix (not used).
            labels (Optional[List[Any]]): A list of labels corresponding to original item indices.
            min_class_size (int): Minimum size a true class must have to be included
                in the PCCF calculation. Must be >= 2.
            **kwargs: Additional keyword arguments (not used).

        Returns:
            Dict[str, Any]: A dictionary containing the PCCF score ('pccf') and
                            an error message if applicable ('error').
        """
        default_return = {"pccf": None, "error": None}
        raw_dist_mat_input = distance_matrix
        n_samples_orig = 0
        dist_mat_tensor_for_calc = None

        if not isinstance(min_class_size, int) or min_class_size < 2:
            logger.warning(f"min_class_size must be >= 2. Setting to 2.")
            min_class_size = 2

        if raw_dist_mat_input is None:
            default_return["error"] = "distance_matrix not provided."
            return default_return

        if labels is None:
            default_return["error"] = "Labels missing."
            return default_return
        if not isinstance(labels, list):
            try:
                labels = list(labels)
            except TypeError:
                default_return["error"] = f"Labels must be list, got {type(labels)}."
                return default_return

        try:
            if isinstance(raw_dist_mat_input, np.ndarray):
                dist_mat_tensor_for_calc = torch.from_numpy(raw_dist_mat_input).float()
            elif isinstance(raw_dist_mat_input, torch.Tensor):
                dist_mat_tensor_for_calc = raw_dist_mat_input.float()
            else:
                raise TypeError(f"Unsupported distance_matrix: {type(raw_dist_mat_input)}")

            if dist_mat_tensor_for_calc.ndim != 2 or dist_mat_tensor_for_calc.shape[0] != dist_mat_tensor_for_calc.shape[1]:
                raise ValueError(f"dist_mat not square 2D: {dist_mat_tensor_for_calc.shape}")
            n_samples_orig = dist_mat_tensor_for_calc.shape[0]
            dist_mat_tensor_for_calc.fill_diagonal_(0)
            logger.info(f"Using provided distance matrix for PCCF. Shape: {dist_mat_tensor_for_calc.shape}")

        except Exception as e:
            default_return["error"] = f"Distance matrix processing failed: {e}"
            gc.collect()
            return default_return

        if len(labels) != n_samples_orig:
            logger.warning(f"Label count {len(labels)} != matrix dim {n_samples_orig}. Adjusting.")
            if len(labels) > n_samples_orig:
                labels = labels[:n_samples_orig]
            else:
                labels = labels + [None] * (n_samples_orig - len(labels))

        original_indices_with_valid_labels = [i for i, lbl in enumerate(labels) if lbl is not None and 0 <= i < n_samples_orig]
        filtered_labels = [str(labels[i]) for i in original_indices_with_valid_labels]

        if not filtered_labels:
            default_return["error"] = "No valid labels found."
            gc.collect()
            return default_return

        if len(original_indices_with_valid_labels) != n_samples_orig:
            logger.info(f"Filtering distance matrix for {len(filtered_labels)} valid labels.")
            try:
                valid_torch_indices = torch.tensor(original_indices_with_valid_labels, dtype=torch.long, device=dist_mat_tensor_for_calc.device)
                dist_mat_tensor_for_calc = dist_mat_tensor_for_calc[valid_torch_indices][:, valid_torch_indices]
            except Exception as e:
                default_return["error"] = f"Error filtering dist matrix: {e}"
                gc.collect()
                return default_return

        n_samples_valid = dist_mat_tensor_for_calc.shape[0]
        if n_samples_valid != len(filtered_labels):
            default_return["error"] = "Internal filter mismatch."
            gc.collect()
            return default_return

        results = default_return.copy()
        try:
            unique_label_list = sorted(list(set(filtered_labels)))
            label_to_id = {lbl: i for i, lbl in enumerate(unique_label_list)}
            labels_numeric = np.array([label_to_id[lbl] for lbl in filtered_labels], dtype=int)

            valid_class_data = {}

            all_numeric_ids = np.unique(labels_numeric)

            for num_label_id in all_numeric_ids:
                indices_this_class = np.where(labels_numeric == num_label_id)[0]
                if len(indices_this_class) >= min_class_size:
                    intra_class_block = dist_mat_tensor_for_calc[indices_this_class][:, indices_this_class]
                    if len(indices_this_class) > 1:
                        intra_mask = ~torch.eye(len(indices_this_class), dtype=torch.bool, device=intra_class_block.device)
                        valid_intra_distances = intra_class_block[intra_mask]
                        if valid_intra_distances.numel() > 0:
                            avg_intra_dist = torch.mean(valid_intra_distances).item()
                            valid_class_data[num_label_id] = {
                                "indices": torch.tensor(indices_this_class, device=dist_mat_tensor_for_calc.device),
                                "avg_intra_dist": avg_intra_dist,
                            }
                        else:
                            logger.warning(f"Class ID {num_label_id} had no valid intra-pairs despite size {len(indices_this_class)}. Skipping.")

            valid_class_ids = list(valid_class_data.keys())
            num_valid_classes = len(valid_class_ids)

            if num_valid_classes < 2:
                logger.warning(f"PCCF requires at least 2 classes meeting min_class_size. Found {num_valid_classes}.")
                results["error"] = f"Need >= 2 classes with size >= {min_class_size} for PCCF."
                return results

            logger.info(f"Calculating PCCF based on {num_valid_classes} valid classes.")

            total_pairwise_comparisons = 0
            total_confusion_events = 0

            pbar_outer = tqdm(range(num_valid_classes), desc="PCCF Outer Loop (Class i)", leave=False)
            for i_idx in pbar_outer:
                class_id_i = valid_class_ids[i_idx]
                data_i = valid_class_data[class_id_i]
                indices_i = data_i["indices"]
                avg_intra_dist_i = data_i["avg_intra_dist"]

                for j_idx in range(num_valid_classes):
                    if i_idx == j_idx:
                        continue

                    class_id_j = valid_class_ids[j_idx]
                    data_j = valid_class_data[class_id_j]
                    indices_j = data_j["indices"]
                    avg_intra_dist_j = data_j["avg_intra_dist"]

                    inter_block_ij = dist_mat_tensor_for_calc[indices_i][:, indices_j]
                    if inter_block_ij.numel() == 0:
                        logger.warning(f"Empty inter-block between class {class_id_i} and {class_id_j}. Skipping this pair.")
                        continue
                    avg_inter_dist_ij = torch.mean(inter_block_ij).item()

                    total_pairwise_comparisons += 1
                    if avg_inter_dist_ij < avg_intra_dist_i:
                        total_confusion_events += 1

            pbar_outer.close()

            if total_pairwise_comparisons == 0:
                logger.warning("No pairwise class comparisons were made.")
                results["error"] = "No pairwise comparisons possible."
            else:
                pccf = total_confusion_events / total_pairwise_comparisons
                logger.info(
                    f"PCCF: Confusions={total_confusion_events}, Comparisons={total_pairwise_comparisons}, PCCF={pccf:.4f}"
                )
                results = {"pccf": pccf, "error": None}

        except Exception as e:
            logger.error(f"PCCF calculation failed: {e}", exc_info=True)
            results["pccf"] = None
            results["error"] = f"PCCF calculation failed: {e}"
        finally:
            del dist_mat_tensor_for_calc
            gc.collect()
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception as e:
                    logger.warning(f"CUDA cache clear failed: {e}")
        return resultsimport logging
import time
from typing import Any, Dict, List, Optional

import numpy as np
import torch
from tqdm.auto import tqdm
import gc

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)


class ClassSeparationRatio(Benchmark):
    """
    Computes the Class Separation Ratio (CSR) score.

    For each class, CSR compares the average distance to the nearest point of a
    *different* class (Avg NID) with the average distance to the *furthest*
    point of the *same* class (Avg MID).

    Steps:
    1. For each point i: calculate MID(i) and NID(i).
    2. For each class C: calculate Avg_MID(C) and Avg_NID(C) over points i in C.
    3. For each class C: calculate Class_Sep(C) = (Avg_NID(C) - Avg_MID(C)) / (Avg_NID(C) + Avg_MID(C) + epsilon). Ranges [-1, 1].
    4. Calculate overall score as the weighted average of Class_Sep(C) by class size.
    5. Normalize the overall score to [0, 1] using (score + 1) / 2.

    Requires min_class_size >= 2. Requires at least 2 valid classes.
    Score close to 1 indicates good separation (AvgNID >> AvgMID consistently).
    Score close to 0 indicates poor separation (AvgMID >> AvgNID consistently).
    Score around 0.5 indicates AvgNID ≈ AvgMID on average.
    """

    def _initialize(self, min_class_size: int = 2, epsilon: float = 1e-9, **kwargs):
        """
        Initializes CSR benchmark parameters.

        Args:
            min_class_size (int): Minimum size required for a class to be included
                in the calculation. Must be >= 2.
            epsilon (float): Small value added to the denominator to prevent
                division by zero.
            **kwargs: Additional keyword arguments.
        """
        if not isinstance(min_class_size, int) or min_class_size < 2:
            logger.warning(f"min_class_size must be >= 2. Setting to 2.")
            self.min_class_size = 2
        else:
            self.min_class_size = min_class_size
        self.epsilon = epsilon
        logger.info(
            "Initialized ClassSeparationRatio (CSR) Benchmark"
            f" (min_class_size={self.min_class_size}, epsilon={self.epsilon})"
        )

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        labels: Optional[List[Any]] = None,
        distance_matrix_path: Optional[Any] = None,
        item_id_map: Optional[Dict[str, Dict]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Compute the CSR score.

        Args:
            distance_matrix (Optional[Any]): The distance matrix (numpy array or torch tensor).
                Required for this benchmark.
            labels (Optional[List[Any]]): A list of labels for each data point,
                corresponding to the rows/columns of the distance matrix. Required.
            distance_matrix_path (Optional[Any]): Path to the distance matrix file (for context, not used).
            item_id_map (Optional[Dict[str, Dict]]): Mapping of item IDs (for context, not used).
            **kwargs: Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the 'csr_score', 'avg_class_separation',
                            'num_classes_evaluated', 'num_points_evaluated', 'per_class_metrics',
                            'params', and 'error' if applicable.
        """
        start_time = time.time()
        default_return = {
            "csr_score": None,
            "avg_class_separation": None,
            "num_classes_evaluated": 0,
            "num_points_evaluated": 0,
            "per_class_metrics": {},
            "params": {"min_class_size": self.min_class_size, "epsilon": self.epsilon},
            "error": None,
        }
        dist_mat_np = None
        labels_array = None
        dist_mat_filtered = None

        if distance_matrix is None:
            default_return["error"] = "distance_matrix is required."
            return default_return
        if labels is None:
            default_return["error"] = "labels are required."
            return default_return
        try:
            if isinstance(distance_matrix, torch.Tensor):
                dist_mat_np = distance_matrix.cpu().numpy().astype(np.float32)
            elif isinstance(distance_matrix, np.ndarray):
                dist_mat_np = distance_matrix.astype(np.float32)
            else:
                dist_mat_np = np.array(distance_matrix, dtype=np.float32)
            if dist_mat_np.ndim != 2 or dist_mat_np.shape[0] != dist_mat_np.shape[1]:
                raise ValueError(f"distance_matrix must be square 2D, got shape {dist_mat_np.shape}")
            n_samples_orig = dist_mat_np.shape[0]
            if np.any(dist_mat_np < 0):
                logger.warning("Distance matrix contains negative values. Taking absolute.")
                dist_mat_np = np.abs(dist_mat_np)
            np.fill_diagonal(dist_mat_np, 0)
            if not isinstance(labels, list):
                labels = list(labels)
            if len(labels) != n_samples_orig:
                raise ValueError(f"Label count ({len(labels)}) != distance matrix dim ({n_samples_orig})")
            original_indices_with_valid_labels = [i for i, lbl in enumerate(labels) if lbl is not None]
            filtered_labels_str = [str(labels[i]) for i in original_indices_with_valid_labels]
            if not filtered_labels_str:
                raise ValueError("No valid (non-None) labels found.")
            if len(original_indices_with_valid_labels) != n_samples_orig:
                dist_mat_valid_labels = dist_mat_np[np.ix_(original_indices_with_valid_labels, original_indices_with_valid_labels)]
                labels_valid = np.array(filtered_labels_str)
            else:
                dist_mat_valid_labels = dist_mat_np
                labels_valid = np.array(filtered_labels_str)
            n_samples_valid = dist_mat_valid_labels.shape[0]
            if n_samples_valid == 0:
                raise ValueError("No samples remaining after filtering None labels.")
            unique_labels, counts = np.unique(labels_valid, return_counts=True)
            valid_classes = {label for label, count in zip(unique_labels, counts) if count >= self.min_class_size}
            if len(valid_classes) < 2:
                raise ValueError(f"Need at least 2 classes with >= {self.min_class_size} samples. Found {len(valid_classes)} valid classes.")
            final_filter_mask = np.isin(labels_valid, list(valid_classes))
            final_valid_indices = np.where(final_filter_mask)[0]
            if len(final_valid_indices) == 0:
                raise ValueError("No samples remaining after filtering by min_class_size.")
            if len(final_valid_indices) == n_samples_valid:
                dist_mat_filtered = dist_mat_valid_labels
                labels_array = labels_valid
            else:
                dist_mat_filtered = dist_mat_valid_labels[np.ix_(final_valid_indices, final_valid_indices)]
                labels_array = labels_valid[final_valid_indices]
            n_samples_final = dist_mat_filtered.shape[0]
            unique_final_labels = np.unique(labels_array)
            logger.info(f"Final dataset size for CSR calculation: {n_samples_final} points across {len(unique_final_labels)} classes.")
        except ValueError as ve:
            default_return["error"] = str(ve)
            logger.error(f"Filtering error: {ve}")
            gc.collect()
            return default_return
        except Exception as e:
            default_return["error"] = f"Filtering failed: {e}"
            logger.error(f"Filtering failed: {e}", exc_info=True)
            gc.collect()
            return default_return

        per_class_mid_sums: Dict[str, float] = {lbl: 0.0 for lbl in unique_final_labels}
        per_class_nid_sums: Dict[str, float] = {lbl: 0.0 for lbl in unique_final_labels}
        per_class_counts: Dict[str, int] = {lbl: 0 for lbl in unique_final_labels}
        evaluated_points_total = 0

        try:
            pbar = tqdm(range(n_samples_final), desc="Calculating MID/NID per point", leave=False)
            for i in pbar:
                anchor_label = labels_array[i]
                mid_i = None
                nid_i = None

                same_label_mask = labels_array == anchor_label
                same_label_mask[i] = False
                if np.any(same_label_mask):
                    dists_same = dist_mat_filtered[i, same_label_mask]
                    if dists_same.size > 0:
                        mid_i = np.max(dists_same)

                diff_label_mask = labels_array != anchor_label
                if np.any(diff_label_mask):
                    dists_diff = dist_mat_filtered[i, diff_label_mask]
                    if dists_diff.size > 0:
                        nid_i = np.min(dists_diff)

                if mid_i is not None and nid_i is not None:
                    per_class_mid_sums[anchor_label] += mid_i
                    per_class_nid_sums[anchor_label] += nid_i
                    per_class_counts[anchor_label] += 1
                    evaluated_points_total += 1
            pbar.close()

            if evaluated_points_total == 0:
                raise ValueError("No points had both MID and NID calculated.")

        except Exception as e:
            default_return["error"] = f"MID/NID calculation loop failed: {e}"
            logger.error(f"MID/NID calculation loop failed: {e}", exc_info=True)
            gc.collect()
            return default_return

        class_separation_scores = []
        class_weights = []
        per_class_metrics_detail = {}

        for class_label in unique_final_labels:
            count = per_class_counts[class_label]
            if count == 0:
                logger.warning(f"Class '{class_label}' had 0 valid points for MID/NID, skipping.")
                continue

            avg_mid = per_class_mid_sums[class_label] / count
            avg_nid = per_class_nid_sums[class_label] / count
            denominator = avg_nid + avg_mid + self.epsilon

            if denominator < self.epsilon:
                class_sep = 0.0
                logger.warning(f"Class '{class_label}': AvgNID and AvgMID are both near zero. Setting ClassSep=0.")
            else:
                class_sep = (avg_nid - avg_mid) / denominator

            class_separation_scores.append(class_sep)
            class_weights.append(count)
            per_class_metrics_detail[class_label] = {
                "avg_mid": float(avg_mid),
                "avg_nid": float(avg_nid),
                "class_sep": float(class_sep),
                "count": count,
            }

        if not class_separation_scores:
            default_return["error"] = "No classes had valid separation scores."
            logger.error("CSR: No class separation scores could be calculated.")
            gc.collect()
            return default_return

        weighted_avg_class_sep = np.average(class_separation_scores, weights=class_weights)

        csr_score = (weighted_avg_class_sep + 1.0) / 2.0

        default_return["csr_score"] = float(csr_score)
        default_return["avg_class_separation"] = float(weighted_avg_class_sep)
        default_return["num_classes_evaluated"] = len(class_separation_scores)
        default_return["num_points_evaluated"] = evaluated_points_total
        default_return["per_class_metrics"] = per_class_metrics_detail

        logger.info(f"CSR Score: {csr_score:.4f}")
        logger.info(f"  Avg Class Separation (Weighted, -1 to 1): {weighted_avg_class_sep:.4f}")
        logger.info(f"  Evaluated {evaluated_points_total} points across {len(class_separation_scores)} classes.")

        elapsed_time = time.time() - start_time
        logger.info(f"CSR benchmark finished in {elapsed_time:.2f}s")
        gc.collect()
        return default_returnimport logging
from typing import Any, Dict, List, Optional
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import gc
import h5py

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"
DEFAULT_MEMORY_LIMIT_GB = 4.0


class FValueBenchmark(Benchmark):
    """
    Computes the F-Value, averaged.
    For each ordered pair of distinct valid classes (C_i, C_j):
    1. Calculate AvgIntra(C_i).
    2. Calculate AvgInter(C_i, C_j).
    3. F_orig_ij = AvgInter(C_i, C_j) / AvgIntra(C_i).
    4. F_transformed_ij = 1 / (1 + F_orig_ij).
    The final reported F-value is the mean of these F_transformed_ij values
    over all M * (M-1) ordered pairs.
    Result is in [0, 1], where 0 indicates better separation on average.
    Operates directly on the provided distance matrix.
    """

    def _initialize(self, memory_limit_gb: float = DEFAULT_MEMORY_LIMIT_GB, **kwargs):
        """
        Initializes the Pairwise F-Value benchmark parameters.

        Args:
            memory_limit_gb (float): The maximum amount of memory in GB allowed
                for loading the distance matrix. If the matrix size exceeds this,
                the benchmark will be skipped.
            **kwargs: Additional keyword arguments.
        """
        self.memory_limit_gb = memory_limit_gb
        logger.info(f"Initialized PairwiseFValueBenchmark with Memory Limit: {self.memory_limit_gb} GB")

    def _estimate_memory_gb(self, num_samples: int) -> float:
        """Estimates the memory required for a square float32 matrix."""
        bytes_needed = num_samples * num_samples * 4
        return bytes_needed / (1024**3)

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        distance_matrix_path: Optional[Path] = None,
        labels: Optional[List[Any]] = None,
        min_class_size: int = 2,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Computes the mean pairwise transformed F-value.

        Args:
            distance_matrix (Optional[Any]): The distance matrix (numpy array or torch tensor).
                If None, distance_matrix_path must be provided and loadable within memory_limit_gb.
            distance_matrix_path (Optional[Path]): Path to an HDF5 file containing the distance matrix.
                Used if distance_matrix is None.
            labels (Optional[List[Any]]): A list of labels for each data point,
                corresponding to the rows/columns of the distance matrix. Required.
            min_class_size (int): Minimum size required for a class to be included
                in the calculation. Must be >= 2.
            **kwargs: Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the 'pairwise_f_value' and
                            an error message if applicable ('error').
        """
        default_return = {"pairwise_f_value": None, "error": None}
        raw_dist_mat_input = distance_matrix
        dist_mat_tensor_for_calc = None
        n_samples_orig = 0

        if raw_dist_mat_input is None:
            if distance_matrix_path is None or not distance_matrix_path.exists():
                default_return["error"] = "Dist matrix path missing/invalid."
                return default_return
            try:
                with h5py.File(distance_matrix_path, "r") as f:
                    if HDF5_DISTANCE_DATASET_NAME not in f:
                        raise ValueError("HDF5 dataset name not found")
                    dset = f[HDF5_DISTANCE_DATASET_NAME]
                    matrix_shape = dset.shape
                    if len(matrix_shape) != 2 or matrix_shape[0] != matrix_shape[1]:
                        raise ValueError("Matrix not square 2D")
                    n_samples_orig = matrix_shape[0]
                estimated_gb = self._estimate_memory_gb(n_samples_orig)
                if estimated_gb > self.memory_limit_gb:
                    error_msg = f"Skipped: Est. memory ({estimated_gb:.2f}GB) > limit ({self.memory_limit_gb:.2f}GB)"
                    default_return["error"] = error_msg
                    return default_return
            except Exception as e:
                default_return["error"] = f"Failed read dist matrix meta: {e}"
                return default_return
            logger.warning("PairwiseFValue: distance_matrix not provided directly. Attempting load from path.")
            try:
                with h5py.File(distance_matrix_path, "r") as f:
                    raw_dist_mat_input = f[HDF5_DISTANCE_DATASET_NAME][()]
            except Exception as e:
                default_return["error"] = f"Failed to load dist matrix from path: {e}"
                return default_return

        elif isinstance(raw_dist_mat_input, (np.ndarray, torch.Tensor)):
            if raw_dist_mat_input.ndim != 2 or raw_dist_mat_input.shape[0] != raw_dist_mat_input.shape[1]:
                default_return["error"] = f"Passed distance_matrix not square 2D: {raw_dist_mat_input.shape}"
                return default_return
            n_samples_orig = raw_dist_mat_input.shape[0]
            estimated_gb = self._estimate_memory_gb(n_samples_orig)
            if estimated_gb > self.memory_limit_gb:
                default_return["error"] = f"Passed matrix too large ({estimated_gb:.2f}GB > limit)."
                return default_return
        else:
            default_return["error"] = "Invalid distance_matrix type."
            return default_return

        if labels is None:
            default_return["error"] = "Labels missing."
            return default_return
        if not isinstance(labels, list):
            try:
                labels = list(labels)
            except TypeError:
                default_return["error"] = f"Labels must be list, got {type(labels)}."
                return default_return
        if len(labels) != n_samples_orig:
            logger.warning(f"Label count {len(labels)} != matrix dim {n_samples_orig}. Adjusting.")
            labels = labels[:n_samples_orig] if len(labels) > n_samples_orig else labels + [None] * (n_samples_orig - len(labels))

        try:
            if isinstance(raw_dist_mat_input, np.ndarray):
                dist_mat_tensor_for_calc = torch.from_numpy(raw_dist_mat_input).float()
            elif isinstance(raw_dist_mat_input, torch.Tensor):
                dist_mat_tensor_for_calc = raw_dist_mat_input.float()
            else:
                raise TypeError(f"Unsupported matrix type: {type(raw_dist_mat_input)}")
            if dist_mat_tensor_for_calc.shape[0] != n_samples_orig:
                raise ValueError(f"Matrix shape {dist_mat_tensor_for_calc.shape} mismatch")
            dist_mat_tensor_for_calc.fill_diagonal_(0)
        except Exception as e:
            default_return["error"] = f"Dist matrix processing failed: {e}"
            gc.collect()
            return default_return

        original_indices_with_valid_labels = [i for i, lbl in enumerate(labels) if lbl is not None and 0 <= i < n_samples_orig]
        filtered_labels = [str(labels[i]) for i in original_indices_with_valid_labels]
        if not filtered_labels:
            default_return["error"] = "No valid labels found."
            gc.collect()
            return default_return

        if len(original_indices_with_valid_labels) != n_samples_orig:
            try:
                valid_torch_indices = torch.tensor(original_indices_with_valid_labels, dtype=torch.long, device=dist_mat_tensor_for_calc.device)
                dist_mat_tensor_for_calc = dist_mat_tensor_for_calc[valid_torch_indices][:, valid_torch_indices]
            except Exception as e:
                default_return["error"] = f"Error filtering dist matrix: {e}"
                gc.collect()
                return default_return

        n_samples_valid = dist_mat_tensor_for_calc.shape[0]
        if n_samples_valid != len(filtered_labels):
            default_return["error"] = "Internal filter mismatch."
            gc.collect()
            return default_return

        results = default_return.copy()
        try:
            unique_label_list = sorted(list(set(filtered_labels)))
            label_to_id_map = {lbl: i for i, lbl in enumerate(unique_label_list)}
            labels_numeric = np.array([label_to_id_map[lbl] for lbl in filtered_labels], dtype=int)

            valid_class_data = {}
            all_numeric_ids = np.unique(labels_numeric)

            for num_label_id in all_numeric_ids:
                indices_this_class = np.where(labels_numeric == num_label_id)[0]
                if len(indices_this_class) >= min_class_size:
                    intra_class_block = dist_mat_tensor_for_calc[indices_this_class][:, indices_this_class]
                    if len(indices_this_class) > 1:
                        intra_mask = ~torch.eye(len(indices_this_class), dtype=torch.bool, device=intra_class_block.device)
                        valid_intra_distances = intra_class_block[intra_mask]
                        if valid_intra_distances.numel() > 0:
                            avg_intra_dist = torch.mean(valid_intra_distances).item()
                            valid_class_data[num_label_id] = {
                                "indices": torch.tensor(indices_this_class, device=dist_mat_tensor_for_calc.device),
                                "avg_intra_dist": avg_intra_dist,
                            }

            valid_class_ids = list(valid_class_data.keys())
            num_valid_classes = len(valid_class_ids)

            if num_valid_classes < 2:
                results["error"] = f"Need >= 2 classes (size >= {min_class_size}) for Pairwise F-value."
                return results

            logger.info(f"Calculating Pairwise F-values for {num_valid_classes} valid classes.")

            all_pairwise_transformed_f_values = []
            epsilon = 1e-9

            pbar_outer = tqdm(range(num_valid_classes), desc="Pairwise F-Value (Class i)", leave=False)
            for i_idx in pbar_outer:
                class_id_i = valid_class_ids[i_idx]
                data_i = valid_class_data[class_id_i]
                indices_i = data_i["indices"]
                avg_intra_dist_i = data_i["avg_intra_dist"]

                for j_idx in range(num_valid_classes):
                    if i_idx == j_idx:
                        continue

                    class_id_j = valid_class_ids[j_idx]
                    data_j = valid_class_data[class_id_j]
                    indices_j = data_j["indices"]

                    inter_block_ij = dist_mat_tensor_for_calc[indices_i][:, indices_j]
                    if inter_block_ij.numel() == 0:
                        logger.warning(f"Empty inter-block C{class_id_i}-C{class_id_j}. Skipping.")
                        continue
                    avg_inter_dist_ij = torch.mean(inter_block_ij).item()

                    f_orig_ij: Optional[float] = None
                    f_transformed_ij: Optional[float] = None

                    if abs(avg_intra_dist_i) < epsilon:
                        if abs(avg_inter_dist_ij) >= epsilon:
                            f_orig_ij = float("inf")
                            f_transformed_ij = 0.0
                        else:
                            f_orig_ij = 1.0
                            f_transformed_ij = 0.5
                    elif abs(avg_inter_dist_ij) < epsilon and abs(avg_intra_dist_i) >= epsilon:
                        f_orig_ij = 0.0
                        f_transformed_ij = 1.0
                    else:
                        f_orig_ij = avg_inter_dist_ij / avg_intra_dist_i
                        f_transformed_ij = 1.0 / (1.0 + f_orig_ij)

                    all_pairwise_transformed_f_values.append(f_transformed_ij)
            pbar_outer.close()

            if not all_pairwise_transformed_f_values:
                results["error"] = "No pairwise F-values could be calculated."
            else:
                final_pairwise_f_value = np.mean(all_pairwise_transformed_f_values)
                logger.info(
                    "Mean Pairwise Transformed F-Value (0=best):"
                    f" {final_pairwise_f_value:.4f} (from {len(all_pairwise_transformed_f_values)} pairs)"
                )
                results = {"pairwise_f_value": final_pairwise_f_value, "error": None}

        except Exception as e:
            logger.error(f"Pairwise F-Value calculation failed: {e}", exc_info=True)
            results["error"] = str(e)
        finally:
            del dist_mat_tensor_for_calc
            gc.collect()
            if torch.cuda.is_available():
                try:
                    torch.cuda.empty_cache()
                except Exception:
                    pass
        return results# In benchmarks/gsr.py

import logging
import time
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
import torch
from tqdm.auto import tqdm
import gc

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)


class GlobalSeparationRate(Benchmark):
    """
    Computes a point-wise Global Separation Rate (GSR) to measure class integrity.

    This metric provides a robust, instance-level measure of class separability.
    For each data point, it calculates a "local separation score" by comparing two
    key distances:
    1.  NID (Nearest Inter-class Distance): The distance to the closest point
        belonging to any *other* class.
    2.  Avg_ID (Average Intra-class Distance): The average distance to all
        other points within its *own* class.

    The local score for each point `i` is a ratio that quantifies its separation:
    Local_Score(i) = (NID(i) - Avg_ID(i)) / (NID(i) + Avg_ID(i) + epsilon)

    The final GSR score is the average of these local scores over all evaluable
    points, normalized to a [0, 1] range. A score of 1.0 indicates excellent
    separation, while a score of 0.5 suggests that inter-class and intra-class
    distances are, on average, equal for most points.
    """

    def _initialize(self, min_class_size: int = 2, epsilon: float = 1e-9, **kwargs):
        """
        Initializes the GlobalSeparationRate benchmark parameters.

        Args:
            min_class_size (int): The minimum number of samples a class must have
                to be included in the evaluation. Must be at least 2.
            epsilon (float): A small constant to prevent division by zero in the
                local score calculation.
            **kwargs: Additional keyword arguments, including 'device'.
        """
        if not isinstance(min_class_size, int) or min_class_size < 2:
            logger.warning(f"min_class_size must be >= 2. Setting to 2.")
            self.min_class_size = 2
        else:
            self.min_class_size = min_class_size
        self.epsilon = epsilon

        # FIX: The device needs to be initialized from the kwargs passed by the manager.
        device_str = kwargs.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self.device = torch.device(device_str)
        
        logger.info(
            "Initialized GlobalSeparationRate (GSR) Benchmark "
            f"(min_class_size={self.min_class_size}, epsilon={self.epsilon}, device={self.device})"
        )

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        labels: Optional[List[Any]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Computes the robust GSR score from a distance matrix and corresponding labels.

        Args:
            distance_matrix (Optional[Any]): A pre-computed square distance matrix
                (numpy array or torch tensor).
            labels (Optional[List[Any]]): A list of class labels corresponding to the
                rows/columns of the distance matrix.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            A dictionary containing the final normalized 'gsr_score', a 'raw_gsr_score'
            in the [-1, 1] range, the number of points evaluated, and an error
            message if applicable.
        """
        start_time = time.time()
        default_return = {"gsr_score": None, "raw_gsr_score": None, "error": None}

        # --- 1. Input Validation and Data Preparation ---
        if distance_matrix is None:
            return {**default_return, "error": "distance_matrix is required."}
        if labels is None:
            return {**default_return, "error": "labels are required."}
        
        try:
            dist_mat = torch.from_numpy(distance_matrix).float() if isinstance(distance_matrix, np.ndarray) else distance_matrix.float()
            
            if dist_mat.ndim != 2 or dist_mat.shape[0] != dist_mat.shape[1]:
                raise ValueError(f"Distance matrix must be a square 2D tensor, got {dist_mat.shape}")

            n_samples_orig = dist_mat.shape[0]
            if len(labels) != n_samples_orig:
                raise ValueError(f"Label count ({len(labels)}) does not match matrix dimension ({n_samples_orig})")

            # Filter out samples with None labels
            valid_indices_mask = np.array([lbl is not None for lbl in labels])
            if not np.any(valid_indices_mask):
                raise ValueError("No valid (non-None) labels found.")
            labels_valid_str = np.array([str(lbl) for lbl in labels])[valid_indices_mask]
            
            # Identify classes large enough for evaluation
            unique_labels, counts = np.unique(labels_valid_str, return_counts=True)
            valid_classes = {label for label, count in zip(unique_labels, counts) if count >= self.min_class_size}

            if len(valid_classes) < 2:
                raise ValueError(f"GSR requires at least 2 classes with size >= {self.min_class_size} samples. Found {len(valid_classes)} valid classes from {len(unique_labels)} total.")

            # Filter the distance matrix and labels to only include valid classes
            final_filter_mask_in_valid = np.isin(labels_valid_str, list(valid_classes))
            original_indices_where_valid = np.where(valid_indices_mask)[0]
            final_indices = original_indices_where_valid[final_filter_mask_in_valid]

            if len(final_indices) == 0:
                raise ValueError("No samples remaining after filtering for min_class_size.")

            dist_mat = dist_mat[final_indices][:, final_indices]
            labels_array = np.array(labels)[final_indices]
            
            # Factorize string labels to integer IDs for efficient tensor operations
            labels_tensor = torch.tensor(pd.factorize(labels_array, sort=True)[0])
            n_samples_final = dist_mat.shape[0]
            
            # Move data to the specified evaluation device (e.g., CUDA)
            dist_mat = dist_mat.to(self.device)
            labels_tensor = labels_tensor.to(self.device)

        except ValueError as ve:
            return {**default_return, "error": str(ve)}
        except Exception as e:
            return {**default_return, "error": f"Data preparation failed: {e}"}

        # --- 2. Core Point-wise Score Calculation ---
        local_separation_scores = []
        pbar = tqdm(range(n_samples_final), desc="Calculating GSR", leave=False)

        for i in pbar:
            anchor_label_id = labels_tensor[i]
            distances_from_i = dist_mat[i]

            # Create boolean masks to identify intra-class and inter-class neighbors
            same_label_mask = (labels_tensor == anchor_label_id)
            same_label_mask[i] = False  # Exclude the point itself from its own neighbors
            diff_label_mask = (labels_tensor != anchor_label_id)

            # A point can only be evaluated if it has at least one intra-class neighbor
            # and at least one inter-class point exists in the dataset.
            if not torch.any(same_label_mask) or not torch.any(diff_label_mask):
                continue

            # Calculate Average Intra-class Distance (Avg_ID) for this point
            avg_id = torch.mean(distances_from_i[same_label_mask])

            # Calculate Nearest Inter-class Distance (NID) for this point
            nid = torch.min(distances_from_i[diff_label_mask])

            # Calculate the local separation score
            denominator = nid + avg_id + self.epsilon
            if denominator > self.epsilon:
                local_score = (nid - avg_id) / denominator
                local_separation_scores.append(local_score.item())

        pbar.close()

        if not local_separation_scores:
            return {**default_return, "error": "No points could be evaluated."}

        # --- 3. Final Score Aggregation ---
        # The raw score is the average of all local scores (ranges from -1 to 1)
        raw_gsr = np.mean(local_separation_scores)
        
        # Normalize the score to a [0, 1] range for intuitive interpretation in tables
        gsr_score_normalized = (raw_gsr + 1.0) / 2.0

        elapsed_time = time.time() - start_time
        logger.info(f"GSR benchmark finished in {elapsed_time:.2f}s")
        gc.collect()

        return {
            "gsr_score": float(gsr_score_normalized),
            "raw_gsr_score": float(raw_gsr),
            "num_points_evaluated": len(local_separation_scores),
            "error": None,
        }"""
Benchmark for evaluating embedding alignment with avian perceptual judgments.

This module defines the PerceptualAlignment benchmark, which assesses how well
the distance metric of a given feature embedding aligns with zebra finch
perceptual judgments collected using probe (AXB) and derived triplet tasks.

Reference:
  Zandberg L, Morfi V, George JM, Clayton DF, Stowell D, Lachlan RF (2024)
  Bird song comparison using deep learning trained from avian perceptual judgments.
  PLOS Computational Biology 20(8): e1012329.
  https://doi.org/10.1371/journal.pcbi.1012329
  Dataset: https://doi.org/10.5281/zenodo.5545872
"""

import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
import torch
import h5py
import gc
from scipy.stats import binomtest, kendalltau, spearmanr
from tqdm import tqdm
from sklearn.utils import resample

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"


class PerceptualAlignment(Benchmark):
    """
    Evaluates alignment with perceptual judgments using probe and triplet tasks.
    """

    def _initialize(
        self,
        probe_csv_path: str,
        triplet_csv_path: str,
        probe_consistency_threshold: Optional[float] = 0.7,
        bootstrap_ci: bool = True,
        n_bootstraps: int = 1000,
        **kwargs,
    ):
        """
        Initializes PerceptualAlignment benchmark parameters.

        Returns:
            None
        """
        self.probe_csv_path_str = probe_csv_path
        self.triplet_csv_path_str = triplet_csv_path
        self.probe_consistency_threshold = probe_consistency_threshold
        self.bootstrap_ci = bootstrap_ci
        self.n_bootstraps = n_bootstraps
        self.probe_csv_path: Optional[Path] = None
        self.triplet_csv_path: Optional[Path] = None
        logger.info("Initialized PerceptualAlignment config:")
        logger.info(f"  Probes: {self.probe_csv_path_str}")
        logger.info(f"  Triplets: {self.triplet_csv_path_str} (will evaluate 'all' and 'high' margin types)")
        logger.info(f"  Probe Consistency Threshold: {self.probe_consistency_threshold}")
        logger.info(f"  Bootstrap CI: {self.bootstrap_ci}, N_Bootstraps: {self.n_bootstraps}")

    def _load_and_prepare_data(
        self, item_id_map: Optional[Dict[str, Dict]] = None
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, int]]:
        """
        Loads perceptual data, performs filtering, creates name-to-index map.

        Returns:
            Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, int]]: Raw probes dataframe,
            filtered probes dataframe, raw triplets dataframe, and name-to-index map.
        """
        if self.probe_csv_path is None or self.triplet_csv_path is None:
            raise ValueError("Probe/Triplet CSV paths must be set before calling _load_and_prepare_data.")
        if not self.probe_csv_path.exists():
            raise FileNotFoundError(f"Probe CSV not found: {self.probe_csv_path}")
        if not self.triplet_csv_path.exists():
            raise FileNotFoundError(f"Triplet CSV not found: {self.triplet_csv_path}")
        if not item_id_map:
            raise ValueError("item_id_map is required to map names to matrix indices.")

        try:
            probes_df = pd.read_csv(self.probe_csv_path)
            required_probe_cols = {"sound_id", "left", "right", "decision"}
            if not required_probe_cols.issubset(probes_df.columns):
                missing = required_probe_cols - set(probes_df.columns)
                raise ValueError(f"Probe CSV missing columns: {missing}")
            for col in required_probe_cols:
                probes_df[col] = probes_df[col].astype(str)
            logger.info(f"Loaded {len(probes_df)} probe judgments from {self.probe_csv_path.name}.")
        except Exception as e:
            logger.error("Failed to load probe CSV file: %s", e, exc_info=True)
            raise

        triplet_required_cols = {"Anchor", "Positive", "Negative", "Margin_Type"}
        try:
            triplets_df_full = pd.read_csv(self.triplet_csv_path, usecols=lambda c: c in triplet_required_cols)
            if not triplet_required_cols.issubset(triplets_df_full.columns):
                missing_cols = triplet_required_cols - set(triplets_df_full.columns)
                raise ValueError(f"Triplet CSV missing required columns: {missing_cols}")
            for col in triplet_required_cols:
                triplets_df_full[col] = triplets_df_full[col].astype(str)
            logger.info(f"Loaded {len(triplets_df_full)} derived triplet entries from {self.triplet_csv_path.name}.")
        except Exception as e:
            logger.error("Failed to load or process triplet CSV file: %s", e, exc_info=True)
            raise

        name_to_idx: Dict[str, int] = {}
        map_build_errors = []
        logger.info("Building name_to_idx from item_id_map...")
        found_indices = set()
        for item_id, meta in item_id_map.items():
            try:
                original_name = meta.get("original_name")
                index = meta.get("index")
                if original_name is not None and index is not None:
                    original_name_str = str(original_name)
                    current_index = int(index)
                    if original_name_str in name_to_idx and name_to_idx[original_name_str] != current_index:
                        logger.warning(
                            "Duplicate original_name '%s' in item_id_map with different indices. Using first found (%s).",
                            original_name_str,
                            name_to_idx[original_name_str],
                        )
                    elif original_name_str not in name_to_idx:
                        name_to_idx[original_name_str] = current_index
                        found_indices.add(current_index)
                else:
                    map_build_errors.append(f"{item_id}:missing_keys")
            except (TypeError, ValueError) as e:
                map_build_errors.append(f"{item_id}:error({e})")

        if map_build_errors:
            logger.warning("%d items had issues during name_to_idx build: %s...", len(map_build_errors), map_build_errors[:5])
        if not name_to_idx:
            raise ValueError("Failed to build any name_to_idx entries.")
        logger.info(f"Built name_to_idx map covering {len(name_to_idx)} unique names and {len(found_indices)} unique indices.")
        self.max_index_from_map = max(found_indices) if found_indices else -1

        initial_triplet_count = len(triplets_df_full)
        triplets_df_full.dropna(subset=["Anchor", "Positive", "Negative"], inplace=True)
        dropped_nan_count = initial_triplet_count - len(triplets_df_full)
        if dropped_nan_count > 0:
            logger.info(f"Dropped {dropped_nan_count} triplets with missing A/P/N.")
        triplets_df_full = triplets_df_full.drop_duplicates(subset=["Anchor", "Positive", "Negative"], keep="first")
        dropped_dup_count = (initial_triplet_count - dropped_nan_count) - len(triplets_df_full)
        if dropped_dup_count > 0:
            logger.warning("Removed %d duplicate triplets (A,P,N). Kept first.", dropped_dup_count)
        logger.info(f"Triplet processing complete. Keeping {len(triplets_df_full)} unique triplets.")

        filtered_probes_df = probes_df.copy()
        initial_probe_count = len(probes_df)
        probes_df.dropna(subset=["sound_id", "left", "right", "decision"], inplace=True)
        if len(probes_df) < initial_probe_count:
            logger.info("Dropped %d probes with missing values.", initial_probe_count - len(probes_df))
        probes_df["decision_lower"] = probes_df["decision"].str.lower()
        probes_df = probes_df[probes_df["decision_lower"].isin(["left", "right"])].copy()
        if self.probe_consistency_threshold is not None:
            logger.info("Applying probe consistency filter (threshold: %s)...", self.probe_consistency_threshold)
            decision_counts = probes_df.groupby("sound_id")["decision_lower"].value_counts().unstack(fill_value=0)
            if "left" not in decision_counts.columns:
                decision_counts["left"] = 0
            if "right" not in decision_counts.columns:
                decision_counts["right"] = 0
            decision_counts["total"] = decision_counts["left"] + decision_counts["right"]
            decision_counts["majority_choice"] = decision_counts.apply(
                lambda row: "left" if row["left"] >= row["right"] else "right", axis=1
            )
            decision_counts["majority_count"] = decision_counts.apply(lambda row: row[row["majority_choice"]], axis=1)
            decision_counts["consistency"] = decision_counts["majority_count"] / decision_counts["total"]
            consistent_sound_ids = decision_counts[decision_counts["consistency"] >= self.probe_consistency_threshold].index
            logger.info("Found %d sound IDs meeting consistency threshold.", len(consistent_sound_ids))
            probes_df_merged = probes_df.merge(decision_counts[["majority_choice"]], left_on="sound_id", right_index=True)
            filtered_probes_df = probes_df_merged[
                (probes_df_merged["sound_id"].isin(consistent_sound_ids))
                & (probes_df_merged["decision_lower"] == probes_df_merged["majority_choice"])
            ].copy()
            filtered_probes_df = filtered_probes_df.drop_duplicates(subset=["sound_id"], keep="first")
            logger.info("Filtered probes based on consistency. Kept %d probes.", len(filtered_probes_df))
        else:
            logger.info("Skipping probe consistency filter.")
            filtered_probes_df = probes_df.drop_duplicates(subset=["sound_id"], keep="first").copy()
            logger.info("Filtered probes (example: unique sound_id). Kept %d probes.", len(filtered_probes_df))

        probes_df.drop(columns=["decision_lower"], inplace=True, errors="ignore")
        if "decision_lower" in filtered_probes_df.columns:
            filtered_probes_df.drop(columns=["decision_lower"], inplace=True, errors="ignore")
        if "majority_choice" in filtered_probes_df.columns:
            filtered_probes_df.drop(columns=["majority_choice"], inplace=True, errors="ignore")

        logger.info(f"Data loading complete. Probes: {len(probes_df)} raw, {len(filtered_probes_df)} filtered. Triplets: {len(triplets_df_full)} total unique.")
        return probes_df, filtered_probes_df, triplets_df_full, name_to_idx

    def _compute_correlations(self, model_dists: List[float], bird_dists: List[float], task_name: str) -> Dict:
        """
        Computes Spearman and Kendall correlations with optional CIs.

        Returns:
            Dict: Dictionary containing correlation results and related metrics.
        """
        results = {
            f"{task_name}_spearman_rho": None,
            f"{task_name}_spearman_p": None,
            f"{task_name}_spearman_ci95": [None, None],
            f"{task_name}_kendall_tau": None,
            f"{task_name}_kendall_p": None,
            f"{task_name}_kendall_ci95": [None, None],
            f"{task_name}_num_comparisons": 0,
        }
        if not model_dists or not bird_dists or len(model_dists) != len(bird_dists):
            logger.warning("Cannot compute correlations for %s: Invalid input lists.", task_name)
            return results
        try:
            model_dists_arr = np.array(model_dists, dtype=np.float64)
            bird_dists_arr = np.array(bird_dists, dtype=np.float64)
            valid_mask = np.isfinite(model_dists_arr) & np.isfinite(bird_dists_arr)
            valid_count = int(np.sum(valid_mask))
            if valid_count < 2:
                logger.warning("Cannot compute correlations for %s: < 2 valid pairs (%d).", task_name, valid_count)
                results[f"{task_name}_num_comparisons"] = valid_count
                return results

            model_dists_valid = model_dists_arr[valid_mask]
            bird_dists_valid = bird_dists_arr[valid_mask]
            results[f"{task_name}_num_comparisons"] = valid_count
            std_model = np.std(model_dists_valid)
            std_bird = np.std(bird_dists_valid)
            variance_threshold = 1e-9
            if std_model < variance_threshold or std_bird < variance_threshold:
                logger.warning(
                    "Cannot compute correlations for %s: Zero variance detected (std_model=%.2e, std_bird=%.2e).",
                    task_name,
                    std_model,
                    std_bird,
                )
                for k in results:
                    results[k] = np.nan if not isinstance(results[k], list) else [np.nan, np.nan]
                results[f"{task_name}_num_comparisons"] = valid_count
                return results

            try:
                rho_s, p_s = spearmanr(model_dists_valid, bird_dists_valid)
                results[f"{task_name}_spearman_rho"] = float(rho_s) if np.isfinite(rho_s) else None
                results[f"{task_name}_spearman_p"] = float(p_s) if np.isfinite(p_s) else None
            except Exception as e:
                logger.error("Spearman failed for %s: %s", task_name, e)
            try:
                rho_k, p_k = kendalltau(model_dists_valid, bird_dists_valid, variant="b")
                results[f"{task_name}_kendall_tau"] = float(rho_k) if np.isfinite(rho_k) else None
                results[f"{task_name}_kendall_p"] = float(p_k) if np.isfinite(p_k) else None
            except Exception as e:
                logger.error("Kendall failed for %s: %s", task_name, e)

            if self.bootstrap_ci and valid_count >= 10:
                spear_boot_rhos, kend_boot_taus = [], []
                logger.debug("Running %d bootstrap samples for %s correlations...", self.n_bootstraps, task_name)
                indices = np.arange(valid_count)
                for i in range(self.n_bootstraps):
                    try:
                        boot_idx = resample(indices, replace=True, n_samples=valid_count, random_state=i)
                        if len(boot_idx) < 2:
                            continue
                        boot_model = model_dists_valid[boot_idx]
                        boot_bird = bird_dists_valid[boot_idx]
                        if np.std(boot_model) < variance_threshold or np.std(boot_bird) < variance_threshold:
                            continue
                        try:
                            rho_s_boot, _ = spearmanr(boot_model, boot_bird)
                            spear_boot_rhos.append(rho_s_boot) if np.isfinite(rho_s_boot) else None
                        except ValueError:
                            pass
                        try:
                            rho_k_boot, _ = kendalltau(boot_model, boot_bird, variant="b")
                            kend_boot_taus.append(rho_k_boot) if np.isfinite(rho_k_boot) else None
                        except ValueError:
                            pass
                    except Exception as boot_err:
                        logger.warning("Bootstrap sample %d error for %s: %s", i, task_name, boot_err)
                if spear_boot_rhos:
                    try:
                        ci_s = np.percentile(spear_boot_rhos, [2.5, 97.5])
                        results[f"{task_name}_spearman_ci95"] = [float(x) for x in ci_s] if np.all(np.isfinite(ci_s)) else [np.nan, np.nan]
                    except Exception as ci_err:
                        logger.error("Spearman CI failed for %s: %s", task_name, ci_err)
                else:
                    logger.warning("No valid Spearman rhos for CI in %s.", task_name)
                if kend_boot_taus:
                    try:
                        ci_k = np.percentile(kend_boot_taus, [2.5, 97.5])
                        results[f"{task_name}_kendall_ci95"] = [float(x) for x in ci_k] if np.all(np.isfinite(ci_k)) else [np.nan, np.nan]
                    except Exception as ci_err:
                        logger.error("Kendall CI failed for %s: %s", task_name, ci_err)
                else:
                    logger.warning("No valid Kendall taus for CI in %s.", task_name)
            elif self.bootstrap_ci and valid_count < 10:
                logger.warning("Skipping bootstrap CI for %s: Need >= 10 valid points.", task_name)
        except Exception as e:
            logger.error("Correlation calculation failed for %s: %s", task_name, e, exc_info=True)
            num_comp = results[f"{task_name}_num_comparisons"]
            for k in results:
                results[k] = np.nan if not isinstance(results[k], list) else [np.nan, np.nan]
            results[f"{task_name}_num_comparisons"] = num_comp
        return results

    def evaluate(
        self,
        *,
        distance_matrix_path: Optional[Path] = None,
        feature_hdf5_path: Optional[Path] = None,
        distance_matrix: Optional[Any] = None,
        features: Optional[Any] = None,
        dataset: Optional[Any] = None,
        labels: Optional[List[Any]] = None,
        item_id_map: Optional[Dict[str, Dict]] = None,
        probe_csv_path: Optional[str] = None,
        triplet_csv_path: Optional[str] = None,
        feature_config: Optional[Dict[str, Any]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Evaluates alignment using distances read selectively from HDF5.

        Returns:
            Dictionary containing evaluation results.
        """
        logger.info("Starting PerceptualAlignment evaluation (HDF5 Mode)...")
        final_results = {}
        h5_file = None

        if item_id_map is None:
            return {"error": "item_id_map is required."}

        current_probe_path = Path(probe_csv_path if probe_csv_path else self.probe_csv_path_str)
        current_triplet_path = Path(triplet_csv_path if triplet_csv_path else self.triplet_csv_path_str)
        self.probe_csv_path = current_probe_path
        self.triplet_csv_path = current_triplet_path
        if not current_probe_path.is_file():
            return {"error": f"Probe CSV not found: {current_probe_path}"}
        if not current_triplet_path.is_file():
            return {"error": f"Triplet CSV not found: {current_triplet_path}"}

        dist_mat = None
        dist_mat_source_desc = "N/A"
        if distance_matrix is not None:
            logger.warning("Using directly provided distance_matrix instead of HDF5 path.")
            dist_mat = distance_matrix
            dist_mat_source_desc = "direct_matrix"
            if not isinstance(dist_mat, np.ndarray):
                try:
                    dist_mat = np.array(dist_mat, dtype=np.float32)
                except Exception as e:
                    return {"error": f"Could not convert direct distance_matrix to numpy: {e}"}
        elif distance_matrix_path is not None and distance_matrix_path.exists():
            dist_mat_source_desc = f"hdf5:{distance_matrix_path.name}"
        else:
            return {"error": "distance_matrix_path is required and was not found or provided."}

        try:
            probes_df, filtered_probes_df, triplets_df_full, name_to_idx = self._load_and_prepare_data(item_id_map=item_id_map)
            max_allowable_index = self.max_index_from_map
            if max_allowable_index < 0:
                raise ValueError("Could not determine max index from item_id_map.")
        except Exception as e:
            logger.error("Failed to load/prepare perceptual data: %s", e, exc_info=True)
            return {"error": f"Failed to load/prepare perceptual data: {e}"}

        h5_dset = None
        if dist_mat is None:
            try:
                h5_file = h5py.File(distance_matrix_path, "r")
                if HDF5_DISTANCE_DATASET_NAME not in h5_file:
                    raise ValueError(f"Dataset '{HDF5_DISTANCE_DATASET_NAME}' not found in HDF5 file.")
                h5_dset = h5_file[HDF5_DISTANCE_DATASET_NAME]
                if h5_dset.shape[0] <= max_allowable_index or h5_dset.shape[1] <= max_allowable_index:
                    raise ValueError(
                        "HDF5 matrix shape %s is too small for max index %s found in item_id_map.",
                        h5_dset.shape,
                        max_allowable_index,
                    )
                logger.info(f"Opened distance HDF5 dataset. Shape: {h5_dset.shape}")
            except Exception as e:
                logger.error("Failed to open or validate HDF5 distance matrix file %s: %s", distance_matrix_path, e, exc_info=True)
                if h5_file:
                    h5_file.close()
                return {"error": f"Failed to open/validate HDF5 distance matrix: {e}"}

        triplet_results = {}
        probe_results = {}
        correlation_results = {}

        def get_distance(idx1, idx2):
            if dist_mat is not None:
                return float(dist_mat[idx1, idx2])
            elif h5_dset is not None:
                if not (0 <= idx1 < h5_dset.shape[0] and 0 <= idx2 < h5_dset.shape[1]):
                    logger.warning("Index out of bounds requested (%d, %d) for HDF5 shape %s", idx1, idx2, h5_dset.shape)
                    return np.nan
                return float(h5_dset[idx1, idx2])
            else:
                raise RuntimeError("No distance data source available (direct matrix or HDF5).")

        logger.info("--- Evaluating Triplet Task ---")
        for margin_type_eval in ["all", "high"]:
            if margin_type_eval == "all":
                current_triplets_df = triplets_df_full
            elif margin_type_eval == "high":
                if "Margin_Type" not in triplets_df_full.columns:
                    logger.error("Margin_Type column missing from triplets CSV, cannot filter for 'high' margin.")
                    continue
                current_triplets_df = triplets_df_full[triplets_df_full["Margin_Type"].str.lower() == "high"].copy()

            if current_triplets_df.empty:
                logger.warning("No triplets found for margin type '%s'. Skipping.", margin_type_eval)
                continue
            triplet_acc = 0
            triplet_total = 0
            triplet_model_diffs = []
            triplet_bird_diffs = []
            skipped_names = 0
            skipped_nan = 0

            for row in tqdm(
                current_triplets_df.itertuples(index=False), total=len(current_triplets_df), desc=f"Triplets ({margin_type_eval})", leave=False
            ):
                try:
                    anchor_name = str(getattr(row, "Anchor", ""))
                    pos_name = str(getattr(row, "Positive", ""))
                    neg_name = str(getattr(row, "Negative", ""))

                    if not all(n in name_to_idx for n in [anchor_name, pos_name, neg_name]):
                        skipped_names += 1
                        continue
                    anchor_idx = name_to_idx[anchor_name]
                    pos_idx = name_to_idx[pos_name]
                    neg_idx = name_to_idx[neg_name]

                    dist_pos = get_distance(anchor_idx, pos_idx)
                    dist_neg = get_distance(anchor_idx, neg_idx)

                    if not (np.isfinite(dist_pos) and np.isfinite(dist_neg)):
                        skipped_nan += 1
                        continue

                    model_agrees_with_bird = dist_pos < dist_neg
                    if model_agrees_with_bird:
                        triplet_acc += 1
                    triplet_total += 1
                    triplet_model_diffs.append(dist_pos - dist_neg)
                    triplet_bird_diffs.append(1.0)

                except Exception as e:
                    logger.warning("Err triplet row (%s): %s. E: %s", margin_type_eval, row, e, exc_info=False)
                    continue
            if skipped_names > 0:
                logger.warning("[%s] Skipped %d triplets (names not in map).", margin_type_eval, skipped_names)
            if skipped_nan > 0:
                logger.warning("[%s] Skipped %d triplets (NaN/Inf distances).", margin_type_eval, skipped_nan)

            accuracy = (triplet_acc / triplet_total) if triplet_total > 0 else 0.0
            pvalue = None
            if triplet_total > 0:
                try:
                    pvalue = binomtest(triplet_acc, n=triplet_total, p=0.5, alternative="greater").pvalue
                except ValueError as e:
                    logger.error("Binomial test failed for triplets (%s): %s", margin_type_eval, e)
            triplet_results[f"triplet_{margin_type_eval}_accuracy"] = accuracy
            triplet_results[f"triplet_{margin_type_eval}_pvalue"] = pvalue
            triplet_results[f"triplet_{margin_type_eval}_num_valid"] = triplet_total

            p_str = f"{pvalue:.3e}" if pvalue is not None and np.isfinite(pvalue) else "N/A"
            logger.info("Triplets (%s): Acc=%.4f (%d/%d), p=%s", margin_type_eval, accuracy, triplet_acc, triplet_total, p_str)

            correlation_results.update(
                self._compute_correlations(triplet_model_diffs, triplet_bird_diffs, f"triplet_{margin_type_eval}")
            )

        logger.info("--- Evaluating Probe Task ---")
        probe_corr_data = {"unfiltered": {"model_diffs": [], "bird_choices": []}, "filtered": {"model_diffs": [], "bird_choices": []}}
        for filter_type, df in [("unfiltered", probes_df), ("filtered", filtered_probes_df)]:
            probe_acc = 0
            probe_total = 0
            model_diffs_list = probe_corr_data[filter_type]["model_diffs"]
            bird_choices_list = probe_corr_data[filter_type]["bird_choices"]
            skipped_names = 0
            skipped_nan = 0
            skipped_invalid_decision = 0

            if df.empty:
                logger.warning("No probes found for filter type '%s'. Skipping.", filter_type)
                continue
            for row in tqdm(df.itertuples(index=False), total=len(df), desc=f"Probes ({filter_type})", leave=False):
                try:
                    probe_name = str(getattr(row, "sound_id", ""))
                    left_name = str(getattr(row, "left", ""))
                    right_name = str(getattr(row, "right", ""))
                    decision = str(getattr(row, "decision", "")).lower()

                    if decision not in ["left", "right"]:
                        skipped_invalid_decision += 1
                        continue
                    if not all(n in name_to_idx for n in [probe_name, left_name, right_name]):
                        skipped_names += 1
                        continue
                    probe_idx = name_to_idx[probe_name]
                    left_idx = name_to_idx[left_name]
                    right_idx = name_to_idx[right_name]

                    dist_left = get_distance(probe_idx, left_idx)
                    dist_right = get_distance(probe_idx, right_idx)

                    if not (np.isfinite(dist_left) and np.isfinite(dist_right)):
                        skipped_nan += 1
                        continue

                    model_choice_is_left = dist_left < dist_right
                    bird_choice_is_left = decision == "left"
                    if model_choice_is_left == bird_choice_is_left:
                        probe_acc += 1
                    probe_total += 1
                    model_diffs_list.append(dist_left - dist_right)
                    bird_choices_list.append(1.0 if bird_choice_is_left else -1.0)

                except Exception as e:
                    logger.warning("Error processing probe row (%s): %s. E: %s", filter_type, row, e, exc_info=False)
                    continue
            if skipped_names > 0:
                logger.warning("[%s] Skipped %d probes (names not in map).", filter_type, skipped_names)
            if skipped_nan > 0:
                logger.warning("[%s] Skipped %d probes (NaN/Inf dist).", filter_type, skipped_nan)
            if skipped_invalid_decision > 0:
                logger.warning("[%s] Skipped %d probes (invalid decision value).", filter_type, skipped_invalid_decision)

            accuracy = (probe_acc / probe_total) if probe_total > 0 else 0.0
            pvalue = None
            if probe_total > 0:
                try:
                    pvalue = binomtest(probe_acc, n=probe_total, p=0.5, alternative="greater").pvalue
                except ValueError as e:
                    logger.error("Binomial test failed for probes (%s): %s", filter_type, e)
            probe_results[f"probe_accuracy_{filter_type}"] = accuracy
            probe_results[f"probe_pvalue_{filter_type}"] = pvalue
            probe_results[f"probe_num_valid_{filter_type}"] = probe_total

            p_str = f"{pvalue:.3e}" if pvalue is not None and np.isfinite(pvalue) else "N/A"
            logger.info("Probe (%s): Acc=%.4f (%d/%d), p=%s", filter_type, accuracy, probe_acc, probe_total, p_str)

        correlation_results.update(
            self._compute_correlations(probe_corr_data["unfiltered"]["model_diffs"], probe_corr_data["unfiltered"]["bird_choices"], "probe_unfiltered")
        )
        correlation_results.update(
            self._compute_correlations(probe_corr_data["filtered"]["model_diffs"], probe_corr_data["filtered"]["bird_choices"], "probe_filtered")
        )

        if h5_file:
            h5_file.close()
            logger.debug("Closed HDF5 distance matrix file.")
        del dist_mat
        gc.collect()

        final_results = {**triplet_results, **probe_results, **correlation_results}
        for key, value in final_results.items():
            if isinstance(value, (np.float_, np.float16, np.float32, np.float64)):
                final_results[key] = float(value) if np.isfinite(value) else None
            elif isinstance(value, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):
                final_results[key] = int(value)
            elif isinstance(value, list) and len(value) == 2:
                final_results[key] = [float(v) if isinstance(v, (np.number, float, int)) and np.isfinite(v) else None for v in value]
            elif isinstance(value, np.bool_):
                final_results[key] = bool(value)
            elif pd.isna(value):
                final_results[key] = None

        logger.info("PerceptualAlignment evaluation finished.")
        return final_resultsimport logging
from typing import Any, Dict, List, Optional
import torch
import numpy as np
from tqdm import tqdm
from pathlib import Path
import gc

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"

DEFAULT_METRICS_PREFIX = "P@"


class PrecisionAtK(Benchmark):
    def _initialize(self, k_values: List[int] = [1, 5], **kwargs):
        """
        Initializes the PrecisionAtK benchmark parameters.

        Args:
            k_values (List[int]): A list of positive integers representing the
                k values for which to compute precision. Defaults to [1, 5].
            **kwargs: Additional keyword arguments.

        Returns:
            None
        """
        if not k_values or not all(isinstance(k, int) and k > 0 for k in k_values):
            raise ValueError("k_values must be non-empty list of positive integers.")
        self.k_values = sorted(k_values)
        self.max_k = max(self.k_values)
        self._default_metrics = {f"{DEFAULT_METRICS_PREFIX}{k}": None for k in self.k_values}
        logger.info(f"Initialized PrecisionAtK (Avg Prop Mode) with k_values: {self.k_values}")

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        distance_matrix_path: Optional[Path] = None,
        labels: Optional[List[Any]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Compute P@K scores (as average proportion).

        Calculates the average proportion of neighbors within the top K closest
        points that belong to the same class as the query point. Operates on
        a distance matrix and a corresponding list of labels. Handles filtering
        out points with None labels.

        Args:
            distance_matrix (Optional[Any]): The distance matrix (numpy array or torch tensor).
                If None, distance_matrix_path is used as a fallback (though direct loading
                from path within this method is simplified/removed). Required.
            distance_matrix_path (Optional[Path]): Path to the distance matrix file (for context, not used).
            labels (Optional[List[Any]]): A list of labels for each data point,
                corresponding to the rows/columns of the distance matrix. Required.
            **kwargs: Additional keyword arguments.

        Returns:
            Dict[str, Any]: A dictionary containing the P@K scores for each
                            specified k value, or default None values if
                            required inputs are missing or invalid.
        """
        default_return = self._default_metrics.copy()
        dist_mat = distance_matrix

        if dist_mat is None:
            logger.error("PrecisionAtK requires distance_matrix.")
            return default_return

        try:
            if isinstance(dist_mat, np.ndarray):
                dist_mat_tensor = torch.from_numpy(dist_mat).float()
            elif isinstance(dist_mat, torch.Tensor):
                dist_mat_tensor = dist_mat.float()
            else:
                raise TypeError(f"Unsupported distance_matrix type: {type(dist_mat)}")

            if dist_mat_tensor.ndim != 2 or dist_mat_tensor.shape[0] != dist_mat_tensor.shape[1]:
                raise ValueError(f"Distance matrix must be square 2D, got {dist_mat_tensor.shape}")
            n_samples = dist_mat_tensor.shape[0]
            logger.info(f"Using provided distance matrix for P@K. Shape: {dist_mat_tensor.shape}")

        except Exception as e:
            logger.error("Error processing distance matrix: %s", e, exc_info=True)
            gc.collect()
            return default_return

        if labels is None:
            logger.error("PrecisionAtK requires 'labels'.")
            gc.collect()
            return default_return
        if not isinstance(labels, list):
            logger.error("PrecisionAtK received non-list labels: %s.", type(labels))
            gc.collect()
            return default_return
        if len(labels) != n_samples:
            logger.error("Label count %d != matrix (%d).", len(labels), n_samples)
            gc.collect()
            return default_return
        try:
            valid_labels_with_indices = [(idx, str(lbl)) for idx, lbl in enumerate(labels) if lbl is not None]
            if not valid_labels_with_indices:
                logger.error("No valid (non-None) labels.")
                gc.collect()
                return default_return
            unique_labels = sorted(list(set(lbl for _, lbl in valid_labels_with_indices)))
            label_to_id = {label: i for i, label in enumerate(unique_labels)}
            label_ids = torch.full((n_samples,), -1, dtype=torch.long)
            valid_sample_indices = []
            for idx, label_str in valid_labels_with_indices:
                label_id = label_to_id.get(label_str, -1)
                if label_id != -1:
                    label_ids[idx] = label_id
                    valid_sample_indices.append(idx)
            num_valid_samples_for_eval = len(valid_sample_indices)
            if num_valid_samples_for_eval == 0:
                logger.warning("No samples with valid labels.")
                gc.collect()
                return default_return
        except Exception as e:
            logger.error("Label processing failed: %s", e, exc_info=True)
            gc.collect()
            return default_return

        results = {}
        try:
            target_device = torch.device("cpu")
            dist_mat_tensor = dist_mat_tensor.to(target_device)
            label_ids = label_ids.to(target_device)
            dist_mat_tensor.fill_diagonal_(float("inf"))
            logger.debug("Sorting distances for top %d neighbors...", self.max_k)
            sorted_indices = torch.argsort(dist_mat_tensor, dim=1)[:, : self.max_k]
            logger.debug("Sorting complete.")
            neighbor_labels = label_ids[sorted_indices]
            true_labels_expanded = label_ids.view(-1, 1).expand(-1, self.max_k)
            matches = neighbor_labels == true_labels_expanded
            logger.info("Calculating P@K (Avg Prop) for %d valid queries...", num_valid_samples_for_eval)
            valid_query_mask = label_ids != -1
            for k in self.k_values:
                num_correct_at_k = matches[:, :k].sum(dim=1)
                num_correct_valid_queries_at_k = num_correct_at_k[valid_query_mask]
                total_correct_neighbors = torch.sum(num_correct_valid_queries_at_k.float()).item()
                total_neighbors_considered = num_valid_samples_for_eval * k
                average_proportion_at_k = total_correct_neighbors / total_neighbors_considered if total_neighbors_considered > 0 else 0.0
                metric_name = f"{DEFAULT_METRICS_PREFIX}{k}"
                results[metric_name] = average_proportion_at_k
                logger.debug("%s: %.4f (on %d valid queries)", metric_name, average_proportion_at_k, num_valid_samples_for_eval)
            logger.info("P@K (Avg Prop) calculation finished.")
        except Exception as e:
            logger.error("Error during P@K calculation: %s", e, exc_info=True)
            results = default_return
        finally:
            del dist_mat_tensor, label_ids, sorted_indices, neighbor_labels, true_labels_expanded, matches
            if "valid_query_mask" in locals():
                del valid_query_mask
            if "num_correct_at_k" in locals():
                del num_correct_at_k
            if "num_correct_valid_queries_at_k" in locals():
                del num_correct_valid_queries_at_k
            gc.collect()
        return results# In benchmarks/silhouette.py

import logging
import time
from typing import Any, Dict, List, Optional

import numpy as np
import torch
from tqdm.auto import tqdm
import gc
from sklearn.metrics import silhouette_score, silhouette_samples

from benchmarks.base import Benchmark

logger = logging.getLogger(__name__)

class SilhouetteBenchmark(Benchmark):
    """
    Computes the Silhouette Score, a measure of cluster cohesion vs. separation.
    Operates on a pre-computed distance matrix and corresponding labels.
    """

    def _initialize(
        self,
        sample_size: Optional[int] = 5000,
        random_state: int = 42,
        metric: str = "precomputed",
        chunk_size: int = 2048,
        **kwargs
    ):
        """
        Initializes Silhouette benchmark parameters.

        Args:
            sample_size (Optional[int]): The number of samples to use for the calculation.
                If the dataset is larger, a random subset of this size is taken.
                If None, the entire dataset is used (may be slow or memory-intensive).
            random_state (int): Seed for the random sampler.
            metric (str): The metric for silhouette_score. Must be "precomputed" when
                a distance matrix is provided.
            chunk_size (int): Size of chunks for processing silhouette_samples if the
                full matrix is too large for memory.
            **kwargs: Additional keyword arguments.
        """
        self.sample_size = sample_size
        self.random_state = random_state
        if metric != "precomputed":
            raise ValueError("SilhouetteBenchmark with a distance matrix requires metric='precomputed'.")
        self.metric = metric
        self.chunk_size = chunk_size
        logger.info(
            f"Initialized SilhouetteBenchmark (sample_size={self.sample_size}, random_state={self.random_state})"
        )

    def evaluate(
        self,
        *,
        distance_matrix: Optional[Any] = None,
        labels: Optional[List[Any]] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Compute the mean Silhouette Score.

        Args:
            distance_matrix (Optional[Any]): The distance matrix (numpy array or torch tensor). Required.
            labels (Optional[List[Any]]): A list of labels for each data point. Required.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            Dict[str, Any]: A dictionary containing the 'silhouette_score' and an 'error' message if applicable.
        """
        start_time = time.time()
        default_return = {"silhouette_score": None, "error": None}

        if distance_matrix is None:
            return {**default_return, "error": "distance_matrix is required."}
        if labels is None:
            return {**default_return, "error": "labels are required."}

        try:
            if isinstance(distance_matrix, torch.Tensor):
                dist_mat_np = distance_matrix.cpu().numpy().astype(np.float32)
            else:
                dist_mat_np = np.array(distance_matrix, dtype=np.float32)

            if dist_mat_np.ndim != 2 or dist_mat_np.shape[0] != dist_mat_np.shape[1]:
                raise ValueError(f"distance_matrix must be square 2D, got shape {dist_mat_np.shape}")
            
            if not isinstance(labels, list):
                labels = list(labels)
            
            if len(labels) != dist_mat_np.shape[0]:
                 raise ValueError(f"Label count ({len(labels)}) != distance matrix dim ({dist_mat_np.shape[0]})")
            
            # --- Filtering for valid data ---
            valid_indices_mask = np.array([lbl is not None for lbl in labels])
            if not np.any(valid_indices_mask):
                raise ValueError("No valid (non-None) labels found.")
            
            labels_valid_str = np.array([str(lbl) for lbl in labels])[valid_indices_mask]
            
            # Identify classes with at least 2 members (required for silhouette)
            unique_labels, counts = np.unique(labels_valid_str, return_counts=True)
            valid_classes = {label for label, count in zip(unique_labels, counts) if count >= 2}

            if len(valid_classes) < 2:
                raise ValueError(f"Silhouette score requires at least 2 classes with >= 2 samples. Found {len(valid_classes)} valid classes.")

            final_filter_mask_in_valid = np.isin(labels_valid_str, list(valid_classes))
            original_indices_where_valid = np.where(valid_indices_mask)[0]
            final_indices = original_indices_where_valid[final_filter_mask_in_valid]

            if len(final_indices) == 0:
                raise ValueError("No samples remaining after filtering for min_class_size >= 2.")
            
            dist_mat_filtered = dist_mat_np[np.ix_(final_indices, final_indices)]
            labels_array_final = np.array(labels)[final_indices]
            
            n_samples_final = len(labels_array_final)
            logger.info(f"Filtered data for Silhouette. Kept {n_samples_final} samples across {len(valid_classes)} classes.")

            # --- Subsampling if necessary ---
            if self.sample_size is not None and n_samples_final > self.sample_size:
                logger.info(f"Subsampling {n_samples_final} samples down to {self.sample_size} for Silhouette calculation.")
                rng = np.random.RandomState(self.random_state)
                sample_indices = rng.choice(n_samples_final, self.sample_size, replace=False)
                
                dist_mat_sampled = dist_mat_filtered[np.ix_(sample_indices, sample_indices)]
                labels_sampled = labels_array_final[sample_indices]
                X_eval = dist_mat_sampled
                labels_eval = labels_sampled
            else:
                X_eval = dist_mat_filtered
                labels_eval = labels_array_final

            logger.info(f"Calculating Silhouette score on {len(labels_eval)} samples...")
            
            score = silhouette_score(X_eval, labels_eval, metric=self.metric)
            
            default_return["silhouette_score"] = float(score)

        except ValueError as ve:
            # Catch specific value errors from sklearn, which are informative
            default_return["error"] = f"Silhouette calculation failed: {ve}"
            logger.error(f"Silhouette calculation error: {ve}")
        except Exception as e:
            default_return["error"] = f"An unexpected error occurred during Silhouette calculation: {e}"
            logger.error(f"Silhouette calculation failed: {e}", exc_info=True)
        finally:
            gc.collect()

        elapsed_time = time.time() - start_time
        logger.info(f"Silhouette benchmark finished in {elapsed_time:.2f}s. Score: {default_return['silhouette_score']}")
        return default_return# -*- coding: utf-8 -*-
"""
VocSim Benchmark: Evaluation Modules.
"""

from .base import Benchmark
from .csr import ClassSeparationRatio
from .precision import PrecisionAtK
from .f_value import FValueBenchmark
from .cscf import CSCFBenchmark
from .clustering import ClusteringPurity
from .perceptual import PerceptualAlignment
from .classification import ClassificationBenchmark
from .gsr import GlobalSeparationRate
from .silhouette import SilhouetteBenchmark 

__all__ = [
    "Benchmark",
    "PrecisionAtK",
    "FValueBenchmark",
    "CSCFBenchmark",
    "ClassSeparationRatio",
    "ClusteringPurity",
    "PerceptualAlignment",
    "ClassificationBenchmark",   
    "GlobalSeparationRate",
    "SilhouetteBenchmark"
]results_dir: results
features_dir: features_cache
models_dir: models
data_dir: data

general_settings:
  force_cpu: false
  target_sample_rate: 16000
  feature_chunk_size_gb: 3
  run_id: default_run_${now:%Y%m%d_%H%M%S}

logging:
  level: INFO
  log_dir: logs
  log_file: benchmark_run.logbenchmarks:
  - name: PrecisionAtK
    module: benchmarks.precision
    params:
      k_values: [1, 5, 10]

  - name: GlobalSeparationRate
    module: benchmarks.gsr
    params:
      min_class_size: 2
      epsilon: 1.0e-9
      
  - name: FValueBenchmark
    module: benchmarks.f_value
    params:
      min_class_size: 2

  - name: CSCFBenchmark
    module: benchmarks.cscf
    params:
      min_class_size: 2

  - name: ClusteringPurity
    module: benchmarks.clustering
    params:
      umap_n_components: 2
      hdbscan_min_cluster_size: 5
      use_distance_matrix_for_umap: true
      umap_metric: precomputed
      hdbscan_metric: euclidean

  - name: PerceptualAlignment
    module: benchmarks.perceptual
    params:
      probe_consistency_threshold: 0.7
      bootstrap_ci: true
      n_bootstraps: 100

  - name: ClassSeparationRatio
    module: benchmarks.csr
    params:
      min_class_size: 2
      epsilon: 1.0e-9

  - name: ClassificationBenchmark
    module: benchmarks.classification
    params:
      n_splits: 5
      random_state: 42
      classifiers: [knn, rf, mlp]
      eval_metrics: [accuracy]
      top_k: 5
      classifier_params:
        knn:
          n_neighbors: [3, 10, 30]
          n_jobs: [-1]
        rf:
          max_depth: [10, 15, 20]
          random_state: 42
          n_jobs: [-1]
        mlp:
          alpha: [0.1, 0.01, 0.001]
          random_state: 42
          max_iter: 500
          batch_size: autodatasets:
  vocsim_full:
    id: anonymous-submission000/vocsim
    subset: null
    split: train

  vocsim_subset_bs1:
    id: anonymous-submission000/vocsim
    subset: BS1
    split: train

  avian_perception:
    id: anonymous-submission000/vocsim-applications-avian-perception
    subset: null
    split: train

  mouse_identity:
    id: anonymous-submission000/vocsim-applications-mouse-identity
    subset: null
    split: train

  mouse_strain:
    id: anonymous-submission000/vocsim-applications-mouse-strain
    subset: null
    split: traindistances:
  - name: cosine
  - name: euclidean
    params:
      use_torchmetrics: true 
      zero_diagonal: true
  - name: spearmanfeature_extractors:

  - name: MelExtractor
    module: features.mel
    params:
      sr: 16000 
      n_fft: 512
      hop_length: 256
      n_mels: 128
      log_scale: true

  - name: WavLMExtractor
    module: features.wavlm
    params:
      model_id: microsoft/wavlm-large
      output_hidden_states: false

  - name: Wav2Vec2Extractor
    module: features.wav2vec2
    params:
      model_id: facebook/wav2vec2-base-960h
      output_hidden_states: false

  - name: WhisperEncoderExtractor
    module: features.whisper
    params:
      model_id: openai/whisper-large-v3 
      output_hidden_states: false

  - name: WhisperSegExtractor_Embedding
    module: features.whisperseg.extractor
    params:
      model_path: "nccratliri/whisperseg-base-animal-vad-ct2" 
      output_type: "embedding"

  - name: WhisperSegExtractor_Spectrogram
    module: features.whisperseg.extractor
    params:
      model_path: "nccratliri/whisperseg-base-animal-vad-ct2"
      output_type: "spectrogram"

  - name: CLAPExtractor
    module: features.clap
    params:
      model_id: "laion/larger_clap_general"

  - name: AudioMAEExtractor
    module: features.audiomae
    params:
      model_id: "hance-ai/audiomae"
      trust_remote_code: true

  - name: EncodecExtractor
    module: features.encodec
    params:
      model_id: "facebook/encodec_24khz"
      bandwidth: 6.0 


  - name: WavLMExtractor_mean_time
    base_extractor: WavLMExtractor
    averaging: mean_time

  - name: WavLMExtractor_mean_time_pca_100
    base_extractor: WavLMExtractor
    averaging: mean_time
    pca: 100

  - name: WavLMExtractor_pca_100
    base_extractor: WavLMExtractor
    pca: 100trainers: []
"""Abstract Base Class for distance calculators."""

from abc import ABC, abstractmethod
from typing import Any
import numpy as np

class DistanceCalculator(ABC):
    """
    Abstract Base Class for calculating pairwise distances between feature vectors.

    Subclasses must implement the `compute_pairwise` method.
    """

    def __init__(self, **kwargs):
        """
        Initialize the base distance calculator.

        Args:
            **kwargs: Additional keyword arguments for subclasses.
        """
        self._initialize(**kwargs)

    def _initialize(self, **kwargs):
        """Optional initialization for subclasses."""
        pass

    @abstractmethod
    def compute_pairwise(self, features: Any) -> Any:
        """
        Compute the pairwise distance matrix for a set of features.

        Args:
            features (Any): A collection of features, typically a 2D tensor or array
                           where rows represent samples and columns represent feature dimensions
                           (e.g., torch.Tensor or np.ndarray of shape [n_samples, feature_dim]).

        Returns:
            Any: A square matrix (e.g., torch.Tensor or np.ndarray of shape
                 [n_samples, n_samples]) containing the pairwise distances.
                 The exact format depends on the implementation.
        """
        pass

    def __call__(self, features: Any) -> Any:
        """Allows calling the instance like a function."""
        return self.compute_pairwise(features)import logging
from typing import Any, Union, Optional
from distances.base import DistanceCalculator
import torch
import numpy as np
from torchmetrics.functional.pairwise import pairwise_cosine_similarity

logger = logging.getLogger(__name__)


class CosineDistance(DistanceCalculator):
    """
    Calculates pairwise Cosine distances using torchmetrics.
    """

    def _initialize(self, use_torchmetrics: bool = True, zero_diagonal: bool = True, **kwargs):
        """
        Initialize CosineDistance calculator.

        Args:
            use_torchmetrics (bool): Whether to use torchmetrics for calculation (currently required).
            zero_diagonal (bool): Whether to set the diagonal of the distance matrix to zero
                when computing distances between the same set of features (X vs X).
            **kwargs: Additional keyword arguments.
        """
        if not use_torchmetrics or torch is None:
            raise NotImplementedError("CosineDistance currently requires torchmetrics and PyTorch.")
        self.zero_diagonal = zero_diagonal
        logger.debug(f"CosineDistance initialized (torchmetrics, zero_diag={zero_diagonal})")

    def compute_pairwise(self, features_X: Any, features_Y: Optional[Any] = None) -> Union[torch.Tensor, np.ndarray]:
        """
        Compute the pairwise Cosine distance matrix.

        Args:
            features_X (Any): A 2D tensor or array [n_samples_X, feature_dim].
            features_Y (Optional[Any]): An optional second 2D tensor or array [n_samples_Y, feature_dim].
                                       If provided, computes distances between X and Y.
                                       If None, computes distances between X and X.

        Returns:
            torch.Tensor: Pairwise distance matrix [n_samples_X, n_samples_Y (or X)], scaled [0, 1].
                         Always returns a torch.Tensor on the device used for computation.
        """
        if not isinstance(features_X, (torch.Tensor, np.ndarray)):
            raise TypeError(f"Input features_X must be Tensor/ndarray, got {type(features_X)}")
        if features_X.ndim != 2:
            raise ValueError(f"Input features_X must be 2D, got {features_X.shape}")

        if isinstance(features_X, np.ndarray):
            features_tensor_X = torch.from_numpy(features_X).float()
        else:
            features_tensor_X = features_X.float()

        features_tensor_Y = None
        if features_Y is not None:
            if not isinstance(features_Y, (torch.Tensor, np.ndarray)):
                raise TypeError(f"Input features_Y must be Tensor/ndarray, got {type(features_Y)}")
            if features_Y.ndim != 2:
                raise ValueError(f"Input features_Y must be 2D, got {features_Y.shape}")
            if features_X.shape[1] != features_Y.shape[1]:
                raise ValueError(f"Feature dimensions mismatch: X={features_X.shape[1]}, Y={features_Y.shape[1]}")

            if isinstance(features_Y, np.ndarray):
                features_tensor_Y = torch.from_numpy(features_Y).float()
            else:
                features_tensor_Y = features_Y.float()
            features_tensor_Y = features_tensor_Y.to(features_tensor_X.device)
            target_shape = (features_tensor_X.shape[0], features_tensor_Y.shape[0])
            compute_mode = "X vs Y"
            zero_diag_effective = False
        else:
            target_shape = (features_tensor_X.shape[0], features_tensor_X.shape[0])
            compute_mode = "X vs X"
            zero_diag_effective = self.zero_diagonal

        logger.debug(f"Computing Cosine Distance ({compute_mode}) for shapes {features_tensor_X.shape} vs {features_tensor_Y.shape if features_tensor_Y is not None else 'self'}")

        try:
            similarity_matrix = pairwise_cosine_similarity(features_tensor_X, features_tensor_Y)

            distance_matrix = (1.0 - similarity_matrix) / 2.0

            if zero_diag_effective:
                torch.diagonal(distance_matrix).fill_(0)

            distance_matrix.clamp_(min=0.0, max=1.0)

            if distance_matrix.shape != target_shape:
                logger.warning(f"Output shape mismatch: {distance_matrix.shape} != {target_shape}. Check torchmetrics version/behavior.")

            return distance_matrix
        except Exception as e:
            logger.error("Torchmetrics Cosine distance failed: %s", e, exc_info=True)
            raiseimport logging
from typing import Any, Union, Optional
from distances.base import DistanceCalculator
import torch
import numpy as np
from torchmetrics.functional.pairwise import pairwise_euclidean_distance

logger = logging.getLogger(__name__)


class EuclideanDistance(DistanceCalculator):
    """
    Calculates pairwise Euclidean distances using torchmetrics.
    """

    def _initialize(self, use_torchmetrics: bool = True, zero_diagonal: bool = True, **kwargs):
        """
        Initialize EuclideanDistance calculator.

        Args:
            use_torchmetrics (bool): Whether to use torchmetrics for calculation (currently required).
            zero_diagonal (bool): Whether to set the diagonal of the distance matrix to zero
                when computing distances between the same set of features (X vs X).
            **kwargs: Additional keyword arguments.
        """
        if not use_torchmetrics or torch is None:
            raise NotImplementedError("EuclideanDistance currently requires torchmetrics and PyTorch.")
        self.zero_diagonal = zero_diagonal
        logger.debug(f"EuclideanDistance initialized (torchmetrics, zero_diag={zero_diagonal})")

    def compute_pairwise(self, features_X: Any, features_Y: Optional[Any] = None) -> Union[torch.Tensor, np.ndarray]:
        """
        Compute the pairwise Euclidean distance matrix.

        Args:
            features_X (Any): A 2D tensor or array [n_samples_X, feature_dim].
            features_Y (Optional[Any]): An optional second 2D tensor or array [n_samples_Y, feature_dim].
                                       If provided, computes distances between X and Y.
                                       If None, computes distances between X and X.

        Returns:
            torch.Tensor: Pairwise distance matrix [n_samples_X, n_samples_Y (or X)].
                         Always returns a torch.Tensor on the device used for computation.
        """
        if not isinstance(features_X, (torch.Tensor, np.ndarray)):
            raise TypeError(f"Input features_X must be Tensor/ndarray, got {type(features_X)}")
        if features_X.ndim != 2:
            raise ValueError(f"Input features_X must be 2D, got {features_X.shape}")

        if isinstance(features_X, np.ndarray):
            features_tensor_X = torch.from_numpy(features_X).float()
        else:
            features_tensor_X = features_X.float()

        features_tensor_Y = None
        if features_Y is not None:
            if not isinstance(features_Y, (torch.Tensor, np.ndarray)):
                raise TypeError(f"Input features_Y must be Tensor/ndarray, got {type(features_Y)}")
            if features_Y.ndim != 2:
                raise ValueError(f"Input features_Y must be 2D, got {features_Y.shape}")
            if features_X.shape[1] != features_Y.shape[1]:
                raise ValueError(f"Feature dimensions mismatch: X={features_X.shape[1]}, Y={features_Y.shape[1]}")

            if isinstance(features_Y, np.ndarray):
                features_tensor_Y = torch.from_numpy(features_Y).float()
            else:
                features_tensor_Y = features_Y.float()
            features_tensor_Y = features_tensor_Y.to(features_tensor_X.device)
            target_shape = (features_tensor_X.shape[0], features_tensor_Y.shape[0])
            compute_mode = "X vs Y"
            zero_diag_effective = False
        else:
            target_shape = (features_tensor_X.shape[0], features_tensor_X.shape[0])
            compute_mode = "X vs X"
            zero_diag_effective = self.zero_diagonal

        logger.debug(f"Computing Euclidean Distance ({compute_mode}) for shapes {features_tensor_X.shape} vs {features_tensor_Y.shape if features_tensor_Y is not None else 'self'}")

        try:
            distance_matrix = pairwise_euclidean_distance(
                features_tensor_X, features_tensor_Y, zero_diagonal=zero_diag_effective
            )

            distance_matrix.clamp_(min=0.0)

            if distance_matrix.shape != target_shape:
                logger.warning(f"Output shape mismatch: {distance_matrix.shape} != {target_shape}. Check torchmetrics version/behavior.")

            return distance_matrix
        except Exception as e:
            logger.error("Torchmetrics Euclidean distance failed: %s", e, exc_info=True)
            raiseimport logging
from typing import Any, Union, Optional
from distances.base import DistanceCalculator
import torch
import numpy as np
from scipy.stats import spearmanr

logger = logging.getLogger(__name__)


class SpearmanDistance(DistanceCalculator):
    """
    Calculates pairwise Spearman Rank Correlation distances.
    """

    def _initialize(self, use_gpu_if_available: bool = True, **kwargs):
        """
        Initialize SpearmanDistance calculator.

        Args:
            use_gpu_if_available (bool): If True and CUDA is available, use the GPU implementation.
            **kwargs: Additional arguments (ignored).
        """
        self.use_gpu = use_gpu_if_available and torch.cuda.is_available()
        if self.use_gpu:
            logger.debug("SpearmanDistance will use GPU implementation.")
        else:
            logger.debug("SpearmanDistance will use CPU (SciPy) implementation.")

    def _compute_ranks_gpu(self, features: torch.Tensor) -> torch.Tensor:
        """
        Computes ranks along the feature dimension (dim=1) on GPU.

        Args:
            features (torch.Tensor): A 2D tensor [N, D].

        Returns:
            torch.Tensor: A 2D tensor of ranks [N, D].
        """
        ranks = features.argsort(dim=1).argsort(dim=1).float()
        return ranks

    def _fast_spearman_gpu(self, features_X: torch.Tensor, features_Y: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Custom PyTorch implementation for Spearman correlation distance on GPU.

        Args:
            features_X (torch.Tensor): A 2D tensor [n_samples_X, feature_dim] on a GPU device.
            features_Y (Optional[torch.Tensor]): An optional second 2D tensor [n_samples_Y, feature_dim]
                                                 on the same GPU device.

        Returns:
            torch.Tensor: Pairwise distance matrix [n_samples_X, n_samples_Y (or X)] on the GPU.
        """
        n_samples_X, n_features = features_X.shape
        if n_samples_X < 1:
            if features_Y is None:
                return torch.zeros((0, 0), device=features_X.device)
            else:
                return torch.zeros((0, features_Y.shape[0]), device=features_X.device)

        ranks_X = self._compute_ranks_gpu(features_X)

        ranks_X_centered = ranks_X - ranks_X.mean(dim=1, keepdim=True)

        std_devs_X = ranks_X_centered.std(dim=1, unbiased=True)
        std_devs_X[std_devs_X == 0] = 1.0

        if features_Y is None:
            n_samples_Y = n_samples_X
            ranks_Y_centered = ranks_X_centered
            std_devs_Y = std_devs_X
            target_shape = (n_samples_X, n_samples_X)
        else:
            n_samples_Y = features_Y.shape[0]
            if features_Y.shape[1] != n_features:
                raise ValueError(f"Feature dimension mismatch: X={n_features}, Y={features_Y.shape[1]}")
            ranks_Y = self._compute_ranks_gpu(features_Y)
            ranks_Y_centered = ranks_Y - ranks_Y.mean(dim=1, keepdim=True)
            std_devs_Y = ranks_Y_centered.std(dim=1, unbiased=True)
            std_devs_Y[std_devs_Y == 0] = 1.0
            target_shape = (n_samples_X, n_samples_Y)

        numerator = torch.matmul(ranks_X_centered, ranks_Y_centered.t())

        denominator = torch.outer(std_devs_X, std_devs_Y) * (n_features - 1)

        rho = torch.zeros_like(numerator)
        valid_denom_mask = denominator != 0
        rho[valid_denom_mask] = numerator[valid_denom_mask] / denominator[valid_denom_mask]

        rho = torch.clamp(rho, -1.0, 1.0)

        distance_matrix = (1.0 - rho) / 2.0

        if features_Y is None:
            torch.diagonal(distance_matrix).fill_(0.0)

        if distance_matrix.shape != target_shape:
            logger.warning(f"Spearman GPU shape mismatch: {distance_matrix.shape} != {target_shape}")

        return distance_matrix

    def _scipy_spearman_cpu(self, features_X: np.ndarray, features_Y: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Computes Spearman correlation distance using SciPy on CPU.

        Args:
            features_X (np.ndarray): A 2D numpy array [n_samples_X, feature_dim].
            features_Y (Optional[np.ndarray]): An optional second 2D numpy array [n_samples_Y, feature_dim].

        Returns:
            np.ndarray: Pairwise distance matrix [n_samples_X, n_samples_Y (or X)] as a numpy array.
        """
        logger.debug(
            f"Using SciPy Spearman on CPU for shapes {features_X.shape} vs {features_Y.shape if features_Y is not None else 'self'}"
        )

        if features_X.ndim != 2:
            raise ValueError("SciPy Spearman requires 2D input X.")
        n_samples_X = features_X.shape[0]

        try:
            if features_Y is None:
                rho_matrix, _ = spearmanr(features_X, axis=1)
                if np.isscalar(rho_matrix):
                    rho_matrix = np.array([[1.0]]) if n_samples_X == 1 else np.eye(n_samples_X)
            else:
                raise NotImplementedError(
                    "CPU-based Spearman distance computation currently only supports X vs X (calculating the full matrix). Use GPU for X vs Y."
                )

            rho_matrix = np.nan_to_num(rho_matrix, nan=0.0)

            distance_matrix = (1.0 - rho_matrix) / 2.0
            if features_Y is None:
                np.fill_diagonal(distance_matrix, 0.0)

            return distance_matrix.astype(np.float32)

        except ValueError as e:
            logger.error("SciPy spearmanr failed. Check input data (NaNs/Infs?). Error: %s", e, exc_info=True)
            n_samples_Y = features_Y.shape[0] if features_Y is not None else n_samples_X
            dist_mat = np.full((n_samples_X, n_samples_Y), 0.5, dtype=np.float32)
            if features_Y is None:
                np.fill_diagonal(dist_mat, 0.0)
            return dist_mat
        except Exception as e:
            logger.error("Spearman CPU distance calculation failed: %s", e, exc_info=True)
            raise

    def compute_pairwise(self, features_X: Any, features_Y: Optional[Any] = None) -> Union[torch.Tensor, np.ndarray]:
        """
        Compute the pairwise Spearman distance matrix. Uses GPU if enabled and possible.

        Args:
            features_X (Any): A 2D tensor or array [n_samples_X, feature_dim].
            features_Y (Optional[Any]): An optional second 2D tensor or array [n_samples_Y, feature_dim].
                                       If provided, computes distances between X and Y (GPU required).
                                       If None, computes distances between X and X.

        Returns:
            Union[torch.Tensor, np.ndarray]: Pairwise distance matrix [n_samples_X, n_samples_Y (or X)].
                                             Returns torch.Tensor if computed on GPU, np.ndarray if on CPU.
        """
        use_gpu_runtime = self.use_gpu and isinstance(features_X, torch.Tensor) and features_X.is_cuda
        if features_Y is not None:
            use_gpu_runtime = use_gpu_runtime and isinstance(features_Y, torch.Tensor) and features_Y.is_cuda
            if not use_gpu_runtime:
                logger.warning("X vs Y Spearman requested but GPU conditions not met. Trying CPU (may fail/be slow).")
                if isinstance(features_X, torch.Tensor):
                    features_X_np = features_X.cpu().numpy()
                else:
                    features_X_np = np.asarray(features_X)
                features_Y_np = None
                if features_Y is not None:
                    if isinstance(features_Y, torch.Tensor):
                        features_Y_np = features_Y.cpu().numpy()
                    else:
                        features_Y_np = np.asarray(features_Y)
                return self._scipy_spearman_cpu(features_X_np, features_Y_np)

        if use_gpu_runtime:
            logger.debug("Using GPU Spearman computation.")
            try:
                return self._fast_spearman_gpu(features_X, features_Y)
            except Exception as e:
                logger.error("GPU Spearman failed: %s. Falling back to CPU.", e, exc_info=True)
                features_X_np = features_X.cpu().numpy()
                features_Y_np = features_Y.cpu().numpy() if features_Y is not None else None
                return self._scipy_spearman_cpu(features_X_np, features_Y_np)
        else:
            logger.debug("Using CPU Spearman computation.")
            if isinstance(features_X, torch.Tensor):
                features_X_np = features_X.cpu().numpy()
            else:
                features_X_np = np.asarray(features_X)
            features_Y_np = None
            if features_Y is not None:
                if isinstance(features_Y, torch.Tensor):
                    features_Y_np = features_Y.cpu().numpy()
                else:
                    features_Y_np = np.asarray(features_Y)
            return self._scipy_spearman_cpu(features_X_np, features_Y_np)"""
VocSim Benchmark: Distance Computation Modules.
"""

from features.base import FeatureExtractor
from .cosine import CosineDistance
from .euclidean import EuclideanDistance
from .spearman import SpearmanDistance

__all__ = [
    "DistanceCalculator",
    "CosineDistance",
    "EuclideanDistance",
    "SpearmanDistance",
]# -*- coding: utf-8 -*-
"""AudioMAE feature extractor (hance-ai/audiomae)."""

import logging
from typing import Union, Any
from features.base import FeatureExtractor
import numpy as np
import torch
from transformers import AutoModel
import librosa
import soundfile as sf
import tempfile
import os

logger = logging.getLogger(__name__)


class AudioMAEExtractor(FeatureExtractor):
    """
    Feature extractor using the AudioMAE model from hance-ai on Hugging Face.
    Reshapes output to (768, 512).
    """

    def _initialize(self, model_id: str = "hance-ai/audiomae", trust_remote_code: bool = True, **kwargs):
        """
        Load the pre-trained AudioMAE model.

        Args:
            model_id (str): The Hugging Face model identifier.
            trust_remote_code (bool): Whether to trust remote code for loading.
            **kwargs: Additional arguments passed to FeatureExtractor init.
        """
        logger.info("Loading AudioMAE model '%s'...", model_id)
        try:
            self.model = AutoModel.from_pretrained(model_id, trust_remote_code=trust_remote_code).to(self.device)
            self.model.eval()
            logger.info("AudioMAE model loaded successfully.")
        except Exception as e:
            logger.error("Failed to load AudioMAE model '%s': %s", model_id, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor, str], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extracts features using AudioMAE and reshapes them.

        Args:
            audio_data: Input audio as numpy array, torch tensor, or file path.
            sample_rate: Sample rate of the audio (if array/tensor).
            **kwargs: Additional arguments (ignored).

        Returns:
            A torch.Tensor of shape (768, 512) or an empty tensor on failure.
        """
        try:
            tmp_file_to_delete = None
            if isinstance(audio_data, (np.ndarray, torch.Tensor)):
                if isinstance(audio_data, torch.Tensor):
                    audio_data = audio_data.cpu().numpy()

                if audio_data.ndim > 1:
                    if audio_data.shape[0] > audio_data.shape[1]:
                        audio_data = np.mean(audio_data, axis=0)
                    else:
                        audio_data = np.mean(audio_data, axis=1)

                target_sr = 16000
                if sample_rate != target_sr:
                    audio_data = librosa.resample(audio_data.astype(np.float32), orig_sr=sample_rate, target_sr=target_sr)
                    sample_rate = target_sr

                audio_data = audio_data.astype(np.float32)
                max_val = np.max(np.abs(audio_data))
                if max_val > 1e-6:
                    audio_data = audio_data / max_val
                else:
                    logger.warning("Audio data appears to be silent or near-silent.")

                if np.any(np.isnan(audio_data)) or np.any(np.isinf(audio_data)):
                    logger.error("Audio data contains NaN or infinite values after processing.")
                    raise ValueError("Audio data contains NaN or infinite values.")

                custom_temp_dir = "d:/code/vocsim/temp"
                os.makedirs(custom_temp_dir, exist_ok=True)
                with tempfile.NamedTemporaryFile(suffix=".wav", dir=custom_temp_dir, delete=False) as tmpfile:
                    sf.write(tmpfile.name, audio_data, sample_rate)
                    audio_input_path = tmpfile.name
                    tmp_file_to_delete = tmpfile.name

            elif isinstance(audio_data, str):
                audio_input_path = audio_data
            else:
                raise TypeError(f"Unsupported audio_data type: {type(audio_data)}")

            features = self.model(audio_input_path)
            features = features.to(self.device)

            original_shape = features.shape

            target_shape = (768, 512)

            if features.numel() == target_shape[0] * target_shape[1]:
                reshaped_features = features.reshape(target_shape)
            else:
                logger.warning(
                    "Unexpected feature shape %s from AudioMAE model."
                    " Total elements %d do not match expected %d for target shape %s. Returning empty tensor.",
                    original_shape,
                    features.numel(),
                    target_shape[0] * target_shape[1],
                    target_shape,
                )
                return torch.empty(0, device=self.device).cpu()

            return reshaped_features.cpu()

        except Exception as e:
            logger.error("AudioMAE feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0).cpu()

        finally:
            if tmp_file_to_delete and os.path.exists(tmp_file_to_delete):
                try:
                    os.unlink(tmp_file_to_delete)
                except OSError as e:
                    logger.error("Error deleting temporary file %s: %s", tmp_file_to_delete, e)
                    passfrom abc import ABC, abstractmethod
from typing import Any, Union, Dict
import numpy as np
import torch


class FeatureExtractor(ABC):
    """Abstract Base Class for all feature extractors."""

    def __init__(self, device: str = "cpu", **kwargs):
        """
        Initializes the FeatureExtractor base class.

        Args:
            device (str): The device (e.g., 'cpu', 'cuda') to use for feature extraction.
            **kwargs: Additional keyword arguments passed to the _initialize method.
        """
        self.device = torch.device(device)
        self._initialize(**kwargs)

    def _initialize(self, **kwargs):
        """
        Optional initialization hook for subclasses.

        Subclasses can override this method to perform specific setup tasks
        after the instance is created but before extraction begins.

        Args:
            **kwargs: Keyword arguments passed during initialization.
        """
        pass

    @abstractmethod
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> Any:
        """
        Extract features from audio data.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform as a numpy array or torch tensor.
            sample_rate (int): Sample rate of the audio data in Hz.
            **kwargs (Any): Additional keyword arguments specific to the extractor implementation.

        Returns:
            Any: The extracted features. The format and type depend on the specific
                 subclass implementation.
        """
        passimport logging
from typing import Any, Union
import numpy as np
import torch
import librosa
from transformers import ClapModel, ClapProcessor
from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class CLAPExtractor(FeatureExtractor):
    """
    Feature extractor using the CLAP model from LAION on Hugging Face.
    """

    def _initialize(self, model_id: str = "laion/larger_clap_general", **kwargs):
        """
        Load the pre-trained CLAP model and processor.

        Args:
            model_id (str): The Hugging Face model identifier.
            **kwargs: Additional arguments (ignored).
        """
        logger.info("Loading CLAP model and processor '%s'...", model_id)
        try:
            self.model = ClapModel.from_pretrained(model_id).to(self.device)
            self.processor = ClapProcessor.from_pretrained(model_id)
            self.target_sr = self.processor.feature_extractor.sampling_rate
            self.model.eval()
            logger.info("CLAP model and processor loaded successfully.")
        except Exception as e:
            logger.error("Failed to load CLAP model '%s': %s", model_id, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extract audio features using the CLAP model.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform
                (numpy array or torch tensor). Expected to be MONO.
            sample_rate (int): Sample rate of the audio data.

        Returns:
            torch.Tensor: The extracted audio embedding tensor ([1, embedding_dim])
                          on CPU.
        """
        try:
            if isinstance(audio_data, torch.Tensor):
                audio_data = audio_data.cpu().numpy()

            if audio_data.ndim > 1:
                audio_data = np.mean(audio_data, axis=0)
            if sample_rate != self.target_sr:
                audio_data = librosa.resample(audio_data, orig_sr=sample_rate, target_sr=self.target_sr)
                sample_rate = self.target_sr

            inputs = self.processor(audios=audio_data, sampling_rate=sample_rate, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            audio_features = self.model.get_audio_features(**inputs)

            return audio_features.cpu()
        except Exception as e:
            logger.error("CLAP feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0)import logging
from typing import Any, Union, Optional
import torch
import torch.nn.functional as F
import numpy as np
import dac
from audiotools import AudioSignal
from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class DACExtractor(FeatureExtractor):
    """
    Feature extractor using the Descript Audio Codec (DAC).
    This extractor can return the continuous latent 'z', discrete 'codes',
    or the 'latents' from the RVQ bottleneck. It can ensure a fixed-length
    output by padding or truncating if `max_len` is specified.
    """

    def _initialize(
        self,
        model_type: str = "44khz",
        output_type: str = "latents",
        max_len: Optional[int] = None,
        **kwargs,
    ):
        """
        Load the pre-trained DAC model.

        Args:
            model_type (str): The model variant to load ('44khz', '24khz', '16khz').
            output_type (str): The type of feature to return. One of:
                               - 'z': The continuous latent vector before quantization.
                               - 'codes': The discrete integer codes from the RVQ.
                               - 'latents': The continuous vectors from the RVQ bottleneck.
                               - 'all': Concatenation of z, codes (flattened), and latents.
            max_len (Optional[int]): If specified, the number of time steps for the output feature.
                                     Shorter sequences will be padded, longer ones will be truncated.
                                     If None, features are returned with their natural length.
            **kwargs: Additional arguments (ignored).
        """
        logger.info("Loading Descript Audio Codec (DAC) model '%s'...", model_type)
        try:
            model_path = dac.utils.download(model_type=model_type)
            self.model = dac.DAC.load(model_path).to(self.device)
            self.model.eval()

            self.output_type = output_type.lower()
            self.max_len = max_len

            valid_outputs = ["z", "codes", "latents", "all"]
            if self.output_type not in valid_outputs:
                raise ValueError(f"output_type must be one of {valid_outputs}")

            log_msg = f"DAC model '{model_type}' loaded. Output type: '{self.output_type}'."
            if self.max_len is not None:
                log_msg += f" Output length fixed to {self.max_len}."
            else:
                log_msg += " Output length is variable."
            logger.info(log_msg)

        except Exception as e:
            logger.error("Failed to load DAC model '%s': %s", model_type, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extracts features using the DAC model.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform.
            sample_rate (int): Sample rate of the audio data.

        Returns:
            torch.Tensor: The extracted features as a tensor on the CPU. The time dimension
                          will be padded/truncated only if `max_len` was set during initialization.
        """
        try:
            if isinstance(audio_data, torch.Tensor):
                audio_data = audio_data.cpu().numpy()

            signal = AudioSignal(audio_data, sample_rate)
            signal.to(self.model.device)
            x = self.model.preprocess(signal.audio_data, signal.sample_rate)
            z, codes, latents, _, _ = self.model.encode(x)
            z = z.squeeze(0)
            codes = codes.squeeze(0)
            latents = latents.squeeze(0)

            features = None
            if self.output_type == "z":
                features = z.float()
            elif self.output_type == "codes":
                features = codes.float()
            elif self.output_type == "latents":
                features = latents.float()
            elif self.output_type == "all":
                features = torch.cat([z, codes.float(), latents], dim=0)

            if features is None:
                raise ValueError(f"Feature extraction failed for output_type '{self.output_type}'")

            if self.max_len is not None:
                current_len = features.shape[-1]
                if current_len < self.max_len:
                    pad_amount = self.max_len - current_len
                    features = F.pad(features, (0, pad_amount), "constant", 0)
                elif current_len > self.max_len:
                    features = features[..., : self.max_len]

            return features.cpu()

        except Exception as e:
            logger.error("DAC feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0)import logging
from typing import Union, Any
import numpy as np
import torch
import torchaudio
from transformers import EncodecModel, AutoProcessor

from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class EncodecExtractor(FeatureExtractor):
    """
    Feature extractor using the EnCodec model from Facebook/Meta on Hugging Face.
    """

    def _initialize(self, model_id: str = "facebook/encodec_24khz", bandwidth: float = 6.0, **kwargs):
        """
        Load the pre-trained Encodec model and processor.

        Args:
            model_id (str): The Hugging Face model identifier (e.g., "facebook/encodec_24khz").
            bandwidth (float): Target bandwidth for the model (e.g., 1.5, 3.0, 6.0, 12.0, 24.0).
                               This influences the returned codebook size/indices.
            **kwargs: Additional arguments (ignored).
        """
        logger.info("Loading Encodec model and processor '%s'...", model_id)
        try:
            self.model = EncodecModel.from_pretrained(model_id).to(self.device)
            self.processor = AutoProcessor.from_pretrained(model_id)
            self.target_sr = self.processor.sampling_rate
            self.bandwidth = bandwidth
            self.model.eval()
            logger.info(f"Encodec model loaded successfully (Target SR: {self.target_sr} Hz, BW: {self.bandwidth} kbps).")
        except Exception as e:
            logger.error("Failed to load Encodec model '%s': %s", model_id, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extract features (discrete codes) using the Encodec model's encoder.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform
                (numpy array or torch tensor). Assumed MONO.
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            torch.Tensor: The extracted discrete codes tensor ([1, num_codebooks, num_frames])
                        on CPU.
        """
        try:
            if isinstance(audio_data, np.ndarray):
                audio_data = torch.from_numpy(audio_data).float()

            if audio_data.ndim == 1:
                audio_data = audio_data.unsqueeze(0)
            elif audio_data.ndim == 2 and audio_data.shape[0] > 1:
                audio_data = torch.mean(audio_data, dim=0, keepdim=True)
            elif audio_data.ndim != 2 or audio_data.shape[0] != 1:
                raise ValueError(f"Expected mono audio with 1 channel, got shape {audio_data.shape}")

            audio_data = audio_data.to(self.device)

            if sample_rate != self.target_sr:
                logger.debug("Resampling audio from %d Hz to %d Hz.", sample_rate, self.target_sr)
                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.target_sr).to(self.device)
                audio_data = resampler(audio_data)

            audio_np = audio_data.cpu().numpy()
            audio_np = audio_np.squeeze(0)

            inputs = self.processor(raw_audio=audio_np, sampling_rate=self.target_sr, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            encoder_outputs = self.model.encode(inputs["input_values"], inputs["padding_mask"])
            codes = encoder_outputs.audio_codes

            if codes.shape[0] == 1:
                codes = codes.squeeze(0)

            return codes.cpu()
        except Exception as e:
            logger.error("Encodec feature extraction failed: %s", e, exc_info=True)
            raise"""Mel Spectrogram feature extractor."""

import logging
from typing import Any, Optional, Union

import numpy as np
import torch
import torch.nn.functional as F
import torchaudio.transforms as T

from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class MelExtractor(FeatureExtractor):
    """
    Computes Mel spectrograms using torchaudio.
    """

    def _initialize(
        self,
        sr: int = 16000,
        n_fft: int = 1024,
        hop_length: Optional[int] = None,
        win_length: Optional[int] = None,
        n_mels: int = 80,
        f_min: float = 0.0,
        f_max: Optional[float] = None,
        power: float = 2.0,
        normalized: bool = False,
        log_scale: bool = True,
        **kwargs,
    ):
        """
        Initialize the Mel spectrogram extractor.

        Args:
            sr (int): Target sample rate.
            n_fft (int): Size of the FFT window.
            hop_length (Optional[int]): The number of frames between successive STFT windows. Defaults to n_fft // 4.
            win_length (Optional[int]): The window length. Defaults to n_fft.
            n_mels (int): Number of Mel filter banks.
            f_min (float): Minimum frequency.
            f_max (Optional[float]): Maximum frequency. Defaults to sr / 2.0.
            power (float): Exponent for the magnitude spectrogram (2.0 for power spectrogram).
            normalized (bool): Whether to normalize Mel banks.
            log_scale (bool): Whether to convert to log scale using AmplitudeToDB.
            **kwargs: Additional keyword arguments (ignored).
        """
        self.target_sr = sr
        self.log_scale = log_scale
        self.n_fft = n_fft

        _hop_length = hop_length if hop_length is not None else n_fft // 4
        _win_length = win_length if win_length is not None else n_fft
        _f_max = f_max if f_max is not None else sr / 2.0

        self._win_length = _win_length
        self._hop_length = _hop_length

        self.mel_transform = T.MelSpectrogram(
            sample_rate=sr,
            n_fft=n_fft,
            win_length=_win_length,
            hop_length=_hop_length,
            f_min=f_min,
            f_max=_f_max,
            n_mels=n_mels,
            power=power,
            normalized=normalized,
            center=True,
        ).to(self.device)

        if self.log_scale:
            _stype = "power" if power == 2.0 else "magnitude"
            self.log_scaler = T.AmplitudeToDB(stype=_stype, top_db=80.0).to(
                self.device
            )

        logger.info(
            f"MelExtractor initialized (SR: {sr}, N_FFT: {n_fft}, Hop: {_hop_length}, Mels: {n_mels})"
        )

    def extract(
        self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any
    ) -> torch.Tensor:
        """
        Compute the Mel spectrogram.

        Handles resampling, mono conversion, and padding for short inputs.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform
                (numpy array or torch tensor). Can be mono or stereo.
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            torch.Tensor: The computed Mel spectrogram tensor ([n_mels, num_frames]) on CPU,
                          or an empty tensor if extraction fails.
        """
        try:
            if isinstance(audio_data, np.ndarray):
                if audio_data.dtype != np.float32:
                    audio_data = audio_data.astype(np.float32)
                audio_tensor = torch.from_numpy(audio_data)
            elif isinstance(audio_data, torch.Tensor):
                if audio_data.dtype != torch.float32:
                    audio_tensor = audio_data.float()
                else:
                    audio_tensor = audio_data
            else:
                raise TypeError("Input audio_data must be ndarray or tensor")

            audio_tensor = audio_tensor.to(self.device)

            if sample_rate != self.target_sr:
                resampler = T.Resample(
                    orig_freq=sample_rate, new_freq=self.target_sr
                ).to(self.device)
                audio_tensor = resampler(audio_tensor)

            if audio_tensor.ndim > 1:
                if audio_tensor.shape[0] > 1 and audio_tensor.shape[1] > 1:
                    audio_tensor = torch.mean(audio_tensor, dim=0)
                elif audio_tensor.shape[0] == 1:
                    audio_tensor = audio_tensor.squeeze(0)
                elif audio_tensor.shape[1] == 1:
                    audio_tensor = audio_tensor.squeeze(1)
                elif audio_tensor.ndim > 2:
                    raise ValueError(f"Unsupported audio dimensionality {audio_tensor.ndim}. Expected 1D or 2D [C, T].")

            min_len = self._win_length
            current_len = audio_tensor.shape[-1] if audio_tensor.ndim > 0 else 0

            if current_len == 0:
                logger.warning("Input audio is empty after potential resampling/mono.")
                return torch.empty(0, self.mel_transform.n_mels).transpose(0, 1).cpu()

            if current_len < min_len:
                pad_amount = min_len - current_len
                audio_tensor = F.pad(
                    audio_tensor,
                    (pad_amount // 2, pad_amount - pad_amount // 2),
                    mode="constant",
                    value=0,
                )
                logger.debug("Padded short audio to length %d", audio_tensor.shape[-1])

            audio_tensor_input = audio_tensor.unsqueeze(0).unsqueeze(0)

            mel_spec = self.mel_transform(audio_tensor_input)

            if self.log_scale:
                mel_spec = torch.clamp(mel_spec, min=1e-10)
                mel_spec = self.log_scaler(mel_spec)
                if not torch.isfinite(mel_spec).all():
                    logger.warning("Non-finite values found in MelSpec AFTER log scaling. Replacing.")
                    mel_spec = torch.nan_to_num(
                        mel_spec, nan=0.0, posinf=torch.max(mel_spec[torch.isfinite(mel_spec)]), neginf=-80.0
                    )

            return mel_spec.squeeze(0).squeeze(0).cpu()

        except Exception as e:
            input_shape_str = str(getattr(audio_tensor, "shape", "N/A"))
            logger.error("Mel extraction failed for initial input shape %s: %s", input_shape_str, e, exc_info=True)
            return torch.empty(0).cpu()import logging
from typing import List, Union, Any, Optional
import numpy as np
import torch
import torchaudio.transforms as T
from transformers import Wav2Vec2Processor, Wav2Vec2Model

from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class Wav2Vec2Extractor(FeatureExtractor):
    """
    Feature extractor using Wav2Vec2 models from Hugging Face (e.g., facebook/wav2vec2-base-960h).
    """

    def _initialize(self, model_id: str = "facebook/wav2vec2-base-960h", output_hidden_states: bool = False, **kwargs):
        """
        Load the pre-trained Wav2Vec2 model and processor.

        Args:
            model_id (str): The Hugging Face model identifier.
            output_hidden_states (bool): If True, return all hidden states instead of just the last one.
            **kwargs: Additional arguments (ignored).
        """
        logger.info("Loading Wav2Vec2 model and processor '%s'...", model_id)
        try:
            self.processor = Wav2Vec2Processor.from_pretrained(model_id)
            self.model = Wav2Vec2Model.from_pretrained(model_id).to(self.device)
            self.target_sr = self.processor.feature_extractor.sampling_rate
            self.output_hidden_states = output_hidden_states
            self.model.eval()
            logger.info("Wav2Vec2 model and processor loaded successfully.")
        except Exception as e:
            logger.error("Failed to load Wav2Vec2 model '%s': %s", model_id, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> Union[torch.Tensor, List[torch.Tensor]]:
        """
        Extract features using the Wav2Vec2 model.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform
                (numpy array or torch tensor). Expected to be MONO.
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            Union[torch.Tensor, List[torch.Tensor]]: The extracted features.
                - If output_hidden_states is False: Returns the last hidden state tensor
                  ([1, sequence_length, hidden_size]).
                - If output_hidden_states is True: Returns a list of tensors for all hidden states.
                Returns tensor(s) on CPU.
        """
        try:
            if isinstance(audio_data, torch.Tensor):
                audio_data = audio_data.cpu().numpy()

            if audio_data.ndim > 1:
                logger.warning("Wav2Vec2 expects mono audio, input has %d dims. Taking mean.", audio_data.ndim)
                audio_data = np.mean(audio_data, axis=0)

            inputs = self.processor(audio_data, sampling_rate=sample_rate, return_tensors="pt", padding=True)

            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            outputs = self.model(**inputs, output_hidden_states=self.output_hidden_states)

            if self.output_hidden_states:
                features = [h.cpu() for h in outputs.hidden_states]
            else:
                features = outputs.last_hidden_state.cpu()

            return features
        except Exception as e:
            logger.error("Wav2Vec2 feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0)import logging
from typing import Any, Union, List
import numpy as np
import torch
from transformers import AutoFeatureExtractor, WavLMModel

from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class WavLMExtractor(FeatureExtractor):
    """
    Feature extractor using WavLM models from Hugging Face (e.g., microsoft/wavlm-large).
    """

    def _initialize(self, model_id: str = "microsoft/wavlm-large", output_hidden_states: bool = False, **kwargs):
        """
        Load the pre-trained WavLM model and its feature extractor.

        Args:
            model_id (str): The Hugging Face model identifier (e.g., "microsoft/wavlm-large").
            output_hidden_states (bool): If True, return all hidden states instead of just the last one.
            **kwargs: Additional arguments passed to the base class (e.g., device).
        """
        logger.info("Loading WavLM model and feature extractor '%s'...", model_id)
        try:
            self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)
            self.model = WavLMModel.from_pretrained(model_id).to(self.device)
            self.target_sr = self.feature_extractor.sampling_rate
            self.output_hidden_states = output_hidden_states
            self.model.eval()
            logger.info("WavLM model and feature extractor loaded successfully.")
        except Exception as e:
            logger.error("Failed to load WavLM model/feature extractor '%s': %s", model_id, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> Union[torch.Tensor, List[torch.Tensor]]:
        """
        Extract features using the WavLM model.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform
                (numpy array or torch tensor). Expected to be MONO.
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            Union[torch.Tensor, List[torch.Tensor]]: The extracted features.
                - If output_hidden_states is False: Returns the last hidden state tensor
                  ([1, sequence_length, hidden_size]).
                - If output_hidden_states is True: Returns a list of tensors for all hidden states.
                Returns tensor(s) on CPU.
        """
        try:
            if isinstance(audio_data, np.ndarray):
                audio_data = audio_data.astype(np.float32)
            elif isinstance(audio_data, torch.Tensor):
                audio_data = audio_data.float().cpu().numpy()
            else:
                logger.error("Unsupported audio_data type: %s", type(audio_data))
                return torch.empty(0, device="cpu")

            if audio_data.ndim > 1:
                if audio_data.shape[0] > 1 and audio_data.shape[1] > 1:
                    logger.warning("Input audio has more than one channel. Averaging channels to mono.")
                    channel_axis = 0 if audio_data.shape[0] < audio_data.shape[1] else 1
                    audio_data = np.mean(audio_data, axis=channel_axis)
                elif audio_data.shape[0] == 1 or audio_data.shape[1] == 1:
                    audio_data = audio_data.flatten()
            audio_data = audio_data.squeeze()

            inputs = self.feature_extractor(audio_data, sampling_rate=sample_rate, return_tensors="pt", padding=True)

            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            outputs = self.model(**inputs, output_hidden_states=self.output_hidden_states)

            if self.output_hidden_states:
                features = [h.detach().cpu() for h in outputs.hidden_states]
            else:
                features = outputs.last_hidden_state.detach().cpu()

            return features
        except Exception as e:
            logger.error("WavLM feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0, device="cpu")import logging
from typing import Union, Any, List
import numpy as np
import torch
from transformers import WhisperProcessor, WhisperModel
import librosa

from features.base import FeatureExtractor

logger = logging.getLogger(__name__)


class WhisperEncoderExtractor(FeatureExtractor):
    """
    Feature extractor using the encoder part of Whisper models from Hugging Face.
    """

    def _initialize(
        self,
        model_id: str = "openai/whisper-large-v3",
        output_hidden_states: bool = False,
        **kwargs,
    ):
        """
        Load the pre-trained Whisper model and processor.

        Args:
            model_id: The Hugging Face model identifier.
            output_hidden_states: If True, return all hidden states.
        """
        logger.info(f"Loading Whisper model and processor '{model_id}'...")
        try:
            self.processor = WhisperProcessor.from_pretrained(model_id)
            self.model = WhisperModel.from_pretrained(model_id).to(self.device)
            self.target_sr = self.processor.feature_extractor.sampling_rate
            self.output_hidden_states = output_hidden_states
            self.model.eval()
            logger.info(f"Whisper model and processor loaded successfully on device {self.device}.")
        except Exception as e:
            logger.error(f"Failed to load Whisper model '{model_id}': {e}", exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> Union[torch.Tensor, List[torch.Tensor]]:
        """
        Extract features using the Whisper encoder. Handles resampling to 16kHz.

        Args:
            audio_data: Input audio waveform (numpy array or torch tensor). Mono expected.
            sample_rate: Sample rate of the audio data.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            Features from the encoder. Last hidden state ([1, sequence_length, hidden_size])
            or list of all hidden states if output_hidden_states is True. Tensor(s) on CPU.
        """
        try:
            if isinstance(audio_data, torch.Tensor):
                audio_np = audio_data.float().cpu().numpy()
            elif isinstance(audio_data, np.ndarray):
                audio_np = audio_data.astype(np.float32)
            else:
                raise TypeError("audio_data must be numpy array or torch tensor")

            if audio_np.ndim > 1:
                logger.warning(f"Whisper expects mono audio, input has {audio_np.ndim} dims. Taking mean.")
                channel_axis = 0 if audio_np.shape[0] < audio_np.shape[1] else 1
                audio_np = np.mean(audio_np, axis=channel_axis)

            if sample_rate != self.target_sr:
                logger.debug(f"Resampling audio from {sample_rate} Hz to {self.target_sr} Hz...")
                try:
                    audio_np = librosa.resample(y=audio_np, orig_sr=sample_rate, target_sr=self.target_sr)
                    current_sample_rate = self.target_sr
                    logger.debug("Resampling successful.")
                except Exception as resample_err:
                    logger.error(f"Librosa resampling failed: {resample_err}", exc_info=True)
                    return torch.empty(0, device="cpu")
            else:
                current_sample_rate = sample_rate

            inputs = self.processor(audio_np, sampling_rate=current_sample_rate, return_tensors="pt")
            input_features = inputs.input_features.to(self.device)

            encoder_outputs = self.model.encoder(input_features, output_hidden_states=self.output_hidden_states)

            if self.output_hidden_states:
                features = [h.cpu() for h in encoder_outputs.hidden_states]
            else:
                features = encoder_outputs.last_hidden_state.cpu()
            return features

        except Exception as e:
            logger.error(f"Whisper Encoder feature extraction failed: {e}", exc_info=True)
            return torch.empty(0, device="cpu")import logging
from typing import Optional

import numpy as np
import torch
from transformers.audio_utils import mel_filter_bank

logger = logging.getLogger(__name__)

CUSTOM_N_MELS = 128


class WhisperSegFrontend:
    """
    Calculates the 128-bin log-Mel spectrogram required by WhisperSeg
    for a single, pre-processed audio chunk.
    """

    def __init__(
        self,
        sr: int,
        n_fft: int,
        hop_length: int,
        min_frequency: float = 0.0,
        max_frequency: Optional[float] = None,
    ):
        """
        Initializes the WhisperSegFrontend.

        Args:
            sr (int): The sampling rate of the audio.
            n_fft (int): The FFT size for the STFT.
            hop_length (int): The hop length for the STFT.
            min_frequency (float): The minimum frequency for the Mel filterbank.
            max_frequency (Optional[float]): The maximum frequency for the Mel filterbank.
        """
        self.sr = sr
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.num_freq_bins = 1 + self.n_fft // 2
        self.num_mel_bins = CUSTOM_N_MELS

        _max_frequency = max_frequency if max_frequency is not None else float(sr // 2)

        try:
            mel_filters_np = mel_filter_bank(
                num_frequency_bins=self.num_freq_bins,
                num_mel_filters=self.num_mel_bins,
                min_frequency=min_frequency,
                max_frequency=_max_frequency,
                sampling_rate=sr,
                norm="slaney",
                mel_scale="htk",
            )
            mel_filters_np_transposed = mel_filters_np.T
            self.mel_filters_torch = torch.from_numpy(mel_filters_np_transposed).float()

            self.window = torch.hann_window(self.n_fft)
            logger.debug(f"Created Mel filterbank tensor with shape: {self.mel_filters_torch.shape}")

        except Exception as e:
            logger.error(f"Failed to create Mel filterbank: {e}", exc_info=True)
            raise

    def __call__(self, audio_chunk_padded: np.ndarray) -> Optional[np.ndarray]:
        """
        Calculates the log-mel spectrogram for a single audio chunk.

        Args:
            audio_chunk_padded (np.ndarray): A 1D numpy array containing the audio chunk,
                                             zero-padded to be a multiple of hop_length
                                             plus n_fft at the end.

        Returns:
            Optional[np.ndarray]: The log-mel spectrogram as a numpy array
                                  of shape [num_mel_bins, num_frames], or None if an error occurs.
        """
        if not isinstance(audio_chunk_padded, np.ndarray):
            logger.error("Input audio_chunk_padded must be a numpy array.")
            return None
        if audio_chunk_padded.ndim != 1:
            logger.error(f"Input audio_chunk_padded must be 1D, got shape {audio_chunk_padded.shape}")
            return None
        if audio_chunk_padded.dtype != np.float32:
            audio_chunk_padded = audio_chunk_padded.astype(np.float32)

        try:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            waveform_tensor = torch.from_numpy(audio_chunk_padded).to(device)
            window_d = self.window.to(device)
            mel_filters_tensor_d = self.mel_filters_torch.to(device)

            stft = torch.stft(
                waveform_tensor,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                win_length=self.n_fft,
                window=window_d,
                center=True,
                pad_mode="reflect",
                normalized=False,
                onesided=True,
                return_complex=True,
            )
            if stft.shape[0] != self.num_freq_bins:
                logger.error(f"STFT output freq bins {stft.shape[0]} != expected {self.num_freq_bins}")
                return None

            magnitudes_power = stft.abs() ** 2

            if mel_filters_tensor_d.shape[1] != magnitudes_power.shape[0]:
                logger.error(
                    f"Cannot multiply Mel filter ({mel_filters_tensor_d.shape}) and STFT"
                    f" ({magnitudes_power.shape}): Freq bins mismatch."
                )
                return None

            mel_spec = torch.matmul(mel_filters_tensor_d, magnitudes_power)

            log_spec = torch.log10(torch.clamp(mel_spec, min=1e-10))
            if log_spec.numel() > 0:
                log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
            else:
                return np.zeros((self.num_mel_bins, 0), dtype=np.float32)
            log_spec = (log_spec + 4.0) / 4.0

            return log_spec.cpu().numpy()

        except Exception as spec_err:
            logger.error(f"Error computing spectrogram: {spec_err}", exc_info=True)
            return None"""
Constants potentially used by WhisperSeg components.
"""
RATIO_DECODING_TIME_STEP_TO_SPEC_TIME_STEP = 2
import logging
import json
from pathlib import Path
from typing import Union, Any, Optional, List, Dict, Tuple

import ctranslate2
import numpy as np
import torch

from features.base import FeatureExtractor
from features.whisperseg.audio_frontend import WhisperSegFrontend
from features.whisperseg.model_loader import load_whisperseg_model

logger = logging.getLogger(__name__)

DEFAULT_TOTAL_SPEC_COLUMNS = 1500


class WhisperSegExtractor(FeatureExtractor):
    """
    Feature extractor using the WhisperSeg model (CTranslate2 format).
    """

    def _initialize(
        self,
        model_path: str,
        device_index: Union[int, List[int]] = 0,
        compute_type: Optional[str] = None,
        batch_size: int = 8,
        output_type: str = "embedding",
        default_spec_time_step: float = 0.02,
        default_min_frequency: float = 0.0,
        default_num_trials: int = 1,
        **kwargs,
    ):
        """
        Load the CTranslate2 model and tokenizer. Stores default parameters.

        Args:
            model_path (str): Path to the CTranslate2 model directory.
            device_index (Union[int, List[int]]): Device index(es) for CTranslate2.
            compute_type (Optional[str]): Compute type for CTranslate2 (e.g., 'float16', 'int8').
            batch_size (int): Batch size for the CTranslate2 encoder.
            output_type (str): Type of output features ('embedding' or 'spectrogram').
            default_spec_time_step (float): Default time step for spectrogram frames.
            default_min_frequency (float): Default minimum frequency for the Mel filterbank.
            default_num_trials (int): Default number of padding/chunking trials.
            **kwargs: Additional keyword arguments (ignored).

        Returns:
            None
        """
        logger.info(f"Initializing WhisperSegExtractor (Original Loop Mode) with model: {model_path}")
        self.model, self.tokenizer = load_whisperseg_model(
            model_path, device=self.device.type, device_index=device_index, compute_type=compute_type
        )
        self.batch_size = batch_size
        self.output_type = output_type.lower()
        if self.output_type not in ["embedding", "spectrogram"]:
            raise ValueError("output_type must be 'embedding' or 'spectrogram'")

        self.default_spec_time_step = default_spec_time_step
        self.default_min_frequency = default_min_frequency
        self.default_num_trials = default_num_trials

        config_path = Path(model_path) / "config.json"
        model_cfg = {}
        if config_path.is_file():
            try:
                with open(config_path, "r") as f:
                    model_cfg = json.load(f)
            except Exception as e:
                logger.warning("Could not read config.json: %s", e)
        self.total_spec_columns = model_cfg.get("total_spec_columns", DEFAULT_TOTAL_SPEC_COLUMNS)

        self._frontend_cache: Dict[Tuple[int, float, float], WhisperSegFrontend] = {}

        logger.debug(
            "WhisperSeg Extractor (Original Loop) Initialized: Output='%s',"
            " TotalSpecCols=%s, Defaults(step=%s, fmin=%s, trials=%s)"
            " CT2Device=%s:%s, CT2Compute=%s",
            self.output_type,
            self.total_spec_columns,
            self.default_spec_time_step,
            self.default_min_frequency,
            self.default_num_trials,
            self.device.type,
            device_index,
            self.model.compute_type,
        )

    def _get_frontend_calculator(self, sr: int, spec_time_step: float, min_frequency: float) -> WhisperSegFrontend:
        """
        Gets or creates a frontend calculator instance.

        Args:
            sr (int): Sample rate.
            spec_time_step (float): Spectrogram time step.
            min_frequency (float): Minimum frequency for Mel filterbank.

        Returns:
            WhisperSegFrontend: Frontend calculator instance.
        """
        key = (sr, spec_time_step, min_frequency)
        if key not in self._frontend_cache:
            logger.debug("Creating frontend calculator for SR=%d, Step=%f, Fmin=%f", sr, spec_time_step, min_frequency)
            if sr <= 32000:
                n_fft = 512
            elif sr <= 80000:
                n_fft = 1024
            elif sr <= 150000:
                n_fft = 2048
            elif sr <= 300000:
                n_fft = 4096
            else:
                n_fft = 8192
            hop_length = int(round(spec_time_step * sr))

            self._frontend_cache[key] = WhisperSegFrontend(
                sr=sr,
                n_fft=n_fft,
                hop_length=hop_length,
                min_frequency=min_frequency,
                max_frequency=None,
            )
        return self._frontend_cache[key]

    @torch.no_grad()
    def extract(
        self,
        audio_data: Union[np.ndarray, torch.Tensor],
        sample_rate: int,
        spec_time_step: Optional[float] = None,
        min_frequency: Optional[float] = None,
        num_trials: Optional[int] = None,
        **kwargs: Any,
    ) -> torch.Tensor:
        """
        Extract features using the original WhisperSeg padding/chunking loop.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform.
            sample_rate (int): Sample rate of the audio.
            spec_time_step (Optional[float]): Spectrogram time step (overrides default).
            min_frequency (Optional[float]): Minimum frequency (overrides default).
            num_trials (Optional[int]): Number of padding/chunking trials (overrides default).
            **kwargs (Any): Additional keyword arguments.

        Returns:
            torch.Tensor: Extracted features (embeddings or spectrogram chunks).
        """
        _spec_time_step = spec_time_step if spec_time_step is not None else self.default_spec_time_step
        _min_frequency = min_frequency if min_frequency is not None else self.default_min_frequency
        _num_trials = num_trials if num_trials is not None else self.default_num_trials

        try:
            if isinstance(audio_data, torch.Tensor):
                audio_np = audio_data.cpu().numpy()
            elif isinstance(audio_data, np.ndarray):
                audio_np = audio_data
            else:
                raise TypeError("Input must be numpy array or torch tensor.")

            if audio_np.dtype != np.float32:
                audio_np = audio_np.astype(np.float32)
            if audio_np.ndim > 1:
                min_dim = np.argmin(audio_np.shape)
                if audio_np.shape[min_dim] > 1:
                    audio_np = np.mean(audio_np, axis=min_dim)
                else:
                    audio_np = audio_np.squeeze()
            if audio_np.ndim != 1:
                raise ValueError(f"Audio not mono. Shape: {audio_np.shape}")

            frontend_calculator = self._get_frontend_calculator(sample_rate, _spec_time_step, _min_frequency)
            sr = sample_rate

            clip_duration = self.total_spec_columns * _spec_time_step
            max_num_padding_samples = int(np.ceil(clip_duration * sr))
            audio_left_pad = np.zeros(max_num_padding_samples, dtype=np.float32)
            audio_clip_length = int(np.ceil(clip_duration * sr))

            all_valid_feature_chunks = []

            for trial_id in range(_num_trials):
                padding_time = 0.0
                if _num_trials > 1:
                    padding_time = np.round(clip_duration * trial_id / _num_trials / _spec_time_step) * _spec_time_step
                num_padding_samples = int(np.round(padding_time * sr))

                current_left_pad = audio_left_pad[len(audio_left_pad) - num_padding_samples :]
                audio_padded = np.concatenate([current_left_pad, audio_np], axis=0)

                for pos in range(0, max(1, len(audio_padded)), audio_clip_length):
                    audio_clip = audio_padded[pos : pos + audio_clip_length]
                    if len(audio_clip) < audio_clip_length:
                        padding_needed = audio_clip_length - len(audio_clip)
                        audio_clip_padded = np.pad(audio_clip, (0, padding_needed), mode="constant", constant_values=0.0)
                    else:
                        audio_clip_padded = audio_clip

                    input_features = frontend_calculator(audio_clip_padded)

                    if input_features is None:
                        logger.warning("Frontend calculator returned None for a chunk. Skipping.")
                        continue

                    current_width = input_features.shape[1]
                    if current_width > self.total_spec_columns:
                        input_features = input_features[:, : self.total_spec_columns]
                    elif current_width < self.total_spec_columns:
                        pad_cols = self.total_spec_columns - current_width
                        pad_val = input_features.min() if input_features.size > 0 else 0.0
                        input_features = np.pad(input_features, ((0, 0), (0, pad_cols)), mode="constant", constant_values=pad_val)

                    if input_features.shape != (frontend_calculator.num_mel_bins, self.total_spec_columns):
                        logger.warning("Final feature chunk shape %s unexpected. Skipping.", input_features.shape)
                        continue

                    all_valid_feature_chunks.append(input_features.astype(np.float32))

            if not all_valid_feature_chunks:
                logger.warning("No valid feature chunks generated after processing audio.")
                return torch.empty(0, dtype=torch.float32, device="cpu")

            if self.output_type == "spectrogram":
                try:
                    spectrograms_tensor = torch.from_numpy(np.stack(all_valid_feature_chunks)).float()
                    logger.debug("Returning %d WS spectrogram chunks, stacked shape: %s", len(all_valid_feature_chunks), spectrograms_tensor.shape)
                    return spectrograms_tensor.cpu()
                except ValueError as e:
                    logger.error("Could not stack spec chunks: %s. Returning empty.", e)
                    return torch.empty(0, dtype=torch.float32, device="cpu")

            elif self.output_type == "embedding":
                all_encoder_outputs_list = []
                num_total_chunks = len(all_valid_feature_chunks)
                logger.debug("Processing %d feature chunks through CTranslate2 encoder...", num_total_chunks)

                for i in range(0, num_total_chunks, self.batch_size):
                    batch_chunks_list = all_valid_feature_chunks[i : i + self.batch_size]
                    try:
                        batch_features_np = np.stack(batch_chunks_list)
                    except ValueError as stack_err:
                        shapes = [f.shape for f in batch_chunks_list]
                        logger.error("Inconsistent shapes in batch %d: %s. Skip. Err: %s", i // self.batch_size, shapes, stack_err)
                        continue

                    features_view = ctranslate2.StorageView.from_array(batch_features_np)
                    encoder_output_sv = self.model.encode(features_view, to_cpu=True)
                    all_encoder_outputs_list.append(torch.tensor(np.array(encoder_output_sv).tolist(), dtype=torch.float32))

                if not all_encoder_outputs_list:
                    logger.warning("No encoder outputs generated.")
                    return torch.empty(0, dtype=torch.float32, device="cpu")

                final_features = torch.cat(all_encoder_outputs_list, dim=0)
                first_dim_size = torch.prod(torch.tensor(final_features.shape[:-1]))
                last_dim_size = final_features.shape[-1]
                final_features = final_features.view(first_dim_size, last_dim_size)
                return final_features.cpu()

        except Exception as e:
            logger.error("WhisperSeg feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0, dtype=torch.float32, device="cpu")"""Helper to load WhisperSeg CTranslate2 model."""

import logging
import os
import json
from pathlib import Path
from typing import List, Tuple, Union, Optional

import ctranslate2
import tokenizers
from transformers import WhisperTokenizer

logger = logging.getLogger(__name__)

VALID_COMPUTE_TYPES = ["float16", "int8_float16", "int8", "float32"]


def load_whisperseg_model(
    model_path: str,
    device: str = "cuda",
    device_index: Union[int, List[int]] = 0,
    compute_type: Optional[str] = None,
) -> Tuple[ctranslate2.models.Whisper, Union[WhisperTokenizer, tokenizers.Tokenizer]]:
    """
    Loads the WhisperSeg CTranslate2 model and associated tokenizer.

    Handles potential differences in tokenizer file locations based on conversion.

    Args:
        model_path (str): Path to the converted CTranslate2 WhisperSeg model directory.
        device (str): Device to load the model onto ('cuda' or 'cpu').
        device_index (Union[int, List[int]]): Index or list of indices for CUDA devices.
        compute_type (Optional[str]): CTranslate2 computation type (e.g., 'float16', 'int8').
                                       If None, defaults based on device (float16/GPU, float32/CPU).

    Returns:
        Tuple[ctranslate2.models.Whisper, Union[WhisperTokenizer, tokenizers.Tokenizer]]: Loaded model and tokenizer.

    Raises:
        FileNotFoundError: If the model path or required files don't exist.
        ValueError: If an invalid compute_type is provided.
        Exception: For other CTranslate2 or Tokenizer loading errors.
    """
    model_path_obj = Path(model_path)
    if not model_path_obj.is_dir():
        raise FileNotFoundError(f"CTranslate2 model directory not found: {model_path}")
    logger.info(f"Loading WhisperSeg CTranslate2 model from: {model_path_obj}")

    if compute_type is None:
        _compute_type = "float16" if device == "cuda" else "float32"
    elif compute_type in VALID_COMPUTE_TYPES:
        _compute_type = compute_type
    else:
        raise ValueError(f"Invalid compute_type '{compute_type}'. Must be one of {VALID_COMPUTE_TYPES}")

    try:
        if isinstance(device_index, list):
            if len(device_index) > 1:
                logger.warning(
                    "Multiple device indices (%s) provided for CTranslate2 Whisper. Loading model onto first device (%s) only.",
                    device_index,
                    device_index[0],
                )
                _device_index = device_index[0]
            elif len(device_index) == 1:
                _device_index = device_index[0]
            else:
                _device_index = 0
                logger.warning("Empty device_index list provided, defaulting to device 0.")
        else:
            _device_index = device_index

        logger.info(f"Using compute_type: {_compute_type}, device: {device}, device_index: {_device_index}")

        model = ctranslate2.models.Whisper(str(model_path_obj), device=device, device_index=_device_index, compute_type=_compute_type)
        logger.info("CTranslate2 model loaded successfully.")
    except Exception as e:
        logger.error("Failed to load CTranslate2 model from %s: %s", model_path_obj, e, exc_info=True)
        raise

    tokenizer = None
    hf_token_path = model_path_obj / "hf_model"
    tok_json_path = model_path_obj / "tokenizer.json"

    try:
        if hf_token_path.is_dir() and (hf_token_path / "tokenizer_config.json").exists():
            tokenizer = WhisperTokenizer.from_pretrained(hf_token_path)
            logger.info(f"Loaded WhisperTokenizer from HF subdirectory: {hf_token_path}")
        elif tok_json_path.is_file():
            tokenizer = tokenizers.Tokenizer.from_file(str(tok_json_path))
            logger.info(f"Loaded base Tokenizer from: {tok_json_path}")
        else:
            config_path = model_path_obj / "config.json"
            original_model_id = None
            if config_path.is_file():
                try:
                    with open(config_path, "r") as f:
                        config = json.load(f)
                        original_model_id = config.get("_name_or_path") or config.get("model_type")
                        if original_model_id and "/" not in original_model_id:
                            original_model_id = None
                except Exception as cfg_err:
                    logger.warning("Error reading config.json: %s", cfg_err)

            if original_model_id:
                logger.warning("Tokenizer files missing. Attempting load from original ID: %s", original_model_id)
                try:
                    tokenizer = WhisperTokenizer.from_pretrained(original_model_id)
                except Exception as fallback_err:
                    logger.error("Failed tokenizer fallback %s: %s", original_model_id, fallback_err)
                    raise FileNotFoundError(f"Could not find/load tokenizer in {model_path_obj} or from fallback.")
            else:
                raise FileNotFoundError(f"Could not find tokenizer files in {model_path_obj}.")

        logger.info("Tokenizer loaded successfully.")
        return model, tokenizer

    except Exception as e:
        logger.error("Failed to load tokenizer for %s: %s", model_path_obj, e, exc_info=True)
        raise# -*- coding: utf-8 -*-
"""
WhisperSeg Feature Extraction Components.
"""

from .extractor import WhisperSegExtractor

__all__ = ["WhisperSegExtractor"]"""
VocSim Benchmark: Feature Extraction Modules.
"""

from features.base import FeatureExtractor
from .audiomae import AudioMAEExtractor
from .clap import CLAPExtractor
from .encodec import EncodecExtractor
from .mel import MelExtractor
from .wav2vec2 import Wav2Vec2Extractor
from .wavlm import WavLMExtractor
from .whisper import WhisperEncoderExtractor
from .whisperseg.extractor import WhisperSegExtractor
from .dac import DACExtractor

__all__ = [
    "FeatureExtractor",
    "AudioMAEExtractor",
    "CLAPExtractor",
    "EncodecExtractor",
    "MelExtractor",
    "Wav2Vec2Extractor",
    "WavLMExtractor",
    "WhisperEncoderExtractor",
    "WhisperSegExtractor",
    "DACExtractor",
]
project_root: "."
results_dir: "d:/data2/vocsim/results/paper_avian"
output_tables_dir: "d:/data2/vocsim/paper_outputs/avian_tables_generated"
features_dir: "d:/data2/vocsim/features/paper_avian"
models_dir: &models_output_dir "d:/data2/vocsim/avian_perception_models"
run_id: "avian_scoped_${now:%Y%m%d_%H%M%S}"
force_cpu: false
target_sample_rate: 16000
feature_chunk_size_gb: 3  
logging:
  level: INFO

dataset:
  id: anonymous-submission000/vocsim-applications-avian-perception
  subset: null
  split: train
  subsets_to_run: [avian_perception]

definitions:
  vae_trainer_def: &vae_trainer_def
    name: PaperVAETrainer
    module: reproducibility.trainers.vae
    params:
      z_dim: 32
      learning_rate: 0.001
      model_precision: 10.0
      vae_frontend_params: &vae_frontend_params
        target_sr: 16000
        n_fft: 512
        hop_length: 256
        win_length: 512
        window_fn_str: "hann"
        spec_height: 128
        spec_width: 128
        window_overlap: 0.5
  vae_model_def: &vae_model_def
    name: VariationalAutoencoder
    module: reproducibility.models.vae
    params:
      z_dim: 32
      model_precision: 10.0
  vae_loop_params: &vae_loop_params
    num_epochs: 50
    batch_size: 64
    save_frequency_epochs: 10
    test_frequency: 10
  ae_trainer_def: &ae_trainer_def
    name: PaperAutoencoderTrainer
    module: reproducibility.trainers.autoencoder
    params:
      learning_rate: 0.0003
      regularization_weight: 0.01
  ae_model_def: &ae_model_def
    name: Autoencoder
    module: reproducibility.models.autoencoder
    params:
      bottleneck_dim: 256
      audio_config: &ae_audio_config
        sr: 16000
        n_mels: 128
        nfft: 2048
        fmin: 100.0
        fmax: 8000.0
      dimensions: &ae_dimensions
        nfft: 2048
        max_spec_width: 512
  ae_loop_params: &ae_loop_params
    num_epochs: 50
    batch_size: 128
    early_stopping_patience: 10
    mixed_precision: true
    save_frequency_epochs: 10
  vae_extractor_params: &vae_extractor_params
    <<: *vae_frontend_params
  ae_extractor_params: &ae_extractor_params
    audio_config: *ae_audio_config
    dimensions: *ae_dimensions
    bottleneck_dim: 256

train: 

  - trainer: *vae_trainer_def
    model: *vae_model_def
    train_on_subset: "all"
    <<: *vae_loop_params 

  - trainer: *ae_trainer_def
    model: *ae_model_def
    train_on_subset: "all"
    <<: *ae_loop_params

feature_extractors: 

  - name: PaperVAEExtractor_all
    module: reproducibility.features.vae
    class: PaperVAEExtractor
    params:
      base_models_dir: *models_output_dir
      model_scope: "all"
      <<: *vae_extractor_params
    short_name: CVC
    benchmark_this: true 

  - name: PaperAutoencoderExtractor_all
    module: reproducibility.features.autoencoder
    class: PaperAutoencoderExtractor
    params:
      base_models_dir: *models_output_dir
      model_scope: "all"
      <<: *ae_extractor_params
    short_name: CAC
    benchmark_this: true 

  - name: MelExtractor
    module: features.mel
    params:
      sr: 16000
      n_mels: 80
      n_fft: 2048
      hop_length: 256
      fmin: 100.0
      fmax: 8000.0
      log_scale: true
    short_name: CM
    benchmark_this: true 

  - name: WhisperEncoderExtractor
    module: features.whisper
    params:
      model_id: "openai/whisper-large-v3"
    short_name: EW
    benchmark_this: true 

  - name: WhisperSegExtractor
    module: features.whisperseg.extractor
    params:
      model_path: "d:/data2/vocsim/models/WhisperSegCompatible/whisper-large-v3-ct2"
      output_type: "embedding"
      default_spec_time_step: 0.020
      default_min_frequency: 100.0
      default_num_trials: 1
    short_name: E
    benchmark_this: true 

  - name: WavLMExtractor
    module: features.wavlm
    params:
      model_id: "microsoft/wavlm-large"
    short_name: WLM
    benchmark_this: true 

  - name: Wav2Vec2Extractor
    module: features.wav2vec2
    params:
      model_id: "facebook/wav2vec2-base-960h"
    short_name: CW
    benchmark_this: true 

  - name: CLAPExtractor
    module: features.clap
    params:
      model_id: "laion/larger_clap_general"
    short_name: CLAP
    benchmark_this: true 

  - name: AudioMAEExtractor
    module: features.audiomae
    params:
      model_id: "hance-ai/audiomae"
      trust_remote_code: true
    short_name: MAE
    benchmark_this: true 

  - name: EncodecExtractor
    module: features.encodec
    params:
      model_id: "facebook/encodec_24khz"
      bandwidth: 6.0
    short_name: CC
    benchmark_this: true 

  - name: WhisperEncoderExtractor_first_row
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    short_name: EWF
    benchmark_this: true 

  - name: WhisperEncoderExtractor_first_row_col
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    short_name: EWTF
    benchmark_this: true 

  - name: WhisperSegExtractor_first_row_col
    base_extractor: WhisperSegExtractor
    averaging: first_row_col
    short_name: ETF
    benchmark_this: true 

  - name: WhisperSegExtractor_first_row
    base_extractor: WhisperSegExtractor
    averaging: first_row
    short_name: EF
    benchmark_this: true 

  - name: WhisperSegExtractor_first_col
    base_extractor: WhisperSegExtractor
    averaging: first_col
    short_name: ET
    benchmark_this: true 

  - name: WhisperSegExtractor_first_row_col_pca_100
    base_extractor: WhisperSegExtractor
    averaging: first_row_col
    pca: 100
    pca_load_chunks: -1 
    short_name: ETF (D=100, PCA)
    benchmark_this: true 

  - name: WavLMExtractor_mean_time_pca_30
    base_extractor: WavLMExtractor
    pca: 30
    pca_load_chunks: -1
    short_name: WLM D30
    benchmark_this: true 

  - name: WavLMExtractor_mean_time_pca_100
    base_extractor: WavLMExtractor
    pca: 100
    pca_load_chunks: -1
    short_name: WLM D100
    benchmark_this: true 

  - name: Wav2Vec2Extractor_mean_time_pca_30
    base_extractor: Wav2Vec2Extractor
    pca: 30
    pca_load_chunks: -1
    short_name: W2V D30
    benchmark_this: true 

  - name: Wav2Vec2Extractor_mean_time_pca_100
    base_extractor: Wav2Vec2Extractor
    pca: 100
    pca_load_chunks: -1
    short_name: W2V D100
    benchmark_this: true 

  - name: CLAPExtractor_pca_30
    base_extractor: CLAPExtractor
    averaging: null
    pca: 30
    pca_load_chunks: -1
    short_name: CLP D30
    benchmark_this: true 

  - name: CLAPExtractor_pca_100
    base_extractor: CLAPExtractor
    averaging: null
    pca: 100
    pca_load_chunks: -1
    short_name: CLP D100
    benchmark_this: true 

  - name: AudioMAEExtractor_mean_time_dim_pca_30
    base_extractor: AudioMAEExtractor
    pca: 30
    pca_load_chunks: -1
    short_name: MAE D30
    benchmark_this: true 

  - name: AudioMAEExtractor_mean_time_dim_pca_100
    base_extractor: AudioMAEExtractor
    pca: 100
    pca_load_chunks: -1
    short_name: MAE D100
    benchmark_this: true

distances: 

  - name: cosine
    params:
      use_torchmetrics: true
      zero_diagonal: true 

  - name: euclidean
    params:
      use_torchmetrics: true
      zero_diagonal: true 

  - name: spearman
    params:
      use_gpu_if_available: true

benchmarks: 

  - name: PerceptualAlignment
    module: benchmarks.perceptual
    params:
      probe_csv_path: "${CODE_DIR}/vocsim/data/avian_perception/probes.csv"
      triplet_csv_path: "${CODE_DIR}/vocsim/data/avian_perception/triplets.csv"
      probe_consistency_threshold: 0.7
      bootstrap_ci: true
      n_bootstraps: 1000

extraction_batch_size: 512 
distance_gpu_block_size: 1024 
project_root: "."
results_dir: "d:/data2/vocsim/results/paper_mouse_identity"
output_tables_dir: "d:/data2/vocsim/paper_outputs/mouse_identity_tables_generated"
features_dir: "d:/data2/vocsim/features/paper_mouse_identity"
models_dir: "d:/data2/vocsim/mouse_identity_models"
run_id: "mouse_identity_${now:%Y%m%d_%H%M%S}"
force_cpu: false
target_sample_rate: 250000 
feature_chunk_size_gb: 3 
logging:
  level: INFO

dataset:
  id: anonymous-submission000/vocsim-applications-mouse-identity
  subset: null
  split: train
  subsets_to_run: [mouse_identity]

train: []  

feature_extractors:

  - name: WhisperEncoderExtractor
    module: features.whisper
    params:
      model_id: "openai/whisper-large-v3"
    benchmark_this: false
    short_name: EW-Raw

  - name: WhisperEncoderExtractor_first_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=10)

  - name: WhisperEncoderExtractor_first_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=30)

  - name: WhisperEncoderExtractor_first_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=100)

  - name: WhisperEncoderExtractor_first_row_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=10)

  - name: WhisperEncoderExtractor_first_row_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=30)

  - name: WhisperEncoderExtractor_first_row_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=100)

  - name: WhisperEncoderExtractor_mean_row_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=10)

  - name: WhisperEncoderExtractor_mean_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=30)

  - name: WhisperEncoderExtractor_mean_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=100)

  - name: WhisperEncoderExtractor_first_row_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=10)

  - name: WhisperEncoderExtractor_first_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=30)

  - name: WhisperEncoderExtractor_first_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=100)

distances: []  
benchmarks:

  - name: ClassificationBenchmark
    module: benchmarks.classification
    params:
      n_splits: 5
      random_state: 42
      classifiers: ["mlp"]
      label_source_key: speaker
      eval_metrics: ["accuracy"]
      top_k: 5
      classifier_params:
        mlp:
          alpha: [0.01, 0.001, 0.0001]
          hidden_layer_sizes: [[400], [200, 200]]
          batch_size: [256]
          random_state: [42]
          max_iter: [10000]

extraction_batch_size: 32  
distance_gpu_block_size: 1024 project_root: "."
results_dir: "d:/data2/vocsim/results/paper_mouse_strain"
output_tables_dir: "d:/data2/vocsim/paper_outputs/mouse_strain_tables_generated"
features_dir: "d:/data2/vocsim/features/paper_mouse_strain"
models_dir: "d:/data2/vocsim/strain_models"
run_id: "mouse_strain_${now:%Y%m%d_%H%M%S}"
force_cpu: false
target_sample_rate: 250000 
feature_chunk_size_gb: 3 
logging:
  level: INFO

dataset:
  id: anonymous-submission000/vocsim-applications-mouse-strain
  subset: null
  split: train
  subsets_to_run: [mouse_strain]

definitions:
  whisper_encoder_params: &whisper_encoder_params
    model_id: "openai/whisper-large-v3"
  classifier_params: &classifier_params
    knn:
      n_neighbors: [3, 10, 30]
      n_jobs: [-1]
    rf:
      max_depth: [10, 15, 20]
      random_state: [42]
      n_jobs: [-1]
    mlp:
      alpha: [0.1, 0.01, 0.001]
      random_state: [42]
      max_iter: [10000]
      hidden_layer_sizes: [[400], [200, 200]]
      batch_size: [256]
      early_stopping: [false]
      solver: [adam]
      activation: [relu]
      learning_rate_init: [0.001]
      learning_rate: [adaptive]
      tol: [0.0001]

train: []  

feature_extractors:

  - name: WhisperEncoderExtractor
    module: features.whisper
    params: *whisper_encoder_params
    benchmark_this: false
    short_name: EW-Raw

  - name: WhisperEncoderExtractor_first_col
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca_load_chunks: -1  
    benchmark_this: true
    short_name: EF

  - name: WhisperEncoderExtractor_first_row
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET

  - name: WhisperEncoderExtractor_first_row_col
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF

  - name: WhisperEncoderExtractor_mean_row_col
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF

  - name: WhisperEncoderExtractor_first_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=10)

  - name: WhisperEncoderExtractor_first_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=30)

  - name: WhisperEncoderExtractor_first_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EF (D=100)

  - name: WhisperEncoderExtractor_first_row_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=10)

  - name: WhisperEncoderExtractor_first_row_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=30)

  - name: WhisperEncoderExtractor_first_row_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ET (D=100)

  - name: WhisperEncoderExtractor_first_row_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=10)

  - name: WhisperEncoderExtractor_first_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=30)

  - name: WhisperEncoderExtractor_first_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: ETF (D=100)

  - name: WhisperEncoderExtractor_mean_row_col_pca_10
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 10
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=10)

  - name: WhisperEncoderExtractor_mean_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 30
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=30)

  - name: WhisperEncoderExtractor_mean_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 100
    pca_load_chunks: -1
    benchmark_this: true
    short_name: EMTF (D=100)

distances: []  

benchmarks:

  - name: ClassificationBenchmark
    module: benchmarks.classification
    params:
      n_splits: 5
      random_state: 42
      classifiers: ["knn", "rf", "mlp"]
      label_source_key: label
      eval_metrics: ["accuracy"]
      top_k: 5
      classifier_params: *classifier_params

extraction_batch_size: 32  
distance_gpu_block_size: 1024  project_root: "."
results_dir: "d:/data2/vocsim/results/vocsim"
# results_dir: "d:/data2/vocsim/results/vocsim_gsr_only"
features_dir: "d:/data2/vocsim/features/vocsim"
models_dir: &models_output_dir "d:/data2/vocsim/paper_models_scoped"
output_tables_dir: "d:/data2/vocsim/paper_outputs/vocsim_tables_generated"
run_id: "vocsim_${now:%Y%m%d_%H%M%S}"
force_cpu: false
target_sample_rate: 16000
feature_chunk_size_gb: 3  
logging:
  level: INFO

# --- Dataset ---
dataset:
  id: anonymous-submission000/vocsimfull
  subset: null
  split: train
  subsets_to_run: &subsets_to_process [BS1, BS2, BS3, BS4, BS5, BC, HP, HS1, HS2, HU1, HU2, HU3, HU4, HW1, HW2, HW3, HW4, ES1, OC1]

whisperseg_subset_params: &ws_params
  all: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }  
  BS1: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS2: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS3: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS4: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS5: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BC: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  HP: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HS1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HS2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW3: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW4: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU3: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU4: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  OC1: { params: { spec_time_step: 0.01, min_frequency: 100.0, num_trials: 1 } }
  ES1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }

definitions:
  vae_trainer_def: &vae_trainer_def
    name: PaperVAETrainer
    module: reproducibility.trainers.vae
    params:
      z_dim: 32
      learning_rate: 0.001
      model_precision: 10.0
      vae_frontend_params: &vae_frontend_params
        target_sr: 16000
        n_fft: 512
        hop_length: 256
        win_length: 512
        window_fn_str: "hann"
        spec_height: 128
        spec_width: 128
        window_overlap: 0.5
  vae_model_def: &vae_model_def
    name: VariationalAutoencoder
    module: reproducibility.models.vae
    params: &vae_model_params
      z_dim: 32
      model_precision: 10.0
  vae_loop_params: &vae_loop_params
    num_epochs: 50
    batch_size: 64
    save_frequency_epochs: 10
    test_frequency: 10
  ae_trainer_def: &ae_trainer_def
    name: PaperAutoencoderTrainer
    module: reproducibility.trainers.autoencoder
    params:
      learning_rate: 0.0003
      regularization_weight: 0.01
  ae_model_def: &ae_model_def
    name: Autoencoder
    module: reproducibility.models.autoencoder
    params: &ae_model_params
      bottleneck_dim: 256
      audio_config: &ae_audio_config
        sr: 16000
        n_mels: 128
        nfft: 1024
        fmin: 0.0
      dimensions: &ae_dimensions
        nfft: 1024
        max_spec_width: 512
  ae_loop_params: &ae_loop_params
    num_epochs: 50
    batch_size: 128
    early_stopping_patience: 10
    mixed_precision: true
    save_frequency_epochs: 10
  vae_extractor_params: &vae_extractor_params
    <<: *vae_frontend_params
  ae_extractor_params: &ae_extractor_params
    audio_config: *ae_audio_config
    dimensions: *ae_dimensions
    bottleneck_dim: 256

train:
  - trainer: *vae_trainer_def
    model: *vae_model_def
    train_on_subset: "all"
    <<: *vae_loop_params
  - trainer: *ae_trainer_def
    model: *ae_model_def
    train_on_subset: "all"
    <<: *ae_loop_params

feature_extractors:
  - name: PaperVAEExtractor_all
    module: reproducibility.features.vae
    class: PaperVAEExtractor
    params:
      model_scope: "all"
      base_models_dir: *models_output_dir
      <<: *vae_extractor_params
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: VC
  - name: PaperAutoencoderExtractor_all
    module: reproducibility.features.autoencoder
    class: PaperAutoencoderExtractor
    params:
      model_scope: "all"
      base_models_dir: *models_output_dir
      <<: *ae_extractor_params
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: AC
  - name: MelExtractor
    module: features.mel
    params:
      sr: 16000
      n_mels: 128
      n_fft: 512
      hop_length: 256
      log_scale: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: M
  - name: WhisperEncoderExtractor
    module: features.whisper
    params:
      model_id: "openai/whisper-large-v3"
    compute_distances_for: ["cosine", "euclidean"]
    benchmark_this: true
    short_name: EW
  - name: WhisperSegExtractor
    module: features.whisperseg.extractor
    params:
      model_path: "d:/data2/vocsim/models/WhisperSegCompatible/whisper-large-v3-ct2"
      output_type: "embedding"
      default_spec_time_step: 0.01
      default_min_frequency: 50.0
      default_num_trials: 1
    compute_distances_for: ["cosine", "euclidean"]
    benchmark_this: true
    short_name: E
  - name: WavLMExtractor
    module: features.wavlm
    params:
      model_id: "microsoft/wavlm-large"
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: WLM
  - name: Wav2Vec2Extractor
    module: features.wav2vec2
    params:
      model_id: "facebook/wav2vec2-base-960h"
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: W2V
  - name: CLAPExtractor
    module: features.clap
    params:
      model_id: "laion/larger_clap_general"
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: CLP
  - name: AudioMAEExtractor
    module: features.audiomae
    params:
      model_id: "hance-ai/audiomae"
      trust_remote_code: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: MAE
  - name: EncodecExtractor
    module: features.encodec
    params:
      model_id: "facebook/encodec_24khz"
      bandwidth: 6.0
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: CC
  - name: WhisperEncoderExtractor_first_row_col
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWTF
  - name: WhisperEncoderExtractor_first_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 30
    pca_load_chunks: -1  
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWTF D30
  - name: WhisperEncoderExtractor_first_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWTF D100
  - name: WhisperEncoderExtractor_first_col
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWF
  - name: WhisperEncoderExtractor_first_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWF D30
  - name: WhisperEncoderExtractor_first_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWF D100
  - name: WhisperEncoderExtractor_first_row
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWT
  - name: WhisperEncoderExtractor_first_row_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWT D30
  - name: WhisperEncoderExtractor_first_row_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: first_row
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWT D100
  - name: WhisperEncoderExtractor_mean_row_col
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWMTF
  - name: WhisperEncoderExtractor_mean_row_col_pca_30
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWMTF D30
  - name: WhisperEncoderExtractor_mean_row_col_pca_100
    base_extractor: WhisperEncoderExtractor
    averaging: mean_row_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EWMTF D100
  # Derived: WhisperSeg (E based)
  - name: WhisperSegExtractor_first_row_col
    base_extractor: WhisperSegExtractor
    averaging: first_row_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ETF
  - name: WhisperSegExtractor_first_row_col_pca_30
    base_extractor: WhisperSegExtractor
    averaging: first_row_col
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ETF D30
  - name: WhisperSegExtractor_first_row_col_pca_100
    base_extractor: WhisperSegExtractor
    averaging: first_row_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ETF D100
  - name: WhisperSegExtractor_first_row
    base_extractor: WhisperSegExtractor
    averaging: first_row
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ET
  - name: WhisperSegExtractor_first_row_pca_30
    base_extractor: WhisperSegExtractor
    averaging: first_row
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ET D30
  - name: WhisperSegExtractor_first_row_pca_100
    base_extractor: WhisperSegExtractor
    averaging: first_row
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: ET D100
  - name: WhisperSegExtractor_first_col
    base_extractor: WhisperSegExtractor
    averaging: first_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EF
  - name: WhisperSegExtractor_first_col_pca_30
    base_extractor: WhisperSegExtractor
    averaging: first_col
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EF D30
  - name: WhisperSegExtractor_first_col_pca_100
    base_extractor: WhisperSegExtractor
    averaging: first_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EF D100
  - name: WhisperSegExtractor_mean_row_col
    base_extractor: WhisperSegExtractor
    averaging: mean_row_col
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EMTF
  - name: WhisperSegExtractor_mean_row_col_pca_30
    base_extractor: WhisperSegExtractor
    averaging: mean_row_col
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EMTF D30
  - name: WhisperSegExtractor_mean_row_col_pca_100
    base_extractor: WhisperSegExtractor
    averaging: mean_row_col
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: EMTF D100
  - name: WavLMExtractor_pca_30
    base_extractor: WavLMExtractor 
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: WLM D30
  - name: WavLMExtractor_pca_100
    base_extractor: WavLMExtractor 
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: WLM D100
  - name: Wav2Vec2Extractor_pca_30
    base_extractor: Wav2Vec2Extractor 
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: W2V D30
  - name: Wav2Vec2Extractor_pca_100
    base_extractor: Wav2Vec2Extractor 
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: W2V D100
  - name: CLAPExtractor_pca_30
    base_extractor: CLAPExtractor
    averaging: null
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: CLP D30
  - name: CLAPExtractor_pca_100
    base_extractor: CLAPExtractor
    averaging: null
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: CLP D100
  - name: AudioMAEExtractor_pca_30
    base_extractor: AudioMAEExtractor
    pca: 30
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: MAE D30
  - name: AudioMAEExtractor_pca_100
    base_extractor: AudioMAEExtractor
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: MAE D100

distances:
  - name: cosine
    params:
      use_torchmetrics: true
      zero_diagonal: true
  - name: euclidean
    params:
      use_torchmetrics: true
      zero_diagonal: true
  - name: spearman
    params:
      use_gpu_if_available: true

benchmarks:
  - name: PrecisionAtK
    params:
      k_values: [1, 5]
  - name: FValueBenchmark
    params:
      min_class_size: 5
  - name: GlobalSeparationRate
    module: benchmarks.gsr 
    params:
      min_class_size: 5
  - name: SilhouetteBenchmark
    module: benchmarks.silhouette
    params:
      sample_size: 15000 
      random_state: 42
  - name: CSCFBenchmark
    params:
      min_class_size: 5
  - name: ClusteringPurity
    params:
      use_distance_matrix_for_umap: true
      hdbscan_min_cluster_size: 5
  - name: ClassSeparationRatio
    module: benchmarks.csr
    params:
      min_class_size: 5

extraction_batch_size: 12 
distance_gpu_block_size: 1024 project_root: "."
results_dir: "d:/data2/vocsim/results/dac"
features_dir: "d:/data2/vocsim/features/dac"
models_dir: &models_output_dir "d:/data2/vocsim/paper_models_scoped"
output_tables_dir: "d:/data2/vocsim/paper_outputs/vocsim_tables_generated"
run_id: "vocsim_${now:%Y%m%d_%H%M%S}"
force_cpu: false
target_sample_rate: 16000
feature_chunk_size_gb: 3
logging:
  level: INFO

dataset:
  id: anonymous-submission000/vocsimfull
  subset: null
  split: train
  subsets_to_run: &subsets_to_process [BS1, BS2, BS3, BS4, BS5, BC, HP, HS1, HS2, HU1, HU2, HU3, HU4, HW1, HW2, HW3, HW4, ES1, OC1]

whisperseg_subset_params: &ws_params
  all: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  BS1: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS2: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS3: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS4: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BS5: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  BC: { params: { spec_time_step: 0.0025, min_frequency: 250.0, num_trials: 1 } }
  HP: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HS1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HS2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW3: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HW4: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU2: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU3: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  HU4: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
  OC1: { params: { spec_time_step: 0.01, min_frequency: 100.0, num_trials: 1 } }
  ES1: { params: { spec_time_step: 0.01, min_frequency: 50.0, num_trials: 1 } }
feature_extractors:
  - name: DACExtractor_z
    module: features.dac
    class: DACExtractor
    params:
      model_type: "16khz"
      output_type: "z"
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC-z-raw

  - name: DACExtractor_codes
    module: features.dac
    class: DACExtractor
    params:
      model_type: "16khz"
      output_type: "codes"
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC-c-raw

  - name: DACExtractor_latents
    module: features.dac
    class: DACExtractor
    params:
      model_type: "16khz"
      output_type: "latents"
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC-l-raw

  - name: DACExtractor_all
    module: features.dac
    class: DACExtractor
    params:
      model_type: "16khz"
      output_type: "all"
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC-all-raw

  - name: DAC_z_Avg
    base_extractor: DACExtractor_z
    averaging: mean_time_dim
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC (z)

  - name: DAC_codes_Avg
    base_extractor: DACExtractor_codes
    averaging: mean_time_dim
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC (codes)

  - name: DAC_latents_Avg
    base_extractor: DACExtractor_latents
    averaging: mean_time_dim
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC (latents)

  - name: DAC_all_Avg
    base_extractor: DACExtractor_all
    averaging: mean_time_dim
    benchmark_this: true
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    short_name: DAC (all)

  - name: DAC_z_Avg_pca_100
    base_extractor: DAC_z_Avg
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC (z, D=100)

  - name: DAC_codes_Avg_pca_100
    base_extractor: DAC_codes_Avg
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC (codes, D=100)

  - name: DAC_latents_Avg_pca_100
    base_extractor: DAC_latents_Avg
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC (latents, D=100)

  - name: DAC_all_Avg_pca_100
    base_extractor: DAC_all_Avg
    pca: 100
    pca_load_chunks: -1
    compute_distances_for: ["cosine", "euclidean", "spearman"]
    benchmark_this: true
    short_name: DAC (all, D=100)

distances:
  - name: cosine
    params:
      use_torchmetrics: true
      zero_diagonal: true
  - name: euclidean
    params:
      use_torchmetrics: true
      zero_diagonal: true
  - name: spearman
    params:
      use_gpu_if_available: true

benchmarks:
  # - name: PrecisionAtK
  #   params:
  #     k_values: [1, 5]
  - name: GlobalSeparationRate
    module: benchmarks.gsr 
    params:
      min_class_size: 5
  # - name: FValueBenchmark
  #   params:
  #     min_class_size: 5
  # - name: CSCFBenchmark
  #   params:
  #     min_class_size: 5
  # - name: ClusteringPurity
  #   params:
  #     use_distance_matrix_for_umap: true
  #     hdbscan_min_cluster_size: 5
  # - name: ClassSeparationRatio
  #   module: benchmarks.csr
  #   params:
  #     min_class_size: 5

extraction_batch_size: 12
distance_gpu_block_size: 1024import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any

import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
import os

from features.base import FeatureExtractor
from reproducibility.models.autoencoder import Autoencoder, AudioConfig

logger = logging.getLogger(__name__)


class PaperAutoencoderExtractor(FeatureExtractor):
    """
    Loads and uses the specific Autoencoder model trained for the paper (scoped).
    """

    def _initialize(
        self,
        model_scope: str,
        base_models_dir: Union[str, Path],
        audio_config: Dict[str, Any],
        dimensions: Dict[str, Any],
        bottleneck_dim: int,
        checkpoint_key: str = "model_state_dict",
        **kwargs,
    ):
        """
        Load the AE model trained for a specific scope (e.g., 'all' or subset key).

        Args:
            model_scope (str): The scope identifier for the model (e.g., dataset subset name).
            base_models_dir (Union[str, Path]): Base directory where trainer checkpoints are stored.
            audio_config (Dict[str, Any]): Dictionary containing audio configuration details (sr, n_mels, fmin, fmax).
            dimensions (Dict[str, Any]): Dictionary containing model dimension details (nfft, max_spec_width).
            bottleneck_dim (int): The dimension of the bottleneck layer in the autoencoder.
            checkpoint_key (str): The key in the checkpoint file containing the model state dictionary.
            **kwargs: Additional keyword arguments (ignored).
        """
        if not model_scope:
            raise ValueError("model_scope must be provided ('all' or subset key)")
        self.model_scope = model_scope
        self.base_models_dir = Path(base_models_dir)
        self.audio_config = audio_config
        self.dimensions = dimensions
        self.bottleneck_dim = bottleneck_dim

        logger.info("Initializing PaperAutoencoderExtractor for scope: '%s'", self.model_scope)
        trainer_base_name = "PaperAutoencoderTrainer"
        scoped_trainer_name = f"{trainer_base_name}_{self.model_scope}"
        checkpoint_dir = self.base_models_dir / scoped_trainer_name / "checkpoints"
        model_path = checkpoint_dir / "final_model.pt"

        if not model_path.exists():
            logger.warning("'final_model.pt' not found in %s. Looking for latest checkpoint...", checkpoint_dir)
            available_checkpoints = sorted(checkpoint_dir.glob("*.pt"), key=os.path.getmtime, reverse=True)
            if available_checkpoints:
                model_path = available_checkpoints[0]
                logger.info("Using latest checkpoint: %s", model_path.name)
            else:
                raise FileNotFoundError(f"No AE model checkpoint found for scope '{self.model_scope}' in {checkpoint_dir}")

        try:
            ae_audio_cfg = AudioConfig(
                sr=self.audio_config["sr"],
                n_mels=self.audio_config["n_mels"],
                nfft=self.dimensions["nfft"],
                fmin=self.audio_config.get("fmin", 0.0),
                fmax=self.audio_config.get("fmax", self.audio_config["sr"] // 2),
            )
            max_spec_width = self.dimensions["max_spec_width"]
        except KeyError as e:
            raise ValueError(f"Missing key in audio_config/dimensions for AE init: {e}") from e
        except Exception as e:
            raise ValueError(f"Error creating AudioConfig: {e}") from e

        try:
            self.model = Autoencoder(config=ae_audio_cfg, max_spec_width=max_spec_width, bottleneck_dim=self.bottleneck_dim)
        except Exception as e:
            logger.error("Failed instantiate Autoencoder: %s", e, exc_info=True)
            raise

        try:
            logger.info("Loading AE model checkpoint: %s", model_path)
            checkpoint = torch.load(model_path, map_location=self.device)
            state_dict = checkpoint.get(checkpoint_key)
            if state_dict is None:
                dummy_keys = self.model.state_dict().keys()
                if all(k in checkpoint for k in dummy_keys):
                    state_dict = checkpoint
                else:
                    raise KeyError(f"Key '{checkpoint_key}' not found. Keys: {list(checkpoint.keys())}")

            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            if missing:
                logger.warning("Missing keys loading AE state_dict: %s", missing)
            if unexpected:
                logger.warning("Unexpected keys loading AE state_dict: %s", unexpected)

            self.model.to(self.device)
            self.model.eval()
            logger.info("Paper AE model (Scope: %s) loaded successfully.", self.model_scope)
        except Exception as e:
            logger.error("Failed to load AE state_dict from %s: %s", model_path, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extract features using the loaded paper-specific Autoencoder's encoder.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform (numpy array or torch tensor).
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            torch.Tensor: The extracted pooled bottleneck features ([bottleneck_dim]) on CPU,
                          or an empty tensor if extraction fails.
        """
        try:
            if isinstance(audio_data, np.ndarray):
                audio_tensor = torch.from_numpy(audio_data.astype(np.float32))
            else:
                audio_tensor = audio_data.float()

            if audio_tensor.ndim > 1 and audio_tensor.shape[0] > 1:
                audio_tensor = torch.mean(audio_tensor, dim=0)
            if audio_tensor.ndim == 1:
                audio_tensor = audio_tensor.unsqueeze(0)

            audio_tensor = audio_tensor.to(self.device)

            if sample_rate != self.model.config.sr:
                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.model.config.sr).to(self.device)
                audio_tensor = resampler(audio_tensor)

            max_val = torch.max(torch.abs(audio_tensor))
            if max_val > 1e-6:
                audio_tensor = audio_tensor / max_val

            _, encoded_features = self.model(audio_tensor)

            pooled_features = torch.mean(encoded_features, dim=[2, 3])

            return pooled_features.squeeze(0).cpu()

        except Exception as e:
            logger.error("Paper AE feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0, device="cpu")import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn.functional as F
import torchaudio
from torch.distributions import LowRankMultivariateNormal

from features.base import FeatureExtractor
from reproducibility.models.vae import VariationalAutoencoder, preprocess_vae_input

logger = logging.getLogger(__name__)


class PaperVAEExtractor(FeatureExtractor):
    """
    Loads and uses the specific VAE model trained for the paper (scoped).
    """

    def _initialize(
        self,
        model_scope: str,
        base_models_dir: Union[str, Path],
        checkpoint_key: str = "model_state_dict",
        extract_mode: str = "mean",
        target_sr: int = 16000,
        n_fft: int = 512,
        hop_length: int = 256,
        win_length: int = 512,
        window_fn_str: str = "hann",
        spec_height: int = 128,
        spec_width: int = 128,
        window_overlap: float = 0.5,
        **kwargs,
    ):
        """
        Load the VAE model trained for a specific scope (e.g., 'all' or subset key).

        Args:
            model_scope (str): The scope identifier for the model (e.g., dataset subset name).
            base_models_dir (Union[str, Path]): Base directory where trainer checkpoints are stored.
            checkpoint_key (str): The key in the checkpoint file containing the model state dictionary.
            extract_mode (str): Determines whether to extract the 'mean' of the latent
                distribution or a 'sample'. Defaults to 'mean'.
            target_sr (int): Target sample rate for VAE frontend processing.
            n_fft (int): FFT size for VAE frontend spectrogram calculation.
            hop_length (int): Hop length for VAE frontend spectrogram calculation.
            win_length (int): Window length for VAE frontend spectrogram calculation.
            window_fn_str (str): Window function name for VAE frontend (e.g., 'hann').
            spec_height (int): Target Mel spectrogram height (number of Mel bins) for VAE.
            spec_width (int): Target spectrogram width (number of time frames) for VAE.
            window_overlap (float): Overlap between spectrogram windows for VAE frontend.
            **kwargs: Additional keyword arguments (ignored).
        """
        if not model_scope:
            raise ValueError("model_scope must be provided ('all' or subset key)")
        self.model_scope = model_scope
        self.base_models_dir = Path(base_models_dir)

        logger.info("Initializing PaperVAEExtractor for scope: '%s'", self.model_scope)

        self.extract_mode = extract_mode
        if self.extract_mode not in ["mean", "sample"]:
            raise ValueError("VAE extract_mode must be 'mean' or 'sample'")

        self.target_sr = target_sr
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = win_length if win_length else n_fft
        self.window_fn_str = window_fn_str
        self.spec_height = spec_height
        self.spec_width = spec_width
        self.window_overlap = window_overlap
        self.window_samples = (self.spec_width - 1) * self.hop_length
        self.hop_samples = int(self.window_samples * (1 - self.window_overlap))

        trainer_base_name = "PaperVAETrainer"
        scoped_trainer_name = f"{trainer_base_name}_{self.model_scope}"
        checkpoint_dir = self.base_models_dir / scoped_trainer_name / "checkpoints"
        model_path = checkpoint_dir / "final_model.pt"

        if not model_path.exists():
            logger.warning("'final_model.pt' not found in %s. Looking for latest checkpoint...", checkpoint_dir)
            available_checkpoints = sorted(checkpoint_dir.glob("*.pt"), key=os.path.getmtime, reverse=True)
            if available_checkpoints:
                model_path = available_checkpoints[0]
                logger.info("Using latest checkpoint: %s", model_path.name)
            else:
                raise FileNotFoundError(f"No VAE model checkpoint found for scope '{self.model_scope}' in {checkpoint_dir}")

        try:
            logger.info("Loading VAE model checkpoint: %s", model_path)
            checkpoint = torch.load(model_path, map_location="cpu")
            state_dict = checkpoint.get(checkpoint_key)
            if state_dict is None:
                dummy_model_state = VariationalAutoencoder(z_dim=32).state_dict().keys()
                if all(k in checkpoint for k in dummy_model_state):
                    state_dict = checkpoint
                else:
                    raise KeyError(f"Key '{checkpoint_key}' not found. Keys: {list(checkpoint.keys())}")

            z_dim = checkpoint.get("z_dim")
            if z_dim is None:
                z_dim_keys = ["fc41.weight", "fc5.bias"]
                for k in z_dim_keys:
                    if k in state_dict:
                        z_dim = state_dict[k].shape[0]
                        break
                if z_dim is None:
                    raise ValueError("Could not determine z_dim from VAE checkpoint.")
            logger.info(f"Inferred z_dim={z_dim} from checkpoint.")

            self.model = VariationalAutoencoder(z_dim=z_dim, device_name=self.device.type)
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            if missing:
                logger.warning("Missing keys loading VAE state_dict: %s", missing)
            if unexpected:
                logger.warning("Unexpected keys loading VAE state_dict: %s", unexpected)

            self.model.eval()
            logger.info("Paper VAE model (Scope: %s) loaded successfully.", self.model_scope)
        except Exception as e:
            logger.error("Failed to load/initialize VAE model from %s: %s", model_path, e, exc_info=True)
            raise

    @torch.no_grad()
    def extract(self, audio_data: Union[np.ndarray, torch.Tensor], sample_rate: int, **kwargs: Any) -> torch.Tensor:
        """
        Extracts features (latent representation) using the loaded VAE encoder.

        Audio is first converted to spectrogram chunks using the VAE's
        frontend parameters, then passed through the VAE encoder. Latent
        vectors are averaged over the chunks.

        Args:
            audio_data (Union[np.ndarray, torch.Tensor]): Input audio waveform.
            sample_rate (int): Sample rate of the audio data.
            **kwargs (Any): Additional keyword arguments (ignored).

        Returns:
            torch.Tensor: The extracted average latent vector ([z_dim]) on CPU,
                          or an empty tensor if extraction fails.
        """
        try:
            spec_chunks = preprocess_vae_input(
                audio_tensor=audio_data,
                sample_rate=sample_rate,
                target_sr=self.target_sr,
                n_fft=self.n_fft,
                hop_length=self.hop_length,
                win_length=self.win_length,
                window_fn_str=self.window_fn_str,
                spec_height=self.spec_height,
                spec_width=self.spec_width,
                window_samples=self.window_samples,
                hop_samples=self.hop_samples,
                device=self.device,
            )
            if not spec_chunks:
                return torch.empty(0, device="cpu")

            all_latents = []
            batch_size = 128
            for i in range(0, len(spec_chunks), batch_size):
                spec_batch = torch.cat([c.to(self.device) for c in spec_chunks[i : i + batch_size]], dim=0)
                mu, u, d_diag = self.model.encode(spec_batch)
                if self.extract_mode == "mean":
                    latent_batch = mu
                else:
                    latent_batch = LowRankMultivariateNormal(mu, u, d_diag).rsample()
                all_latents.append(latent_batch.cpu())

            if not all_latents:
                return torch.empty(0, device="cpu")
            all_latents_tensor = torch.cat(all_latents, dim=0)
            avg_latent = all_latents_tensor.mean(dim=0)
            return avg_latent

        except Exception as e:
            logger.error("Paper VAE feature extraction failed: %s", e, exc_info=True)
            return torch.empty(0, device="cpu")"""
VocSim Benchmark: Paper Reproducibility Feature Extractors.
"""

from .autoencoder import PaperAutoencoderExtractor
from .vae import PaperVAEExtractor

__all__ = [
    "PaperAutoencoderExtractor",
    "PaperVAEExtractor",
]import logging
from dataclasses import dataclass
from typing import Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import torchaudio.transforms as T
from utils.torch_utils import check_tensor

logger = logging.getLogger(__name__)


@dataclass
class AudioConfig:
    """Configuration for audio processing within the Autoencoder."""

    sr: int = 16000
    n_mels: int = 128
    nfft: int = 2048
    fmin: float = 100.0
    fmax: float = 8000.0
    sample_dur: Optional[float] = None


class STFT(nn.Module):
    """STFT front-end for the autoencoder"""

    def __init__(self, nfft: int, hop_length: int, max_spec_width: int):
        """
        Initializes the STFT frontend.

        Args:
            nfft (int): FFT size.
            hop_length (int): Hop length.
            max_spec_width (int): Target spectrogram width (number of time frames).
        """
        super().__init__()
        self.nfft = nfft
        self.hop_length = hop_length
        self.max_spec_width = ((max_spec_width + 31) // 32) * 32
        self.register_buffer("window", torch.hann_window(nfft))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs STFT on the input audio.

        Args:
            x (torch.Tensor): Input audio tensor of shape [B, Time].

        Returns:
            torch.Tensor: Magnitude spectrogram tensor of shape [B, 1, Freq, Time].
        """
        target_length = (self.max_spec_width - 1) * self.hop_length

        if x.size(1) < target_length:
            padding = torch.zeros(x.size(0), target_length - x.size(1), device=x.device)
            x = torch.cat([x, padding], dim=1)
        elif x.size(1) > target_length:
            x = x[:, :target_length]

        z = torch.stft(
            x,
            n_fft=self.nfft,
            hop_length=self.hop_length,
            win_length=self.nfft,
            window=self.window,
            center=True,
            pad_mode="reflect",
            normalized=False,
            onesided=True,
            return_complex=True,
        )

        mag = torch.abs(z)

        if mag.size(-1) < self.max_spec_width:
            pad_size = self.max_spec_width - mag.size(-1)
            mag = torch.nn.functional.pad(mag, (0, pad_size))
        elif mag.size(-1) > self.max_spec_width:
            mag = mag[..., : self.max_spec_width]

        return mag.unsqueeze(1)


class MelFilter(nn.Module):
    """Mel filterbank for the autoencoder"""

    def __init__(self, sr: int, nfft: int, n_mels: int, fmin: float, fmax: float):
        """
        Initializes the Mel filterbank.

        Args:
            sr (int): Sample rate.
            nfft (int): FFT size.
            n_mels (int): Number of Mel bins.
            fmin (float): Minimum frequency.
            fmax (float): Maximum frequency.
        """
        super().__init__()
        self.n_mels = n_mels
        self.mel_scale = T.MelScale(
            n_mels=n_mels,
            sample_rate=sr,
            f_min=fmin,
            f_max=fmax,
            n_stft=nfft // 2 + 1,
            norm="slaney",
            mel_scale="slaney",
        )

    def forward(self, spec: torch.Tensor) -> torch.Tensor:
        """
        Applies the Mel filterbank and log scaling to the magnitude spectrogram.

        Args:
            spec (torch.Tensor): Magnitude spectrogram tensor of shape [B, 1, Freq, Time].

        Returns:
            torch.Tensor: Log-Mel spectrogram tensor of shape [B, 1, Mels, Time].
        """
        spec = spec.squeeze(1)

        power_spec = spec**2
        mel_power_spec = self.mel_scale(power_spec)

        log_mel_spec = torch.log1p(torch.clamp(mel_power_spec, min=1e-9))

        if not check_tensor(log_mel_spec, "LogMelSpec in MelFilter"):
            logger.error("NaN/Inf detected after MelScale+log1p in MelFilter! Returning zeros.")
            return torch.zeros_like(log_mel_spec).unsqueeze(1)

        return log_mel_spec.unsqueeze(1)


class Autoencoder(nn.Module):
    """Autoencoder model for audio features (Paper Specific Structure)."""

    def __init__(
        self,
        config: AudioConfig,
        max_spec_width: int,
        bottleneck_dim: int = 256,
    ):
        """
        Initializes the Autoencoder model.

        Args:
            config (AudioConfig): Audio processing configuration.
            max_spec_width (int): Target spectrogram width.
            bottleneck_dim (int): Dimension of the bottleneck layer.
        """
        super().__init__()
        self.config = config
        hop_length = max(config.nfft // 4, 1)
        self.target_width = ((max_spec_width + 31) // 32) * 32

        self.frontend = nn.Sequential(
            STFT(config.nfft, hop_length, max_spec_width),
            MelFilter(config.sr, config.nfft, config.n_mels, config.fmin, config.fmax),
            nn.InstanceNorm2d(1, eps=1e-4),
        )

        self.encoder = nn.Sequential(
            self._make_conv_block(1, 32),
            self._make_conv_block(32, 64),
            self._make_conv_block(64, 128),
            self._make_conv_block(128, 256),
            nn.Conv2d(256, bottleneck_dim, 3, stride=2, padding=1),
        )

        self.decoder = nn.Sequential(
            self._make_deconv_block(bottleneck_dim, 256),
            self._make_deconv_block(256, 128),
            self._make_deconv_block(128, 64),
            self._make_deconv_block(64, 32),
            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),
        )

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through the autoencoder.

        Args:
            x (torch.Tensor): Input audio tensor of shape [B, Time].

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: A tuple containing the decoded spectrogram
                                               ([B, 1, Mels, Time]) and the encoded features
                                               ([B, bottleneck_dim, H, W]).
        """
        mel_spec = self.frontend(x)
        if not check_tensor(mel_spec, "Frontend Output"):
            logger.error("NaN/Inf detected in frontend output! Returning zeros.")
            est_h = mel_spec.shape[2] // 32
            est_w = mel_spec.shape[3] // 32
            bottleneck_channels = self.encoder[-1].out_channels
            dummy_encoded_shape = (
                x.shape[0],
                bottleneck_channels,
                est_h,
                est_w,
            )
            return torch.zeros_like(mel_spec), torch.zeros(dummy_encoded_shape, device=mel_spec.device)

        encoded = self.encoder(mel_spec)
        if not check_tensor(encoded, "Encoder Output"):
            logger.error("NaN/Inf detected in encoder output! Returning zeros.")
            return torch.zeros_like(mel_spec), torch.zeros_like(encoded)

        decoded = self.decoder(encoded)
        if not check_tensor(decoded, "Decoder Output"):
            logger.error("NaN/Inf detected in decoder output! Returning zeros.")
            return torch.zeros_like(decoded), encoded

        if decoded.shape != mel_spec.shape:
            decoded = torch.nn.functional.interpolate(
                decoded,
                size=mel_spec.shape[2:],
                mode="bilinear",
                align_corners=False,
            )

        return decoded, encoded

    @staticmethod
    def _make_conv_block(in_channels: int, out_channels: int) -> nn.Sequential:
        """Creates a convolutional block for the encoder."""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True),
        )

    @staticmethod
    def _make_deconv_block(in_channels: int, out_channels: int) -> nn.Sequential:
        """Creates a deconvolutional block for the decoder."""
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True),
        )import logging
from typing import List, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from torch.distributions import LowRankMultivariateNormal

logger = logging.getLogger(__name__)

X_SHAPE = (128, 128)
X_DIM = np.prod(X_SHAPE)


def preprocess_vae_input(
    audio_tensor: Union[np.ndarray, torch.Tensor],
    sample_rate: int,
    target_sr: int,
    n_fft: int,
    hop_length: int,
    win_length: int,
    window_fn_str: str,
    spec_height: int,
    spec_width: int,
    window_samples: int,
    hop_samples: int,
    device: torch.device,
) -> List[torch.Tensor]:
    """
    Takes raw audio, resamples, chunks, computes spectrograms, and resizes.

    Args:
        audio_tensor (Union[np.ndarray, torch.Tensor]): Raw audio waveform.
        sample_rate (int): Original sample rate of the audio.
        target_sr (int): Target sample rate for resampling.
        n_fft (int): FFT size for spectrogram calculation.
        hop_length (int): Hop length for spectrogram calculation.
        win_length (int): Window length for spectrogram calculation.
        window_fn_str (str): Name of the window function ('hann', 'hamming').
        spec_height (int): Target spectrogram height (number of Mel bins).
        spec_width (int): Target spectrogram width (number of time frames).
        window_samples (int): Number of audio samples corresponding to the desired spectrogram window width.
        hop_samples (int): Number of audio samples corresponding to the desired hop between spectrogram windows.
        device (torch.device): Device to perform computation on.

    Returns:
        List[torch.Tensor]: A list of preprocessed spectrogram chunks for the VAE.
                            Shape of each chunk: [1, H, W]
    """
    if not isinstance(audio_tensor, torch.Tensor):
        if not isinstance(audio_tensor, np.ndarray):
            raise TypeError(f"Expected numpy array or torch tensor, got {type(audio_tensor)}")
        if audio_tensor.dtype != np.float32:
            audio_tensor = audio_tensor.astype(np.float32)
        audio_tensor = torch.from_numpy(audio_tensor)

    if audio_tensor.ndim > 1 and audio_tensor.shape[0] > 1:
        audio_tensor = torch.mean(audio_tensor, dim=0)
    if audio_tensor.ndim == 2 and audio_tensor.shape[0] == 1:
        audio_tensor = audio_tensor.squeeze(0)

    if sample_rate != target_sr:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sr).to(device)
        audio_tensor = resampler(audio_tensor.to(device))
    else:
        audio_tensor = audio_tensor.to(device)

    if len(audio_tensor) < window_samples:
        padding = window_samples - len(audio_tensor)
        audio_tensor = F.pad(audio_tensor, (0, padding))
        num_chunks = 1
        starts = [0]
    else:
        starts = list(range(0, len(audio_tensor) - window_samples + 1, hop_samples))
        if not starts or (starts[-1] + window_samples < len(audio_tensor)):
            starts.append(len(audio_tensor) - window_samples)
        num_chunks = len(starts)

    if num_chunks == 0:
        return []

    window_fn_map = {"hann": torch.hann_window, "hamming": torch.hamming_window}
    window_fn = window_fn_map.get(window_fn_str, torch.hann_window)
    spec_transform = torchaudio.transforms.Spectrogram(
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window_fn=window_fn,
        power=None,
        center=True,
        pad_mode="reflect",
    ).to(device)

    processed_chunks = []
    for start_idx in starts:
        end_idx = start_idx + window_samples
        audio_chunk = audio_tensor[start_idx:end_idx]

        if audio_chunk.numel() == 0:
            logger.warning("Encountered empty audio chunk during VAE preprocessing, skipping.")
            continue

        spec_complex = spec_transform(audio_chunk)
        spec = spec_complex.abs()
        spec = torch.log1p(spec)

        spec = spec.unsqueeze(0).unsqueeze(0)
        spec_resized = F.interpolate(spec, size=(spec_height, spec_width), mode="bilinear", align_corners=False)
        processed_chunks.append(spec_resized.squeeze(1))

    return processed_chunks


class VariationalAutoencoder(nn.Module):
    """Variational Autoencoder class for single-channel spectrograms (Paper Specific)."""

    def __init__(
        self,
        z_dim: int = 32,
        model_precision: float = 10.0,
        device_name: str = "cpu",
        lr: float = 1e-3,
    ):
        """
        Initializes the Variational Autoencoder model.

        Args:
            z_dim (int): The dimension of the latent space.
            model_precision (float): Precision parameter for the reconstruction likelihood term.
            device_name (str): Name of the device to move the model to.
            lr (float): Learning rate (stored for context, not used in forward pass).
        """
        super().__init__()
        self.z_dim = z_dim
        self.model_precision = model_precision
        self.device = torch.device(device_name)
        self.lr = lr

        self.conv1 = nn.Conv2d(1, 8, 3, 1, padding=1)
        self.conv2 = nn.Conv2d(8, 8, 3, 2, padding=1)
        self.conv3 = nn.Conv2d(8, 16, 3, 1, padding=1)
        self.conv4 = nn.Conv2d(16, 16, 3, 2, padding=1)
        self.conv5 = nn.Conv2d(16, 24, 3, 1, padding=1)
        self.conv6 = nn.Conv2d(24, 24, 3, 2, padding=1)
        self.conv7 = nn.Conv2d(24, 32, 3, 1, padding=1)
        self.bn1 = nn.BatchNorm2d(1)
        self.bn2 = nn.BatchNorm2d(8)
        self.bn3 = nn.BatchNorm2d(8)
        self.bn4 = nn.BatchNorm2d(16)
        self.bn5 = nn.BatchNorm2d(16)
        self.bn6 = nn.BatchNorm2d(24)
        self.bn7 = nn.BatchNorm2d(24)
        self.fc1 = nn.Linear(32 * 16 * 16, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc31 = nn.Linear(256, 64)
        self.fc32 = nn.Linear(256, 64)
        self.fc33 = nn.Linear(256, 64)
        self.fc41 = nn.Linear(64, self.z_dim)
        self.fc42 = nn.Linear(64, self.z_dim)
        self.fc43 = nn.Linear(64, self.z_dim)

        self.fc5 = nn.Linear(self.z_dim, 64)
        self.fc6 = nn.Linear(64, 256)
        self.fc7 = nn.Linear(256, 1024)
        self.fc8 = nn.Linear(1024, 8192)
        self.convt1 = nn.ConvTranspose2d(32, 24, 3, 1, padding=1)
        self.convt2 = nn.ConvTranspose2d(24, 24, 3, 2, padding=1, output_padding=1)
        self.convt3 = nn.ConvTranspose2d(24, 16, 3, 1, padding=1)
        self.convt4 = nn.ConvTranspose2d(16, 16, 3, 2, padding=1, output_padding=1)
        self.convt5 = nn.ConvTranspose2d(16, 8, 3, 1, padding=1)
        self.convt6 = nn.ConvTranspose2d(8, 8, 3, 2, padding=1, output_padding=1)
        self.convt7 = nn.ConvTranspose2d(8, 1, 3, 1, padding=1)
        self.bn8 = nn.BatchNorm2d(32)
        self.bn9 = nn.BatchNorm2d(24)
        self.bn10 = nn.BatchNorm2d(24)
        self.bn11 = nn.BatchNorm2d(16)
        self.bn12 = nn.BatchNorm2d(16)
        self.bn13 = nn.BatchNorm2d(8)
        self.bn14 = nn.BatchNorm2d(8)

        self.to(self.device)

    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Encodes the input spectrogram into latent distribution parameters (mu, u, d_diag).

        Args:
            x (torch.Tensor): Input spectrogram tensor of shape [B, H, W] or [B, 1, H, W].

        Returns:
            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple containing:
                - mu (torch.Tensor): Mean vector of the latent distribution [B, z_dim].
                - u (torch.Tensor): Factor `u` for the covariance matrix [B, z_dim, 1].
                - d_diag (torch.Tensor): Diagonal factor `d` for the covariance matrix [B, z_dim].
        """
        if x.dim() == 3:
            x = x.unsqueeze(1)
        elif x.dim() != 4 or x.shape[1] != 1:
            raise ValueError(f"Input tensor must be [B, H, W] or [B, 1, H, W], got {x.shape}")

        x = F.relu(self.conv1(self.bn1(x)))
        x = F.relu(self.conv2(self.bn2(x)))
        x = F.relu(self.conv3(self.bn3(x)))
        x = F.relu(self.conv4(self.bn4(x)))
        x = F.relu(self.conv5(self.bn5(x)))
        x = F.relu(self.conv6(self.bn6(x)))
        x = F.relu(self.conv7(self.bn7(x)))

        x = x.view(-1, 32 * 16 * 16)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))

        mu = self.fc41(F.relu(self.fc31(x)))
        u = self.fc42(F.relu(self.fc32(x))).unsqueeze(-1)
        d_diag = torch.exp(self.fc43(F.relu(self.fc33(x))))

        return mu, u, d_diag

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """
        Decodes a latent vector z back into a flattened spectrogram representation.

        Args:
            z (torch.Tensor): Latent vector tensor of shape [B, z_dim].

        Returns:
            torch.Tensor: Flattened reconstructed spectrogram tensor of shape [B, H*W].
        """
        z = F.relu(self.fc5(z))
        z = F.relu(self.fc6(z))
        z = F.relu(self.fc7(z))
        z = F.relu(self.fc8(z))
        z = z.view(-1, 32, 16, 16)
        z = F.relu(self.convt1(self.bn8(z)))
        z = F.relu(self.convt2(self.bn9(z)))
        z = F.relu(self.convt3(self.bn10(z)))
        z = F.relu(self.convt4(self.bn11(z)))
        z = F.relu(self.convt5(self.bn12(z)))
        z = F.relu(self.convt6(self.bn13(z)))
        z = self.convt7(self.bn14(z))

        return z.view(-1, X_DIM)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Performs the forward pass and calculates the VAE loss (negative ELBO).

        Args:
            x (torch.Tensor): Input spectrogram tensor of shape [B, 1, H, W] or [B, H, W].

        Returns:
            torch.Tensor: The negative ELBO loss for the batch.
        """
        mu, u, d_diag = self.encode(x)
        latent_dist = LowRankMultivariateNormal(mu, u, d_diag)
        z = latent_dist.rsample()
        x_rec_flat = self.decode(z)
        x_flat = x.view(x.shape[0], -1)

        log_pxz = -0.5 * X_DIM * torch.log(
            torch.tensor(2.0 * np.pi / self.model_precision, device=self.device)
        ) - 0.5 * self.model_precision * torch.sum(torch.pow(x_flat - x_rec_flat, 2), dim=1)
        log_prior_term = -0.5 * torch.sum(torch.pow(z, 2), dim=1) - 0.5 * self.z_dim * np.log(2 * np.pi)
        entropy_term = latent_dist.entropy()
        elbo_per_sample = log_pxz + log_prior_term + entropy_term
        return -torch.mean(elbo_per_sample)"""Models specific to the paper reproducibility."""

from .autoencoder import Autoencoder, AudioConfig, STFT, MelFilter
from .vae import VariationalAutoencoder

__all__ = [
    "Autoencoder",
    "AudioConfig",
    "STFT",
    "MelFilter",
    "VariationalAutoencoder",
]import logging
from pathlib import Path
import sys
import argparse
import os
from typing import List


try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[2]
    expected_vocsim_dir = project_root / "vocsim"
    if not expected_vocsim_dir.is_dir() and not (project_root / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARN: Assuming CWD project root: {project_root}")
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root: {project_root}")
except NameError:
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

from vocsim.runner import PipelineRunner
from utils.config_loader import load_config
from utils.logging_utils import setup_logging

CONFIG_NAME = "avian_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"


def main(steps_to_run: List[str]):
    """
    Main function to run the Avian Perception pipeline stages.

    Args:
        steps_to_run (List[str]): A list of pipeline stage names to execute.
    """
    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        print(f"ERROR: Config not found: {config_path}")
        sys.exit(1)
    cfg = load_config(config_path, base_config_path=base_config_path if base_config_path.exists() else None)
    log_config = cfg.get("logging", {})
    log_config.setdefault("log_dir", project_root / "logs")
    log_config.setdefault("log_file", "avian_perception_run.log")
    setup_logging(log_config)
    logger = logging.getLogger(__name__)
    logger.info(f"Loaded config from {config_path}" + (f" and merged with {base_config_path}" if base_config_path.exists() else ""))
    logger.info(f"Executing pipeline steps: {steps_to_run}")
    if Path.cwd() != project_root:
        logger.warning("Running from '%s', changing CWD to project root '%s'.", Path.cwd(), project_root)
        os.chdir(project_root)
    try:
        runner = PipelineRunner(cfg)
        runner.run(steps=steps_to_run)
        logger.info("Avian Perception script finished successfully.")
    except Exception as e:
        logger.error("Error in Avian Perception run: %s", e, exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Avian Perception Pipeline Stages.")
    parser.add_argument("--steps", nargs="+", default=["features", "distances", "benchmarks"], choices=["train", "features", "distances", "benchmarks", "all"], help="Pipeline stages to run. 'all' runs train -> features -> distances -> benchmarks.")
    args = parser.parse_args()
    if "all" in args.steps:
        selected_steps = ["train", "features", "distances", "benchmarks"]
    else:
        ordered_steps = []
        step_order = ["train", "features", "distances", "benchmarks"]
        for step in step_order:
            if step in args.steps:
                ordered_steps.append(step)
        selected_steps = ordered_steps
    main(selected_steps)# In reproducibility/scripts/misc/calculate_efficiency.py

import torch
import torch.nn as nn
import numpy as np
import sys
from pathlib import Path
import logging
import warnings

# --- Boilerplate to add project root to path ---
try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[3] # Assumes misc/scripts/reproducibility/
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    print(f"INFO: Added project root to path: {project_root}")
except NameError:
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD for interactive session: {project_root}")

warnings.filterwarnings("ignore", category=UserWarning)
from thop import profile, clever_format

# --- Import your project's feature extractors and models ---
from features.base import FeatureExtractor
from features.whisper import WhisperEncoderExtractor
from features.wavlm import WavLMExtractor
from features.clap import CLAPExtractor
from features.encodec import EncodecExtractor
from features.audiomae import AudioMAEExtractor
from features.wav2vec2 import Wav2Vec2Extractor
from reproducibility.features.vae import PaperVAEExtractor
from reproducibility.features.autoencoder import PaperAutoencoderExtractor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
TABLE_HEADER = f"{'Model Pipeline':<30} | {'Parameters (M)':<16} | {'MACs (G)':<10} | {'Peak Memory (GB)':<18}"
TABLE_SEPARATOR = "-" * (len(TABLE_HEADER) + 2)

class FeatureExtractorProfiler(nn.Module):
    """A wrapper to make a FeatureExtractor's `extract` method profilable by thop."""
    def __init__(self, extractor: FeatureExtractor):
        super().__init__()
        # Important: The extractor's internal model must be a property of this wrapper
        # for thop to find its parameters. We'll search for it.
        self.extractor = extractor
        if hasattr(self.extractor, 'model'):
             self.model = self.extractor.model
        if hasattr(self.extractor, 'feature_extractor'):
             # For models like WavLM
             self.feature_extractor = self.extractor.feature_extractor

    def forward(self, audio_data, sample_rate):
        # The forward pass simply calls the extract method
        return self.extractor.extract(audio_data, sample_rate)

def profile_pipeline(model_name: str, extractor: FeatureExtractor, input_args: tuple):
    """
    Profiles a full feature extraction pipeline.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Wrap the extractor to make it profilable
    profiler_wrapper = FeatureExtractorProfiler(extractor).to(device).eval()
    
    # --- Profile MACs and Parameters with thop ---
    try:
        # We pass the raw audio waveform, thop traces PyTorch ops inside .extract()
        macs, params = profile(profiler_wrapper, inputs=input_args, verbose=False)
        params_m = params / 1e6
        macs_g = macs / 1e9
    except Exception:
        # Fallback for complex models (like AudioMAE) where thop might fail
        # but we can still count parameters.
        logging.warning(f"Could not profile MACs for {model_name} with thop. Reporting params only.")
        params = sum(p.numel() for p in profiler_wrapper.parameters())
        params_m = params / 1e6
        macs_g = -1.0

    # --- Measure Peak Memory Usage ---
    peak_mem_gb = -1.0
    if device.type == 'cuda':
        torch.cuda.reset_peak_memory_stats(device)
        try:
            # Run the forward pass to measure activation memory
            _ = profiler_wrapper(*input_args)
            peak_mem = torch.cuda.max_memory_allocated(device)
            peak_mem_gb = peak_mem / 1e9
        except Exception as e:
            logging.error(f"Failed forward pass for memory profiling on {model_name}: {e}")

    # --- Print results ---
    print(f"| {model_name:<28} | {f'{params_m:.2f}':<16} | {f'{macs_g:.2f}' if macs_g >= 0 else 'N/A':<10} | {f'{peak_mem_gb:.3f}' if peak_mem_gb >= 0 else 'N/A':<18} |")


def main():
    """
    Main function to orchestrate the profiling of all model pipelines.
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logging.info(f"Profiling full feature extraction pipelines on device: {device}")

    # --- Define a standard input audio clip ---
    # 30-second, 16kHz mono audio. This will be the input to the entire pipeline.
    sample_rate = 16000
    duration_sec = 30
    input_waveform = torch.randn(sample_rate * duration_sec).to(device)
    input_args = (input_waveform, sample_rate) # Arguments for our profiler wrapper

    print("\n" + TABLE_SEPARATOR)
    print(f"| {'Model Pipeline':<28} | {'Parameters (M)':<16} | {'MACs (G)':<10} | {'Peak Memory (GB)':<18} |")
    print(TABLE_SEPARATOR)

    # === Pretrained Models from Hugging Face ===

    # 1. Whisper Encoder Pipeline
    try:
        extractor = WhisperEncoderExtractor(model_id="openai/whisper-large-v3", device=device.type)
        profile_pipeline("Whisper-L-v3", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile Whisper pipeline: {e}")

    # 2. WavLM Pipeline
    try:
        extractor = WavLMExtractor(model_id="microsoft/wavlm-large", device=device.type)
        profile_pipeline("WavLM-Large", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile WavLM pipeline: {e}")

    # 3. Wav2Vec2 Pipeline
    try:
        extractor = Wav2Vec2Extractor(model_id="facebook/wav2vec2-base-960h", device=device.type)
        profile_pipeline("Wav2Vec2-Base", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile Wav2Vec2 pipeline: {e}")

    # 4. CLAP Pipeline
    try:
        extractor = CLAPExtractor(model_id="laion/larger_clap_general", device=device.type)
        profile_pipeline("CLAP", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile CLAP pipeline: {e}")

    # 5. AudioMAE Pipeline
    # NOTE: AudioMAE's `extract` writes to a file, making it untraceable by thop.
    # We will get an N/A for MACs, but this is the correct behavior.
    try:
        extractor = AudioMAEExtractor(model_id="hance-ai/audiomae", trust_remote_code=True, device=device.type)
        profile_pipeline("AudioMAE", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile AudioMAE pipeline: {e}")

    # 6. Encodec Pipeline
    try:
        extractor = EncodecExtractor(model_id="facebook/encodec_24khz", device=device.type)
        profile_pipeline("Encodec", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile Encodec pipeline: {e}")

    print(TABLE_SEPARATOR)
    print(f"| {'Special Cases / Custom':<28} | {'':<16} | {'':<10} | {'':<18} |")
    print(TABLE_SEPARATOR)

    # 8. Paper VAE Pipeline
    try:
        # Note: You must have a trained model checkpoint for this to work.
        # Update path to your actual model directory.
        models_dir = project_root / "d:/data2/vocsim/paper_models_scoped" 
        extractor = PaperVAEExtractor(model_scope="all", base_models_dir=models_dir, device=device.type)
        profile_pipeline("Paper VAE", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile Paper VAE pipeline: {e}. Check model path and checkpoints.")

    # 9. Paper Autoencoder Pipeline
    try:
        # Update path to your actual model directory.
        models_dir = project_root / "d:/data2/vocsim/paper_models_scoped"
        ae_audio_config = {'sr': 16000, 'n_mels': 128, 'nfft': 1024, 'fmin': 0.0}
        ae_dimensions = {'nfft': 1024, 'max_spec_width': 512}
        extractor = PaperAutoencoderExtractor(model_scope="all", base_models_dir=models_dir, 
                                              audio_config=ae_audio_config, dimensions=ae_dimensions,
                                              bottleneck_dim=256, device=device.type)
        profile_pipeline("Paper Autoencoder", extractor, input_args)
        del extractor
    except Exception as e:
        logging.error(f"Failed to profile Paper Autoencoder pipeline: {e}. Check model path and checkpoints.")

    # 10. Mel Spectrogram (Baseline)
    print(f"| {'Mel Spectrogram':<28} | {'0.00':<16} | {'0.00':<10} | {'Minimal':<18} |")
    print(f"| {'  (Note: Parameter-free transformation, not a neural model.)':<90} |")

    print(TABLE_SEPARATOR)
    if device.type == 'cuda':
        torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

import datetime
import gc
import logging
import sys
import os
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import torch

try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[3]
    expected_vocsim_dir = project_root / "vocsim"
    if not expected_vocsim_dir.is_dir() and not (project_root / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARNING: Assuming CWD project root: {project_root}")
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root: {project_root}")
except NameError:
    project_root = Path.cwd()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root: {project_root}")

from vocsim.managers.dataset_manager import DatasetManager
from vocsim.managers.feature_manager import FeatureManager
from utils.config_loader import load_config
from utils.feature_utils import apply_averaging
from utils.logging_utils import setup_logging
from utils.torch_utils import get_device

TARGET_SUBSET_KEY = "BS1"
CONFIG_NAME = "vocsim_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"
OUTPUT_DIR_BASE = project_root / "reproducibility_outputs" / "sample_features"

log_dir = project_root / "logs"
log_dir.mkdir(parents=True, exist_ok=True)
setup_logging(
    {
        "level": "INFO",
        "log_dir": str(log_dir),
        "log_file": "extract_FIRST_sample.log",
    }
)
logger = logging.getLogger(__name__)


def save_npy_feature(feature_data: Optional[np.ndarray], filename: str, output_dir: Path):
    """
    Saves a numpy array to a .npy file, handles None or empty input.

    Args:
        feature_data (Optional[np.ndarray]): The numpy array containing the feature data.
        filename (str): The base filename (without extension).
        output_dir (Path): The directory where the file should be saved.

    Returns:
        bool: True if the save was successful, False otherwise.
    """
    output_path = output_dir / f"{filename}.npy"
    if feature_data is None:
        logger.warning(f"Feature data is None for: {filename}. Skipping save.")
        return False
    if feature_data.size == 0:
        logger.warning(f"Feature data is empty for: {filename}. Skipping save.")
        return False

    try:
        output_dir.mkdir(parents=True, exist_ok=True)
        np.save(output_path, feature_data)
        logger.info(f"Saved feature: {output_path.name} (Shape: {feature_data.shape}, Dtype: {feature_data.dtype})")
        return True
    except Exception as e:
        logger.error(f"Failed to save feature '{filename}.npy': {e}", exc_info=True)
        return False


def main():
    """
    Main function to extract features from the first valid audio sample
    in a target dataset subset based on configuration.
    """
    logger.info("--- Starting Single Sample Feature Extraction Script (First Item) ---")

    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        logger.error("Config not found: %s", config_path)
        sys.exit(1)
    cfg = load_config(
        config_path,
        base_config_path=base_config_path if base_config_path.exists() else None,
    )
    logger.info("Loaded config from %s", config_path.name)

    device = get_device(cfg.get("force_cpu", False))

    dataset_manager = DatasetManager(cfg)
    if not dataset_manager.load_full_dataset():
        logger.error("Failed to load dataset.")
        sys.exit(1)

    selected_item_data = None
    selected_item_index = -1
    logger.info("Searching for the FIRST valid audio item in subset '%s'...", TARGET_SUBSET_KEY)
    try:
        if not hasattr(dataset_manager.full_dataset_obj, "__iter__"):
            raise TypeError("Dataset object is not iterable.")

        for idx, item in enumerate(dataset_manager.full_dataset_obj):
            if item is None:
                continue
            item_subset = item.get("subset")
            if item_subset != TARGET_SUBSET_KEY:
                continue

            audio_info = item.get("audio")
            if isinstance(audio_info, dict) and audio_info.get("array") is not None and "sampling_rate" in audio_info:
                audio_array_check = audio_info["array"]
                is_valid_audio = False
                if isinstance(audio_array_check, np.ndarray) and audio_array_check.size > 0:
                    is_valid_audio = True
                elif isinstance(audio_array_check, list) and len(audio_array_check) > 0:
                    is_valid_audio = True
                elif isinstance(audio_array_check, torch.Tensor) and audio_array_check.numel() > 0:
                    is_valid_audio = True

                if is_valid_audio:
                    selected_item_data = item
                    selected_item_index = idx
                    logger.info("Found first valid item at index %d.", selected_item_index)
                    break

        if selected_item_data is None:
            logger.error("Could not find any valid audio item in subset '%s'. Cannot proceed.", TARGET_SUBSET_KEY)
            sys.exit(1)

    except Exception as e:
        logger.error("Error while searching for the first item: %s", e, exc_info=True)
        sys.exit(1)

    try:
        item_identifier = f"item_idx_{selected_item_index}"
        for id_field in ["original_name", "id", "filename"]:
            if id_field in selected_item_data and selected_item_data[id_field]:
                safe_name = Path(str(selected_item_data[id_field])).stem
                safe_name = "".join(c if c.isalnum() or c in ("-", "_") else "_" for c in safe_name)
                item_identifier = f"{id_field}_{safe_name}"
                break

        audio_array = selected_item_data["audio"]["array"]
        sample_rate = selected_item_data["audio"]["sampling_rate"]

        logger.info("Processing selected item from subset '%s':", TARGET_SUBSET_KEY)
        logger.info("  Identifier: %s", item_identifier)
        logger.info("  Original Index: %d", selected_item_index)
        logger.info("  Audio Array Shape: %s", audio_array.shape if isinstance(audio_array, np.ndarray) else type(audio_array))
        logger.info("  Sample Rate: %d", sample_rate)
    except Exception as e:
        logger.error("Failed to process selected item data: %s", e, exc_info=True)
        sys.exit(1)

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_subdir = OUTPUT_DIR_BASE / f"{TARGET_SUBSET_KEY}_{item_identifier}_{timestamp}"
    output_subdir.mkdir(parents=True, exist_ok=True)
    logger.info("Saving extracted features to: %s", output_subdir)

    feature_manager = FeatureManager(cfg, Path("./dummy_feature_cache"), device)

    feature_configs = cfg.get("feature_extractors", [])
    logger.info("Found %d feature configurations to process.", len(feature_configs))

    extracted_base_features: Dict[str, Optional[np.ndarray]] = {}

    logger.info("\n--- PASS 1: Extracting and Saving BASE Features ---")
    for feature_config in feature_configs:
        feature_name = feature_config.get("name")
        if not feature_name:
            continue

        is_base = not feature_config.get("base_extractor")

        if is_base:
            logger.info("Processing BASE feature: %s", feature_name)
            effective_config = feature_manager._get_effective_feature_config(feature_config, TARGET_SUBSET_KEY)
            extractor = None
            base_features_np: Optional[np.ndarray] = None
            try:
                extractor = feature_manager._get_feature_extractor(effective_config)
                params = effective_config.get("params", {})
                logger.debug("Extracting with %s using params: %s", extractor.__class__.__name__, params)
                base_features_raw = extractor.extract(
                    audio_data=audio_array,
                    sample_rate=sample_rate,
                    **params,
                )

                if isinstance(base_features_raw, torch.Tensor):
                    base_features_np = base_features_raw.cpu().numpy()
                elif isinstance(base_features_raw, np.ndarray):
                    base_features_np = base_features_raw
                elif base_features_raw is None:
                    logger.warning("Extractor returned None.")
                    base_features_np = None
                else:
                    logger.warning("Extractor returned unexpected type: %s. Treating as failure.", type(base_features_raw))
                    base_features_np = None

                if base_features_np is not None:
                    if base_features_np.size == 0:
                        logger.warning("Extractor returned empty features.")
                        base_features_np = None
                    elif np.isnan(base_features_np).any() or np.isinf(base_features_np).any():
                        logger.error("Features contain NaN/Inf! Discarding.")
                        base_features_np = None

                if base_features_np is not None:
                    save_npy_feature(base_features_np, f"{feature_name}_BASE", output_subdir)
                else:
                    logger.error("Failed to get valid base features for %s.", feature_name)

                extracted_base_features[feature_name] = base_features_np

            except Exception as e:
                logger.error("Error processing BASE feature %s: %s", feature_name, e, exc_info=True)
                extracted_base_features[feature_name] = None
            finally:
                gc.collect()

    logger.info("\n--- PASS 2: Processing and Saving INTERMEDIATE Features (No PCA) ---")
    for feature_config in feature_configs:
        feature_name = feature_config.get("name")
        if not feature_name:
            continue

        base_extractor_name = feature_config.get("base_extractor")
        averaging_method = feature_config.get("averaging")
        has_pca = "pca" in feature_config

        if not base_extractor_name or has_pca or averaging_method is None:
            continue

        logger.info("Processing INTERMEDIATE feature: %s", feature_name)
        logger.info("  (Base: %s, Avg: %s)", base_extractor_name, averaging_method)

        base_features_np = extracted_base_features.get(base_extractor_name)

        if base_features_np is None:
            logger.error(
                "Base features for '%s' were not successfully extracted or are missing. Cannot generate intermediate feature '%s'.",
                base_extractor_name,
                feature_name,
            )
            continue

        intermediate_features_np: Optional[np.ndarray] = None
        try:
            logger.debug("Applying averaging '%s' to base features of shape %s", averaging_method, base_features_np.shape)
            intermediate_features_np = apply_averaging(base_features_np, averaging_method)

            if intermediate_features_np is not None:
                if np.isnan(intermediate_features_np).any() or np.isinf(intermediate_features_np).any():
                    logger.error("Intermediate features for %s contain NaN/Inf! Skipping save.", feature_name)
                elif intermediate_features_np.size == 0:
                    logger.warning("Intermediate features for %s are empty. Skipping save.", feature_name)
                else:
                    save_npy_feature(intermediate_features_np, f"{feature_name}_INTERMEDIATE", output_subdir)
            else:
                logger.error(
                    "Averaging method '%s' failed for base features of '%s' (shape: %s). Cannot generate intermediate feature '%s'.",
                    averaging_method,
                    base_extractor_name,
                    base_features_np.shape,
                    feature_name,
                )

        except Exception as e:
            logger.error("Error processing INTERMEDIATE feature %s: %s", feature_name, e, exc_info=True)
        finally:
            if intermediate_features_np is not None:
                del intermediate_features_np
            gc.collect()

    feature_manager._clear_extractor_cache()
    del extracted_base_features
    gc.collect()
    if device.type == "cuda":
        try:
            torch.cuda.empty_cache()
        except Exception:
            pass

    logger.info("--- Single Sample Feature Extraction Script Finished ---")


if __name__ == "__main__":
    main()import datetime
import gc
import logging
import sys
import os
from pathlib import Path
from typing import Optional
import pandas as pd
import argparse
import re
import numpy as np
from collections import Counter
import json
import torch
import librosa


try:
    script_path = Path(__file__).resolve()
    project_root_candidate1 = script_path.parents[1]
    project_root_candidate2 = script_path.parents[3]

    if (project_root_candidate1 / "vocsim" / "runner.py").exists():
        project_root = project_root_candidate1
    elif (project_root_candidate2 / "vocsim" / "runner.py").exists():
        project_root = project_root_candidate2
    elif (Path.cwd() / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARNING: Script path resolution heuristic failed. Assuming CWD is project root: {project_root}")
    else:
        project_root = Path.cwd()
        print(f"ERROR: Cannot determine project root relative to script location. runner.py not found. Assuming CWD: {project_root}")
        print("Please ensure this script is run from within the project or adjust path setup if it's moved.")

    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root to sys.path: {project_root}")
except NameError:
    project_root = Path.cwd()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

try:
    from vocsim.managers.dataset_manager import DatasetManager
    from utils.config_loader import load_config
    from utils.logging_utils import setup_logging
except ImportError as e:
    print(f"ERROR: Could not import project modules: {e}. Ensure an __init__.py exists in 'vocsim' and 'utils' directories and that the project root ({project_root}) is correctly in PYTHONPATH.")
    print(f"Current sys.path: {sys.path}")
    sys.exit(1)


logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

BLIND_TEST_SUBSETS = ["HU3", "HU4", "HW3", "HW4"]


class NpEncoder(json.JSONEncoder):
    """Helper class for JSON encoding NumPy types, Path, Tensors etc."""

    def default(self, obj):
        """
        Encodes various object types, including NumPy arrays, scalars, Path objects,
        and PyTorch Tensors, into JSON-serializable formats.

        Args:
            obj: The object to encode.

        Returns:
            JSON-serializable representation of the object.
        """
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj) if np.isfinite(obj) else None
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        if isinstance(obj, (torch.Tensor)):
            return obj.detach().cpu().numpy().tolist()
        if isinstance(obj, Path):
            return str(obj)
        if obj is None:
            return None
        if isinstance(obj, set):
            return list(obj)
        try:
            return super().default(obj)
        except TypeError:
            return str(obj)


def sanitize_latex_str(text: str) -> str:
    """
    Sanitizes a string for LaTeX output by escaping special characters.

    Args:
        text (str): The input string.

    Returns:
        str: The sanitized string.
    """
    if not isinstance(text, str):
        text = str(text)
    replacements = {
        "&": r"\&",
        "%": r"\%",
        "$": r"\$",
        "#": r"\#",
        "_": r"\_",
        "{": r"\{",
        "}": r"\}",
        "~": r"\textasciitilde{}",
        "^": r"\^{}",
        "\\": r"\textbackslash{}",
        "<": r"\textless{}",
        ">": r"\textgreater{}",
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text


def df_to_latex_custom(df: pd.DataFrame, caption: str, label: str, column_format: str = None) -> str:
    """
    Converts a Pandas DataFrame to a LaTeX table string with sanitization.

    Args:
        df (pd.DataFrame): The input DataFrame.
        caption (str): The caption for the LaTeX table.
        label (str): The label for the LaTeX table.
        column_format (str, optional): LaTeX column format string. Defaults to None,
                                        in which case pandas infers it.

    Returns:
        str: The LaTeX table string.
    """
    latex_parts = []
    df_sanitized = df.copy()
    df_sanitized.columns = [sanitize_latex_str(col) for col in df_sanitized.columns]
    if df_sanitized.index.name:
        df_sanitized.index.name = sanitize_latex_str(df_sanitized.index.name)

    for col in df_sanitized.columns:
        df_sanitized[col] = df_sanitized[col].apply(lambda x: sanitize_latex_str(str(x)) if pd.notna(x) else "-")

    if not isinstance(df_sanitized.index, pd.RangeIndex):
        df_sanitized.index = [sanitize_latex_str(str(idx_val)) for idx_val in df_sanitized.index]

    latex_parts.append("\\begin{table}[htp!]")
    latex_parts.append("\\centering")
    latex_parts.append(f"\\caption{{{sanitize_latex_str(caption)}}}")
    latex_parts.append(f"\\label{{{sanitize_latex_str(label)}}}")
    latex_table_body = df_sanitized.to_latex(escape=False, index=True, na_rep="-", column_format=column_format)
    latex_parts.append(latex_table_body)
    latex_parts.append("\\end{table}")
    return "\n".join(latex_parts)


def calculate_snr_energy_based(
    audio_array: np.ndarray,
    sample_rate: int,
    frame_length_ms: float = 30,
    hop_length_ms: float = 15,
    signal_energy_percentile: float = 80,
    noise_energy_percentile: float = 20,
    min_frames_for_estimation: int = 10,
    silence_threshold_rms: float = 1e-4,
) -> float:
    """
    Estimates SNR based on energy percentiles of audio frames.

    Args:
        audio_array (np.ndarray): 1D NumPy array containing the audio waveform.
        sample_rate (int): Sample rate of the audio.
        frame_length_ms (float): Length of each frame in milliseconds.
        hop_length_ms (float): Hop length between frames in milliseconds.
        signal_energy_percentile (float): Percentile used to define signal frames.
        noise_energy_percentile (float): Percentile used to define noise frames.
        min_frames_for_estimation (int): Minimum number of frames needed for estimation.
        silence_threshold_rms (float): RMS threshold to consider audio silent.

    Returns:
        float: Estimated SNR in dB, or np.nan if calculation is not robustly possible.
    """
    local_logger = logging.getLogger(__name__ + ".snr_calc")

    if not isinstance(audio_array, np.ndarray) or audio_array.ndim != 1:
        local_logger.debug("SNR: Input audio must be a 1D NumPy array.")
        return np.nan
    if audio_array.size == 0:
        local_logger.debug("SNR: Input audio array is empty.")
        return np.nan
    if sample_rate <= 0:
        local_logger.debug("SNR: Invalid sample rate.")
        return np.nan

    try:
        audio_float32 = audio_array.astype(np.float32)
        if np.max(np.abs(audio_float32)) < silence_threshold_rms:
            local_logger.debug("SNR: Audio is likely silent based on overall RMS. Returning 0 dB.")
            return 0.0

        frame_length = int(frame_length_ms / 1000 * sample_rate)
        hop_length = int(hop_length_ms / 1000 * sample_rate)

        if frame_length <= 0 or hop_length <= 0 or frame_length > audio_float32.size:
            if audio_float32.size > 0:
                power = np.mean(audio_float32**2)
                return 10 * np.log10(power / 1e-12) if power > 1e-10 else 0.0
            return np.nan

        rms_energy = librosa.feature.rms(y=audio_float32, frame_length=frame_length, hop_length=hop_length)[0]

        if rms_energy.size < min_frames_for_estimation:
            local_logger.debug(f"SNR: Not enough frames ({rms_energy.size}) for robust estimation. Min required: {min_frames_for_estimation}.")
            if rms_energy.size > 0:
                mean_power = np.mean(rms_energy**2)
                return 10 * np.log10(mean_power / (mean_power * 0.01 + 1e-12)) if mean_power > 1e-10 else 0.0
            return np.nan

        energy_values_squared = rms_energy**2

        power_threshold_signal = np.percentile(energy_values_squared, signal_energy_percentile)
        power_threshold_noise = np.percentile(energy_values_squared, noise_energy_percentile)

        if power_threshold_signal <= power_threshold_noise:
            if np.isclose(power_threshold_signal, power_threshold_noise):
                power_threshold_noise = power_threshold_signal * 0.9
                power_threshold_signal = power_threshold_signal * 1.1
                if power_threshold_signal < 1e-10:
                    power_threshold_signal = 1e-10
            else:
                local_logger.debug(
                    "SNR: Signal power threshold (%.2e) <= noise threshold (%.2e)." " Assuming low SNR or constant audio.",
                    power_threshold_signal,
                    power_threshold_noise,
                )
                return 0.0

        signal_powers = energy_values_squared[energy_values_squared >= power_threshold_signal]
        noise_powers = energy_values_squared[energy_values_squared <= power_threshold_noise]

        if len(signal_powers) < (min_frames_for_estimation // 2) or len(noise_powers) < (min_frames_for_estimation // 2):
            local_logger.debug(
                "SNR: Insufficient distinct signal (%d) or noise (%d) frames after thresholding.",
                len(signal_powers),
                len(noise_powers),
            )
            if len(signal_powers) < (min_frames_for_estimation // 2) and len(energy_values_squared) > 0:
                signal_powers = energy_values_squared
            if len(noise_powers) < (min_frames_for_estimation // 2) and len(energy_values_squared) > 0:
                noise_powers = np.array([np.percentile(energy_values_squared, 10)])

        if len(signal_powers) == 0 and len(noise_powers) == 0:
            return np.nan
        if len(signal_powers) == 0:
            return 0.0

        avg_signal_power = np.mean(signal_powers) if len(signal_powers) > 0 else 0.0
        avg_noise_power = np.mean(noise_powers) if len(noise_powers) > 0 else 1e-12

        if avg_noise_power < 1e-12:
            avg_noise_power = 1e-12
        if avg_signal_power < 1e-12:
            return 0.0

        snr_ratio = avg_signal_power / avg_noise_power
        snr_db = 10 * np.log10(snr_ratio)

        return max(0, snr_db)

    except Exception as e:
        local_logger.error("SNR calculation error: %s", e, exc_info=False)
        return np.nan


def calculate_dataset_characteristics(config_path: Path, base_config_path: Optional[Path] = None) -> pd.DataFrame:
    """
    Calculates characteristics for datasets specified in the VocSim configuration.

    Processes each dataset subset listed in the config, calculates metrics
    like number of samples, classes, samples per class, average duration,
    availability (based on blind test status), and estimated SNR.

    Args:
        config_path (Path): Path to the main configuration file.
        base_config_path (Optional[Path]): Path to the base configuration file.

    Returns:
        pd.DataFrame: A DataFrame where the index is the subset ID and columns
                      are the calculated characteristics.
    """
    cfg = load_config(config_path, base_config_path=base_config_path)
    if not cfg:
        logger.error("Failed to load configuration.")
        return pd.DataFrame()

    dataset_manager = DatasetManager(cfg)
    if not dataset_manager.load_full_dataset():
        logger.error("Failed to load the full dataset via DatasetManager.")
        return pd.DataFrame()

    subsets_to_run_config = cfg.get("dataset", {}).get("subsets_to_run")
    if subsets_to_run_config is None:
        top_level_subset = cfg.get("dataset", {}).get("subset")
        subsets_to_run = [top_level_subset] if top_level_subset else ["all"]
        if subsets_to_run == [None]:
            subsets_to_run = ["all"]
    elif isinstance(subsets_to_run_config, str):
        subsets_to_run = [subsets_to_run_config]
    elif isinstance(subsets_to_run_config, list):
        subsets_to_run = subsets_to_run_config
    else:
        subsets_to_run = ["all"]

    all_characteristics_records = []
    default_label_key = cfg.get("dataset", {}).get("default_label_column", "label")
    logger.info("Will use '%s' for class label extraction.", default_label_key)

    for subset_key in subsets_to_run:
        logger.info("Processing subset: %s...", subset_key)
        subset_info = dataset_manager.get_subset_dataset(subset_key)
        if subset_info is None:
            logger.warning("Could not load subset '%s'. Skipping.", subset_key)
            continue

        subset_dataset_obj, _ = subset_info

        num_samples_in_subset = 0
        all_labels_in_subset = []
        all_durations_in_subset = []
        all_snr_values_in_subset = []

        logger.info("Iterating through items in subset '%s' for characteristic calculation...", subset_key)

        for item_idx, item_data in enumerate(subset_dataset_obj):
            num_samples_in_subset += 1
            label_value = item_data.get(default_label_key)
            all_labels_in_subset.append(str(label_value) if label_value is not None else None)

            audio_array_np = None
            sample_rate = None
            current_duration = np.nan
            current_snr = np.nan

            audio_info = item_data.get("audio")
            if audio_info and "array" in audio_info and "sampling_rate" in audio_info:
                audio_array_orig = audio_info["array"]
                sample_rate = audio_info["sampling_rate"]

                if audio_array_orig is not None and sample_rate is not None and sample_rate > 0:
                    if isinstance(audio_array_orig, torch.Tensor):
                        audio_array_np = audio_array_orig.cpu().numpy().astype(np.float32)
                    elif isinstance(audio_array_orig, np.ndarray):
                        audio_array_np = audio_array_orig.astype(np.float32)
                    elif isinstance(audio_array_orig, list):
                        audio_array_np = np.array(audio_array_orig, dtype=np.float32)

                    if audio_array_np is not None:
                        if audio_array_np.ndim > 1:
                            audio_array_np = np.mean(audio_array_np, axis=0) if audio_array_np.shape[0] < audio_array_np.shape[1] else np.mean(audio_array_np, axis=1)

                        if audio_array_np.ndim == 1 and audio_array_np.size > 0:
                            current_duration = len(audio_array_np) / sample_rate
                            current_snr = calculate_snr_energy_based(audio_array_np, sample_rate)

            all_durations_in_subset.append(current_duration)
            all_snr_values_in_subset.append(current_snr)

            if (item_idx + 1) % 1000 == 0:
                logger.info("  Processed %d items in '%s'...", item_idx + 1, subset_key)

        logger.info(f"Finished item iteration for subset '{subset_key}'. Total items processed: {num_samples_in_subset}")

        num_classes = 0
        sam_per_cls_avg_range_str = "N/A"
        if all_labels_in_subset:
            valid_labels = [l for l in all_labels_in_subset if l is not None]
            if valid_labels:
                label_counts = Counter(valid_labels)
                num_classes = len(label_counts)
                if num_classes > 0:
                    counts_per_class_values = list(label_counts.values())
                    avg_sam_per_cls_val = sum(counts_per_class_values) / num_classes
                    min_c = min(counts_per_class_values)
                    max_c = max(counts_per_class_values)
                    sam_per_cls_avg_range_str = f"{avg_sam_per_cls_val:.1f} ({min_c}-{max_c})"

        avg_dur_s_min_max_str = "N/A"
        avg_dur_for_sort = np.nan
        valid_durations = [d for d in all_durations_in_subset if pd.notna(d)]
        if valid_durations:
            avg_dur = np.mean(valid_durations)
            min_dur = np.min(valid_durations)
            max_dur = np.max(valid_durations)
            avg_dur_s_min_max_str = f"{avg_dur:.2f} ({min_dur:.2f}-{max_dur:.2f})"
            avg_dur_for_sort = avg_dur

        snr_db_val_str = "N/A"
        valid_snr_values = [s for s in all_snr_values_in_subset if pd.notna(s) and np.isfinite(s)]
        if valid_snr_values:
            avg_snr_db = np.mean(valid_snr_values)
            snr_db_val_str = f"{avg_snr_db:.0f}"

        avail_char = "X" if subset_key in BLIND_TEST_SUBSETS else "\u2713"

        all_characteristics_records.append({
            "ID": subset_key,
            "N. Samples": num_samples_in_subset,
            "Classes": num_classes,
            "Sam/Cls (avg, range)": sam_per_cls_avg_range_str,
            "Avg. Dur (s) (min-max)": avg_dur_s_min_max_str,
            "Avail": avail_char,
            "SNR (dB)": snr_db_val_str,
            "_sort_avg_dur": avg_dur_for_sort,
        })
        logger.info("Aggregated characteristics for subset: %s", subset_key)
        del subset_dataset_obj, all_labels_in_subset, all_durations_in_subset, all_snr_values_in_subset
        gc.collect()

    df = pd.DataFrame(all_characteristics_records)
    ordered_columns = ["ID", "N. Samples", "Classes", "Sam/Cls (avg, range)", "Avg. Dur (s) (min-max)", "Avail", "SNR (dB)"]
    if "_sort_avg_dur" in df.columns:
        df = df[["ID", "_sort_avg_dur"] + ordered_columns[1:]]

    df = df.set_index("ID")
    return df


def main():
    """
    Main function to calculate and save dataset characteristics to CSV and LaTeX files.
    """
    parser = argparse.ArgumentParser(description="Generate dataset characteristics table from VocSim configuration.")
    parser.add_argument(
        "config_file",
        type=str,
        help="Path to the main VocSim YAML configuration file (e.g., reproducibility/configs/vocsim_paper.yaml)",
    )
    parser.add_argument(
        "--base_config_file",
        type=str,
        default=None,
        help="Optional path to a base YAML configuration file (e.g., configs/base.yaml)",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="generated_dataset_tables",
        help="Directory to save the output CSV and LaTeX files (relative to project root or absolute).",
    )
    args = parser.parse_args()

    config_p = Path(args.config_file)
    base_config_p = Path(args.base_config_file) if args.base_config_file else None

    output_dir_arg = Path(args.output_dir)
    output_p = output_dir_arg.resolve() if output_dir_arg.is_absolute() else (project_root / output_dir_arg).resolve()
    output_p.mkdir(parents=True, exist_ok=True)

    logger.info("Using main config file: %s", config_p)
    if base_config_p and base_config_p.exists():
        logger.info("Using base config file: %s", base_config_p)
    elif base_config_p:
        logger.warning("Base config file specified but not found: %s", base_config_p)
        base_config_p = None

    characteristics_df = calculate_dataset_characteristics(config_p, base_config_p)

    if characteristics_df.empty:
        logger.error("No characteristics data generated. Exiting.")
        return

    final_display_columns = ["N. Samples", "Classes", "Sam/Cls (avg, range)", "Avg. Dur (s) (min-max)", "Avail", "SNR (dB)"]
    try:
        if "_sort_avg_dur" in characteristics_df.columns:
            characteristics_df_sorted = characteristics_df.sort_values(by="_sort_avg_dur", ascending=True, na_position="last")
            characteristics_df_final = characteristics_df_sorted[final_display_columns]
        else:
            logger.warning("'_sort_avg_dur' column not found. Cannot sort by average duration. Using original order.")
            characteristics_df_final = characteristics_df[final_display_columns]
    except KeyError as e:
        logger.error("KeyError during sorting or column selection: %s. Using unsorted DataFrame with available columns.", e)
        available_cols = [col for col in final_display_columns if col in characteristics_df.columns]
        characteristics_df_final = characteristics_df[available_cols]
    except Exception as e:
        logger.warning("Could not sort by average duration: %s. Using unsorted DataFrame.", e)
        available_cols = [col for col in final_display_columns if col in characteristics_df.columns]
        characteristics_df_final = characteristics_df[available_cols]

    csv_filename = output_p / "dataset_characteristics_calculated.csv"
    try:
        characteristics_df_final.to_csv(csv_filename, index=True, encoding="utf-8")
        logger.info("Dataset characteristics saved to CSV: %s", csv_filename)
    except Exception as e:
        logger.error("Failed to save CSV: %s", e)

    latex_filename = output_p / "dataset_characteristics_calculated.tex"
    column_format_str = "l" + "c" * len(characteristics_df_final.columns)
    latex_caption = "Calculated Characteristics of Datasets Processed by VocSim"
    latex_label = "tab:dataset_characteristics_calculated"

    try:
        latex_string = df_to_latex_custom(characteristics_df_final, latex_caption, latex_label, column_format_str)
        with open(latex_filename, "w", encoding="utf-8") as f:
            f.write(latex_string)
        logger.info("Dataset characteristics saved to LaTeX: %s", latex_filename)
    except Exception as e:
        logger.error("Failed to save LaTeX: %s", e)
        try:
            if "latex_string" not in locals():
                latex_string = characteristics_df_final.to_latex(escape=True, index=True, na_rep="-", column_format=column_format_str)
            print("\n--- LaTeX Fallback Output ---\n" + latex_string + "\n--- End LaTeX Fallback ---")
        except Exception as e_print:
            logger.error("Failed to even print LaTeX to console: %s", e_print)


if __name__ == "__main__":
    main()# -*- coding: utf-8 -*-
"""
Script to generate images of VAE/AE inputs and reconstructions, and an example P@1 table.
"""

import logging
import sys
import os
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, Union, List

import torch
import numpy as np
import matplotlib.pyplot as plt
import librosa
import librosa.display
import gc
from tqdm import tqdm
import torch
import torchaudio
import torch.nn.functional as F
import pandas as pd
import argparse
from torch.distributions import LowRankMultivariateNormal

# --- Project Path Setup ---
try:
    script_path = Path(__file__).resolve()
    project_root_candidate1 = script_path.parents[1]
    project_root_candidate2 = script_path.parents[3]

    if (project_root_candidate1 / "vocsim" / "runner.py").exists():
        project_root = project_root_candidate1
    elif (project_root_candidate2 / "vocsim" / "runner.py").exists():
        project_root = project_root_candidate2
    elif (Path.cwd() / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARNING: Script path resolution heuristic failed. Assuming CWD is project root: {project_root}")
    else:
        project_root = Path.cwd()
        print(f"ERROR: Cannot determine project root relative to script location. runner.py not found. Assuming CWD: {project_root}")
        print("Please ensure this script is run from within the project or adjust path setup if it's moved.")

    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root to sys.path: {project_root}")
except NameError:
    project_root = Path.cwd()
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

try:
    from vocsim.managers.dataset_manager import DatasetManager
    from utils.config_loader import load_config
    from utils.logging_utils import setup_logging
    from utils.torch_utils import get_device

    from reproducibility.models.vae import VariationalAutoencoder, preprocess_vae_input
    from reproducibility.models.autoencoder import Autoencoder, AudioConfig
except ImportError as e:
    print(f"ERROR: Could not import project modules: {e}. Ensure an __init__.py exists in 'vocsim' and 'utils' directories and that the project root ({project_root}) is correctly in PYTHONPATH.")
    print(f"Current sys.path: {sys.path}")
    sys.exit(1)


logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

CONFIG_NAME = "vocsim_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"

OUTPUT_DIR = project_root / "reproducibility_outputs" / "vae_ae_images"
MODEL_SCOPE = "all"


def find_first_item_for_subset(dataset_manager: DatasetManager, subset_key: str) -> Optional[Dict[str, Any]]:
    """
    Finds the first item in a dataset subset that has valid audio data.

    Args:
        dataset_manager (DatasetManager): The dataset manager instance.
        subset_key (str): The key of the subset to search.

    Returns:
        Optional[Dict[str, Any]]: The first valid item found, or None if none is found.
    """
    logger.info("Searching for first item in subset '%s'...", subset_key)
    if dataset_manager.full_dataset_obj is None:
        logger.error("Full dataset not loaded in DatasetManager.")
        return None
    try:
        for item in dataset_manager.full_dataset_obj:
            if item.get("subset") == subset_key:
                if item.get("audio") and item["audio"].get("array") is not None:
                    logger.info("Found item for subset '%s'.", subset_key)
                    return item
                else:
                    logger.warning("Found item for subset '%s' but audio data is missing/invalid.", subset_key)
    except Exception as e:
        logger.error("Error searching for subset item '%s': %s", subset_key, e, exc_info=True)

    logger.warning("Could not find any valid items for subset '%s'.", subset_key)
    return None


def load_paper_model(model_type: str, config: Dict[str, Any], device: torch.device) -> Optional[torch.nn.Module]:
    """
    Loads either the VAE or AE model trained on the 'all' scope.

    Args:
        model_type (str): Type of model to load ('vae' or 'ae').
        config (Dict[str, Any]): The loaded configuration dictionary.
        device (torch.device): The device to load the model onto.

    Returns:
        Optional[torch.nn.Module]: The loaded model instance, or None if loading fails.
    """
    model_class = None
    model_params = None
    trainer_base_name = ""
    checkpoint_key = "model_state_dict"
    definition_key = ""

    models_base_dir = Path(config.get("models_dir", project_root / "models")).resolve()

    if model_type.lower() == "vae":
        model_class = VariationalAutoencoder
        trainer_base_name = "PaperVAETrainer"
        definition_key = "vae_model_def"

    elif model_type.lower() == "ae":
        model_class = Autoencoder
        trainer_base_name = "PaperAutoencoderTrainer"
        definition_key = "ae_model_def"

    else:
        logger.error("Unknown model_type: %s", model_type)
        return None

    logger.debug("Searching for %s params...", model_type.upper())
    definitions = config.get("definitions", {})
    if definition_key in definitions:
        model_params = definitions[definition_key].get("params")
        if model_params:
            logger.info("Found %s params in config['definitions']['%s'].", model_type.upper(), definition_key)
        else:
            logger.warning("Found '%s' in definitions, but 'params' key is missing or empty.", definition_key)

    if model_params is None:
        logger.debug("'%s' params not found in definitions. Searching 'train' block...", definition_key)
        train_jobs = config.get("train", [])
        for job in train_jobs:
            if job.get("trainer", {}).get("name") == trainer_base_name:
                model_cfg_in_train = job.get("model", {})
                model_params = model_cfg_in_train.get("params")
                if model_params:
                    logger.info("Found %s params in config['train'] block for '%s'.", model_type.upper(), trainer_base_name)
                    break
        if model_params is None:
            logger.error("Could not find %s model 'params' in definitions or train blocks.", model_type.upper())
            return None

    instance_params = {}
    try:
        if model_type.lower() == "vae":
            instance_params["z_dim"] = int(model_params["z_dim"])
            instance_params["model_precision"] = model_params.get("model_precision", 10.0)
            instance_params["device_name"] = device.type
        elif model_type.lower() == "ae":
            audio_config_dict = model_params["audio_config"]
            dims_dict = model_params.get("dimensions", {})
            instance_params["config"] = AudioConfig(**audio_config_dict)
            instance_params["max_spec_width"] = dims_dict.get("max_spec_width", 256)
            instance_params["bottleneck_dim"] = int(model_params["bottleneck_dim"])
    except KeyError as e:
        logger.error("Missing required key '%s' in found %s 'params'. Params found: %s", e, model_type.upper(), model_params)
        return None
    except Exception as e:
        logger.error(f"Error processing {model_type.upper()} params for instantiation: {e}. Params: {model_params}")
        return None

    scoped_trainer_name = f"{trainer_base_name}_{MODEL_SCOPE}"
    checkpoint_dir = models_base_dir / scoped_trainer_name / "checkpoints"
    model_path = checkpoint_dir / "final_model.pt"

    if not model_path.exists():
        logger.warning("'final_model.pt' not found in %s. Looking for latest checkpoint...", checkpoint_dir)
        available_checkpoints = sorted(checkpoint_dir.glob("*.pt"), key=os.path.getmtime, reverse=True)
        if available_checkpoints:
            model_path = available_checkpoints[0]
            logger.info("Using latest checkpoint: %s", model_path.name)
        else:
            logger.error("No %s model checkpoint found for scope '%s' in %s", model_type.upper(), MODEL_SCOPE, checkpoint_dir)
            return None
    try:
        logger.info("Instantiating %s model...", model_type.upper())
        model = model_class(**instance_params)

        logger.info("Loading %s checkpoint: %s", model_type.upper(), model_path)
        checkpoint = torch.load(model_path, map_location=device)
        state_dict = checkpoint.get(checkpoint_key)
        if state_dict is None:
            dummy_keys = model.state_dict().keys()
            if all(k in checkpoint for k in dummy_keys):
                state_dict = checkpoint
            else:
                raise KeyError(f"Checkpoint key '{checkpoint_key}' not found and checkpoint is not a state_dict. Keys: {list(checkpoint.keys())}")

        missing, unexpected = model.load_state_dict(state_dict, strict=False)
        if missing:
            logger.warning("Missing keys loading %s state_dict: %s", model_type.upper(), missing)
        if unexpected:
            logger.warning("Unexpected keys loading %s state_dict: %s", model_type.upper(), unexpected)

        model.to(device)
        model.eval()
        logger.info("%s model (Scope: %s) loaded successfully to %s.", model_type.upper(), MODEL_SCOPE, device)
        return model
    except Exception as e:
        logger.error("Failed to load/initialize %s model from %s: %s", model_type.upper(), model_path, e, exc_info=True)
        return None


def plot_spectrogram(
    spec_data: np.ndarray,
    title: str,
    output_path: Path,
    sr: int,
    hop_length: int,
    is_mel: bool = True,
    fmin: Optional[float] = None,
    fmax: Optional[float] = None,
    n_fft: Optional[int] = None,
):
    """
    Plots and saves a spectrogram.

    Args:
        spec_data (np.ndarray): 2D numpy array containing spectrogram data.
        title (str): Title for the plot.
        output_path (Path): Path to save the plot file.
        sr (int): Sample rate used for the spectrogram.
        hop_length (int): Hop length used for the spectrogram.
        is_mel (bool): True if it's a Mel spectrogram, False otherwise.
        fmin (Optional[float]): Minimum frequency for plotting (for non-Mel).
        fmax (Optional[float]): Maximum frequency for plotting (for non-Mel).
        n_fft (Optional[int]): FFT size used for the spectrogram (for non-Mel).
    """
    if spec_data is None or spec_data.size == 0:
        logger.warning("Cannot plot empty spectrogram for %s", title)
        return
    plt.figure(figsize=(10, 4))
    y_axis = "mel" if is_mel else "linear"
    plot_fmin = fmin if fmin is not None else 0
    plot_fmax = fmax if fmax is not None else sr / 2.0
    try:
        img = librosa.display.specshow(
            spec_data, sr=sr, hop_length=hop_length, x_axis="time", y_axis=y_axis, fmin=plot_fmin, fmax=plot_fmax, cmap="magma"
        )
        plt.colorbar(img, format="%+2.0f dB" if is_mel and np.any(spec_data < -10) else "%.2f")
        plt.title(title)
        plt.tight_layout()
        plt.savefig(output_path, format="svg", bbox_inches="tight")
        logger.info("Saved plot: %s", output_path.name)
    except Exception as e:
        logger.error("Failed to plot/save spectrogram %s: %s", output_path, e, exc_info=True)
    finally:
        plt.close()


def plot_latent_vector(latent_vec: np.ndarray, title: str, output_path: Path):
    """
    Plots and saves a 1D latent vector as an image.

    Args:
        latent_vec (np.ndarray): 1D numpy array containing the latent vector.
        title (str): Title for the plot.
        output_path (Path): Path to save the plot file.
    """
    if latent_vec is None or latent_vec.size == 0:
        logger.warning("Cannot plot empty latent vector for %s", title)
        return
    plt.figure(figsize=(10, 1))
    try:
        img = plt.imshow(latent_vec.reshape(1, -1), aspect="auto", cmap="viridis")
        plt.colorbar(img)
        plt.title(title)
        plt.xlabel("Latent Dimension")
        plt.yticks([])
        plt.tight_layout()
        plt.savefig(output_path, format="svg", bbox_inches="tight")
        logger.info("Saved plot: %s", output_path.name)
    except Exception as e:
        logger.error("Failed to plot/save latent vector %s: %s", output_path, e, exc_info=True)
    finally:
        plt.close()


def sanitize_latex_str(text: str) -> str:
    """
    Sanitizes a string for LaTeX output by escaping special characters.

    Args:
        text (str): The input string.

    Returns:
        str: The sanitized string.
    """
    if not isinstance(text, str):
        text = str(text)
    replacements = {
        "&": r"\&",
        "%": r"\%",
        "$": r"\$",
        "#": r"\#",
        "_": r"\_",
        "{": r"\{",
        "}": r"\}",
        "~": r"\textasciitilde{}",
        "^": r"\^{}",
        "\\": r"\textbackslash{}",
        "<": r"\textless{}",
        ">": r"\textgreater{}",
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text


def generate_p1_latex_table(
    data_rows: list[dict],
    table_label: str = "tab:p1_results",
    caption_text: str = "P@1 Results Across Subsets and Distances ($\\uparrow$ better)",
    table_format_str: str = "2.1",
) -> str:
    """
    Generates LaTeX code for a P@1 results longtable.

    Args:
        data_rows: A list of dictionaries, where each dictionary represents a row.
                   Example: {"Method": "M1", "Dist": "D1", "BC": 10.1, ...}
        table_label: The LaTeX label for the table.
        caption_text: The caption for the table.
        table_format_str: The siunitx table-format string (e.g., "2.1", "3.1").
                          Ensure this matches your data range (e.g., "3.1" if 100.0 is possible).
    Returns:
        A string containing the LaTeX code for the table.
    """
    num_s_columns = 21
    total_columns = 2 + num_s_columns

    s_column_headers_config = [
        {"key": "BC", "display": "BC"},
        {"key": "BS1", "display": "BS1"},
        {"key": "BS2", "display": "BS2"},
        {"key": "BS3", "display": "BS3"},
        {"key": "BS4", "display": "BS4"},
        {"key": "BS5", "display": "BS5"},
        {"key": "ES1", "display": "ES1"},
        {"key": "HP", "display": "HP"},
        {"key": "HS1", "display": "HS1"},
        {"key": "HS2", "display": "HS2"},
        {"key": "HU1", "display": "HU1"},
        {"key": "HU2", "display": "HU2"},
        {"key": "HU3", "display": "HU3"},
        {"key": "HU4", "display": "HU4"},
        {"key": "HW1", "display": "HW1"},
        {"key": "HW2", "display": "HW2"},
        {"key": "HW3", "display": "HW3"},
        {"key": "HW4", "display": "HW4"},
        {"key": "OC1", "display": "OC1"},
        {"key": "Avg", "display": "Avg"},
        {"key": "Avg (Blind)", "display": "\\makecell{Avg\\\\(Blind)}"},
    ]
    s_column_keys = [item["key"] for item in s_column_headers_config]

    latex_lines = []
    latex_lines.append("% Add to your LaTeX preamble: \\usepackage{longtable, booktabs, siunitx, makecell}")
    latex_lines.append("\\scriptsize")
    latex_lines.append("\\setlength{\\tabcolsep}{2pt}")
    latex_lines.append(f"% For S columns, table-format={table_format_str} is used. Ensure this matches your data (e.g., use \"3.1\" if scores like 100.0 can occur).")
    latex_lines.append(f"\\begin{{longtable}}{{l l *{{{num_s_columns}}}{{S[table-format={table_format_str}]}}}}")

    latex_lines.append(f"\\caption{{{caption_text}}}\\label{{{table_label}}}\\\\")
    latex_lines.append("\\toprule")

    header_line_parts = ["Method", "Dist"]
    for item in s_column_headers_config:
        header_line_parts.append(f"\\multicolumn{{1}}{{c}}{{{item['display']}}}")
    header_line = " & ".join(header_line_parts) + " \\\\"

    latex_lines.append(header_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endfirsthead")
    latex_lines.append("")
    latex_lines.append(f"\\caption[]{{(Continued) {caption_text}}}\\\\")
    latex_lines.append("\\toprule")
    latex_lines.append(header_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endhead")
    latex_lines.append("")

    latex_lines.append("\\midrule")
    latex_lines.append(f"\\multicolumn{{{total_columns}}}{{r}}{{\\textit{{Continued on next page}}}}\\\\")
    latex_lines.append("\\endfoot")
    latex_lines.append("")

    latex_lines.append("\\bottomrule")
    latex_lines.append("\\endlastfoot")
    latex_lines.append("")

    for row_dict in data_rows:
        row_str_parts = [str(row_dict.get("Method", "")), str(row_dict.get("Dist", ""))]
        for key in s_column_keys:
            val = row_dict.get(key)
            if isinstance(val, (int, float)):
                row_str_parts.append(str(val))
            elif val is None or str(val).strip() == "":
                row_str_parts.append("{}")
            else:
                row_str_parts.append(f"\\multicolumn{{1}}{{c}}{{{str(val)}}}")
        latex_lines.append(" & ".join(row_str_parts) + " \\\\")

    latex_lines.append("\\end{longtable}")
    return "\n".join(latex_lines)


def main():
    """
    Main function to generate VAE/AE input/reconstruction images and a P@1 LaTeX table example.
    """
    log_dir = OUTPUT_DIR.parent
    log_dir.mkdir(parents=True, exist_ok=True)
    setup_logging({"level": "INFO", "log_file": log_dir / "vae_image_script.log"})
    logger.info("--- Starting Latent Reconstruction Script ---")

    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        logger.error("Config not found: %s", config_path)
        sys.exit(1)
    cfg = load_config(config_path, base_config_path=base_config_path if base_config_path.exists() else None)
    logger.info("Loaded config from %s", config_path.name)

    device = get_device(cfg.get("force_cpu", False))
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    logger.info("Using device: %s. Output directory: %s", device, OUTPUT_DIR)

    dataset_manager = DatasetManager(cfg)
    if not dataset_manager.load_full_dataset():
        logger.error("Failed to load dataset.")
        sys.exit(1)

    item_bs5 = find_first_item_for_subset(dataset_manager, "BS5")
    item_hw2 = find_first_item_for_subset(dataset_manager, "HW2")
    if not item_bs5 or not item_hw2:
        logger.error("Could not find required example items from BS5 and/or hw2.")
        sys.exit(1)
    items_to_process = {"BS5": item_bs5, "HW2": item_hw2}

    vae_model = load_paper_model("vae", cfg, device)
    ae_model = load_paper_model("ae", cfg, device)
    if not vae_model or not ae_model:
        logger.error("Failed to load VAE and/or AE models.")
        sys.exit(1)

    vae_model_params_from_cfg = cfg.get("definitions", {}).get("vae_model_def", {}).get("params", {})
    vae_trainer_params_from_cfg = cfg.get("definitions", {}).get("vae_trainer_def", {}).get("params", {})
    vae_frontend_params = vae_trainer_params_from_cfg.get("vae_frontend_params", {})

    if not vae_frontend_params:
        logger.error("VAE frontend params (vae_frontend_params) not found in config definitions (vae_trainer_def).")
        sys.exit(1)

    target_sr_vae = vae_frontend_params.get("target_sr", 16000)
    n_fft_vae = vae_frontend_params.get("n_fft", 512)
    hop_length_vae = vae_frontend_params.get("hop_length", 128)
    win_length_vae = vae_frontend_params.get("win_length", n_fft_vae)
    window_fn_str_vae = vae_frontend_params.get("window_fn_str", "hann_window")
    spec_height_vae = vae_frontend_params.get("spec_height", 128)
    spec_width_vae = vae_frontend_params.get("spec_width", 128)
    window_overlap_vae = vae_frontend_params.get("window_overlap", 0.75)

    vae_window_samples = (spec_width_vae - 1) * hop_length_vae
    vae_hop_samples = int(vae_window_samples * (1 - window_overlap_vae))

    ae_audio_config: AudioConfig = ae_model.config
    ae_n_fft = ae_audio_config.nfft
    ae_hop_length = ae_audio_config.hop_length
    ae_n_mels = ae_audio_config.n_mels
    ae_fmin = ae_audio_config.fmin
    ae_fmax = ae_audio_config.fmax
    ae_sr = ae_audio_config.sr

    for model_type, model_instance in [("VAE", vae_model), ("AE", ae_model)]:
        logger.info("\n--- Processing Model: %s ---", model_type)
        if model_instance is None:
            logger.warning("Model instance for %s is None. Skipping.", model_type)
            continue

        for subset_key, item_data in items_to_process.items():
            logger.info("-- Processing Example from Subset: %s --", subset_key)
            audio_array = item_data["audio"]["array"]
            sample_rate = item_data["audio"]["sampling_rate"]
            original_name = item_data.get("original_name", f"item_{subset_key}")

            original_mel_spec = None
            reconstructed_mel_spec = None
            latent_representation = None

            try:
                with torch.no_grad():
                    if model_type == "VAE":
                        vae_input_chunks = preprocess_vae_input(
                            audio_tensor=torch.from_numpy(audio_array.astype(np.float32)),
                            sample_rate=sample_rate,
                            device=device,
                            target_sr=target_sr_vae,
                            n_fft=n_fft_vae,
                            hop_length=hop_length_vae,
                            win_length=win_length_vae,
                            window_fn_str=window_fn_str_vae,
                            spec_height=spec_height_vae,
                            spec_width=spec_width_vae,
                            window_samples=vae_window_samples,
                            hop_samples=vae_hop_samples,
                        )
                        if not vae_input_chunks:
                            raise ValueError("VAE preprocessing yielded no chunks.")
                        vae_input_tensor = vae_input_chunks[0].to(device)
                        original_mel_spec = vae_input_tensor.squeeze(0).cpu().numpy()

                        mu, u, d_diag = model_instance.encode(vae_input_tensor.unsqueeze(0))
                        z = mu
                        reconstructed_output = model_instance.decode(z)
                        reconstructed_mel_spec = reconstructed_output.view(spec_height_vae, spec_width_vae).cpu().numpy()
                        latent_representation = mu.squeeze(0).cpu().numpy()

                        plot_sr = target_sr_vae
                        plot_hop_length = hop_length_vae
                        plot_n_fft = n_fft_vae
                        plot_fmin = None
                        plot_fmax = None

                    elif model_type == "AE":
                        audio_tensor = torch.from_numpy(audio_array.astype(np.float32)).unsqueeze(0).to(device)
                        if sample_rate != ae_sr:
                            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=ae_sr).to(device)
                            audio_tensor = resampler(audio_tensor)

                        max_val = torch.max(torch.abs(audio_tensor))
                        if max_val > 1e-6:
                            audio_tensor = audio_tensor / max_val

                        original_mel_spec_tensor = model_instance.frontend(audio_tensor)
                        original_mel_spec = original_mel_spec_tensor.squeeze(0).squeeze(0).cpu().numpy()

                        reconstructed_output, encoded_bottleneck = model_instance(audio_tensor)
                        reconstructed_mel_spec = reconstructed_output.squeeze(0).squeeze(0).cpu().numpy()

                        plot_sr = ae_sr
                        plot_hop_length = ae_hop_length
                        plot_n_fft = ae_n_fft
                        plot_fmin = ae_fmin
                        plot_fmax = ae_fmax

            except Exception as e:
                logger.error("Error processing %s for %s ('%s'): %s", model_type, subset_key, original_name, e, exc_info=True)
                continue

            safe_original_name = Path(original_name).stem.replace(" ", "_").replace("/", "_")
            base_filename = f"{model_type}_{subset_key}_{safe_original_name}"

            plot_spectrogram(
                original_mel_spec,
                f"Original Mel ({model_type} Input) - {subset_key}",
                OUTPUT_DIR / f"{base_filename}_original_mel.svg",
                sr=plot_sr,
                hop_length=plot_hop_length,
                is_mel=True,
                fmin=plot_fmin,
                fmax=plot_fmax,
                n_fft=plot_n_fft,
            )

            plot_spectrogram(
                reconstructed_mel_spec,
                f"Reconstructed Mel - {model_type} - {subset_key}",
                OUTPUT_DIR / f"{base_filename}_reconstructed_mel.svg",
                sr=plot_sr,
                hop_length=plot_hop_length,
                is_mel=True,
                fmin=plot_fmin,
                fmax=plot_fmax,
                n_fft=plot_n_fft,
            )

            if latent_representation is not None:
                plot_latent_vector(
                    latent_representation, f"{model_type} Latent/Bottleneck - {subset_key}", OUTPUT_DIR / f"{base_filename}_latent.svg"
                )

            del original_mel_spec, reconstructed_mel_spec, latent_representation
            if "audio_tensor" in locals():
                del audio_tensor
            if "vae_input_tensor" in locals():
                del vae_input_tensor
            if "vae_input_chunks" in locals():
                del vae_input_chunks
            if "original_mel_spec_tensor" in locals():
                del original_mel_spec_tensor
            if "reconstructed_output" in locals():
                del reconstructed_output
            if "encoded_bottleneck" in locals():
                del encoded_bottleneck
            if "mu" in locals():
                del mu
            if "u" in locals():
                del u
            if "d_diag" in locals():
                del d_diag
            if "z" in locals():
                del z
            gc.collect()

        del model_instance
        gc.collect()
        if device.type == "cuda":
            torch.cuda.empty_cache()

    logger.info("--- VAE/AE Image Generation Finished ---")

    logger.info("--- Generating P@1 LaTeX Table Example ---")
    example_p1_data = [
        {"Method": "MethodA", "Dist": "DistX", "BC": 10.1, "BS1": 12.3, "BS2": 13.4, "BS3": 14.5, "BS4": 15.6, "BS5": 16.7, "ES1": 17.8, "HP": 18.9, "HS1": 19.0, "HS2": 20.1, "HU1": 21.2, "HU2": 22.3, "HU3": 23.4, "HU4": 24.5, "HW1": 25.6, "HW2": 26.7, "HW3": 27.8, "HW4": 28.9, "OC1": 29.0, "Avg": 30.1, "Avg (Blind)": 31.2},
        {"Method": "MethodB", "Dist": "DistY", "BC": 11.1, "BS1": None, "BS2": "E", "BS3": 15.5, "BS4": 16.6, "BS5": 17.7, "ES1": 18.8, "HP": 19.9, "HS1": 20.0, "HS2": 21.1, "HU1": 22.2, "HU2": 23.3, "HU3": 24.4, "HU4": 25.5, "HW1": 26.6, "HW2": 27.7, "HW3": 28.8, "HW4": 29.9, "OC1": 30.0, "Avg": 31.1, "Avg (Blind)": 100.0},
        {"Method": "MethodC", "Dist": "DistZ", "BC": 5.1, "BS1": 3.3, "BS2": 4.4, "BS3": "-", "BS4": 6.6, "BS5": 7.7, "ES1": 8.8, "HP": 9.9, "HS1": 0.0, "HS2": 1.1, "HU1": 2.2, "HU2": 3.3, "HU3": 4.4, "HU4": 5.5, "HW1": 6.6, "HW2": 7.7, "HW3": 8.8, "HW4": 9.9, "OC1": 10.0, "Avg": 11.1, "Avg (Blind)": 12.2},
    ]

    logger.info("Attempting to generate table with table-format='2.1'. This will fail in LaTeX if scores like 100.0 exist with this format.")
    latex_table_2_1 = generate_p1_latex_table(
        example_p1_data,
        table_label="tab:p1_results_example_2_1",
        caption_text="Example P@1 Results (2.1 format)",
        table_format_str="2.1",
    )
    output_path_2_1 = OUTPUT_DIR / "example_p1_table_format_2_1.tex"
    try:
        with open(output_path_2_1, "w", encoding="utf-8") as f:
            f.write(latex_table_2_1)
        logger.info("Generated P@1 LaTeX table (2.1 format) to: %s", output_path_2_1)
    except Exception as e:
        logger.error("Failed to write 2.1 format table: %s", e)

    print("\n--- Example LaTeX P@1 Table (format: 2.1) ---")
    print(latex_table_2_1)

    logger.info("Generating table with table-format='3.1'. This is safer for scores up to 100.0.")
    latex_table_3_1 = generate_p1_latex_table(
        example_p1_data,
        table_label="tab:p1_results_example_3_1",
        caption_text="Example P@1 Results (3.1 format)",
        table_format_str="3.1",
    )
    output_path_3_1 = OUTPUT_DIR / "example_p1_table_format_3_1.tex"
    try:
        with open(output_path_3_1, "w", encoding="utf-8") as f:
            f.write(latex_table_3_1)
        logger.info("Generated P@1 LaTeX table (3.1 format) to: %s", output_path_3_1)
    except Exception as e:
        logger.error("Failed to write 3.1 format table: %s", e)

    print("\n--- Example LaTeX P@1 Table (format: 3.1) ---")
    print(latex_table_3_1)

    logger.info("--- Script Finished (including table generation example) ---")


if __name__ == "__main__":
    main()# -*- coding: utf-8 -*-
import argparse
import json
import logging
import re
from collections import OrderedDict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import yaml
from pandas import DataFrame, MultiIndex
import ast
import io
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
import librosa


logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

FEATURE_CONFIGS: Dict[str, Dict[str, Any]] = {}
FEATURE_SHORT_NAMES: Dict[str, str] = {}
PRETTY_NAMES: Dict[str, str] = {
    "P@1": "P@1",
    "P@5": "P@5",
    "pairwise_f_value": "CS",
    "pccf": "PCCF",
    "weighted_purity": "Purity (W)",
    "num_clusters_found": "Clusters",
    "csr_score": "CSR",
    "accuracy_mean": "Accuracy",
    "top_5_accuracy_mean": "Top-5 Acc.",
    "cosine": "C",
    "euclidean": "E",
    "spearman": "S",
    "all": "Overall",
    "avian_perception": "Avian Perc.",
    "mouse_strain": "Mouse Strain",
    "mouse_identity": "Mouse ID",
    "BS1": "BS1",
    "BS2": "BS2",
    "BS3": "BS3",
    "BS4": "BS4",
    "BS5": "BS5",
    "BC": "BC",
    "ES1": "ES1",
    "HP": "HP",
    "HS1": "HS1",
    "HS2": "HS2",
    "HU1": "HU1",
    "HU2": "HU2",
    "HU3": "HU3",
    "HU4": "HU4",
    "HW1": "HW1",
    "HW2": "HW2",
    "HW3": "HW3",
    "HW4": "HW4",
    "OC1": "OC1",
}

VOCSIM_APPENDIX_S_COLUMN_ORDER = [
    "BC",
    "BS1",
    "BS2",
    "BS3",
    "BS4",
    "BS5",
    "ES1",
    "HP",
    "HS1",
    "HS2",
    "HU1",
    "HU2",
    "HU3",
    "HU4",
    "HW1",
    "HW2",
    "HW3",
    "HW4",
    "OC1",
    "Avg",
    "Avg (Blind)",
]

BLIND_TEST_SUBSETS = ["HU3", "HU4", "HW3", "HW4"]
GOFFINET_FEATURES = OrderedDict(
    [
        ("Spectrogram D=10", "Spectrogram D=10*"),
        ("Spectrogram D=30", "Spectrogram D=30*"),
        ("Spectrogram D=100", "Spectrogram D=100*"),
        ("MUPET D=9", "MUPET D=9*"),
        ("DeepSqueak D=10", "DeepSqueak D=10*"),
        ("Latent D=7", "Latent D=7*"),
        ("Latent D=8", "Latent D=8*"),
    ]
)
GOFFINET_STRAIN_DATA = {
    "k-NN (k=3)": {"Spectrogram D=10": "68.1 (0.2)", "Spectrogram D=30": "76.4 (0.3)", "Spectrogram D=100": "82.3 (0.5)", "MUPET D=9": "86.1 (0.2)", "DeepSqueak D=10": "79.0 (0.3)", "Latent D=7": "89.8 (0.2)"},
    "k-NN (k=10)": {"Spectrogram D=10": "71.0 (0.3)", "Spectrogram D=30": "78.2 (0.1)", "Spectrogram D=100": "82.7 (0.6)", "MUPET D=9": "87.0 (0.1)", "DeepSqueak D=10": "80.7 (0.3)", "Latent D=7": "90.7 (0.4)"},
    "k-NN (k=30)": {"Spectrogram D=10": "72.8 (0.3)", "Spectrogram D=30": "78.5 (0.2)", "Spectrogram D=100": "81.3 (0.5)", "MUPET D=9": "86.8 (0.2)", "DeepSqueak D=10": "81.0 (0.2)", "Latent D=7": "90.3 (0.4)"},
    "RF (depth=10)": {"Spectrogram D=10": "72.8 (0.2)", "Spectrogram D=30": "76.6 (0.2)", "Spectrogram D=100": "79.1 (0.3)", "MUPET D=9": "87.4 (0.5)", "DeepSqueak D=10": "81.2 (0.4)", "Latent D=7": "88.1 (0.5)"},
    "RF (depth=15)": {"Spectrogram D=10": "73.1 (0.3)", "Spectrogram D=30": "78.0 (0.3)", "Spectrogram D=100": "80.5 (0.2)", "MUPET D=9": "87.9 (0.4)", "DeepSqueak D=10": "82.1 (0.3)", "Latent D=7": "89.6 (0.4)"},
    "RF (depth=20)": {"Spectrogram D=10": "73.2 (0.2)", "Spectrogram D=30": "78.3 (0.2)", "Spectrogram D=100": "80.7 (0.3)", "MUPET D=9": "87.9 (0.4)", "DeepSqueak D=10": "81.9 (0.3)", "Latent D=7": "89.6 (0.4)"},
    "MLP (α=0.1)": {"Spectrogram D=10": "72.4 (0.3)", "Spectrogram D=30": "79.1 (0.4)", "Spectrogram D=100": "84.5 (0.3)", "MUPET D=9": "87.8 (0.2)", "DeepSqueak D=10": "82.1 (0.4)", "Latent D=7": "90.1 (0.3)"},
    "MLP (α=0.01)": {"Spectrogram D=10": "72.3 (0.4)", "Spectrogram D=30": "78.6 (0.3)", "Spectrogram D=100": "82.9 (0.4)", "MUPET D=9": "88.1 (0.3)", "DeepSqueak D=10": "82.4 (0.4)", "Latent D=7": "90.0 (0.4)"},
    "MLP (α=0.001)": {"Spectrogram D=10": "72.4 (0.4)", "Spectrogram D=30": "78.5 (0.8)", "Spectrogram D=100": "82.8 (0.1)", "MUPET D=9": "87.9 (0.2)", "DeepSqueak D=10": "81.0 (0.2)", "Latent D=7": "90.4 (0.3)"},
}
GOFFINET_IDENTITY_DATA = {
    "Top-1 accuracy": {
        "MLP (α=0.01)": {"Spectrogram D=10": "9.9 (0.2)", "Spectrogram D=30": "14.9 (0.2)", "Spectrogram D=100": "20.4 (0.4)", "MUPET D=9": "14.7 (0.2)", "Latent D=8": "17.0 (0.3)"},
        "MLP (α=0.001)": {"Spectrogram D=10": "10.8 (0.1)", "Spectrogram D=30": "17.3 (0.4)", "Spectrogram D=100": "25.3 (0.3)", "MUPET D=9": "19.0 (0.3)", "Latent D=8": "22.7 (0.5)"},
        "MLP (α=0.0001)": {"Spectrogram D=10": "10.7 (0.2)", "Spectrogram D=30": "17.3 (0.3)", "Spectrogram D=100": "25.1 (0.3)", "MUPET D=9": "20.6 (0.4)", "Latent D=8": "24.0 (0.2)"},
    },
    "Top-5 accuracy": {
        "MLP (α=0.01)": {"Spectrogram D=10": "36.6 (0.4)", "Spectrogram D=30": "45.1 (0.5)", "Spectrogram D=100": "55.0 (0.3)", "MUPET D=9": "46.5 (0.3)", "Latent D=8": "49.9 (0.4)"},
        "MLP (α=0.001)": {"Spectrogram D=10": "38.6 (0.2)", "Spectrogram D=30": "50.7 (0.6)", "Spectrogram D=100": "62.9 (0.4)", "MUPET D=9": "54.0 (0.2)", "Latent D=8": "59.2 (0.6)"},
        "MLP (α=0.0001)": {"Spectrogram D=10": "38.7 (0.5)", "Spectrogram D=30": "50.8 (0.3)", "Spectrogram D=100": "63.2 (0.4)", "MUPET D=9": "57.3 (0.4)", "Latent D=8": "61.6 (0.4)"},
    },
}
ZANDBERG_RESULTS = {
    "EMB-LUA (Zandberg et al.)": 0.727,
    "Luscinia-U (Zandberg et al.)": 0.698,
    "Luscinia (Zandberg et al.)": 0.66,
    "SAP (Zandberg et al.)": 0.64,
    "Raven (Zandberg et al.)": 0.57,
}
CLASSIFIERS = OrderedDict([
    ("k-NN", OrderedDict([("k=3", {"type_match": "knn", "params_to_match": {"n_neighbors": 3}}), ("k=10", {"type_match": "knn", "params_to_match": {"n_neighbors": 10}}), ("k=30", {"type_match": "knn", "params_to_match": {"n_neighbors": 30}})])),
    ("RF", OrderedDict([("depth=10", {"type_match": "rf", "params_to_match": {"max_depth": 10, "class_weight": "balanced"}}), ("depth=15", {"type_match": "rf", "params_to_match": {"max_depth": 15, "class_weight": "balanced"}}), ("depth=20", {"type_match": "rf", "params_to_match": {"max_depth": 20, "class_weight": "balanced"}})])),
    ("MLP", OrderedDict([("α=0.1", {"type_match": "mlp", "params_to_match": {"alpha": 0.1}}), ("α=0.01", {"type_match": "mlp", "params_to_match": {"alpha": 0.01}}), ("α=0.001", {"type_match": "mlp", "params_to_match": {"alpha": 0.001}})])),
])
MLP_CONFIGS = OrderedDict([("MLP (α=0.01)", {"alpha": 0.01}), ("MLP (α=0.001)", {"alpha": 0.001}), ("MLP (α=0.0001)", {"alpha": 0.0001})])


def parse_value_for_comparison(value: Any) -> float:
    """
    Parses a string value potentially containing LaTeX bolding or parentheses
    into a float for numerical comparison.

    Args:
        value (Any): The input value, typically a string like "12.3 (0.1)".

    Returns:
        float: The parsed numerical value, or np.nan if parsing fails.
    """
    if pd.isna(value) or not isinstance(value, str) or value == "-":
        return np.nan
    cleaned = value.replace("\\textbf{", "").replace("}", "").replace("%", "").strip()
    match = re.match(r"^\s*(-?(?:\d+\.\d+|\d+))", cleaned)
    return float(match.group(1)) if match else np.nan


def bold_string(value: Any) -> str:
    """
    Wraps a string value in LaTeX bold command if it's not already bolded or NaN.

    Args:
        value (Any): The input value.

    Returns:
        str: The bolded LaTeX string, or the original string if NaN or already bolded.
    """
    str_value = str(value)
    if pd.isna(value) or str_value == "-":
        return str_value
    if str_value.startswith("\\textbf{") and str_value.endswith("}"):
        return str_value
    return f"\\textbf{{{str_value}}}"


def bold_best_in_columns(df: DataFrame, columns: List[str], higher_is_better: Dict[str, bool]) -> DataFrame:
    """
    Bolds the best values in specified columns of a DataFrame.

    Args:
        df (DataFrame): The input DataFrame.
        columns (List[str]): List of column names to process.
        higher_is_better (Dict[str, bool]): Dictionary mapping column names to boolean
                                           indicating if higher values are better.

    Returns:
        DataFrame: A new DataFrame with the best values in the specified columns bolded.
    """
    result = df.copy()
    for col in columns:
        if col not in result.columns:
            logger.debug("Column '%s' not found for bolding.", col)
            continue
        is_higher = higher_is_better.get(col, True)
        numeric_values = result[col].apply(parse_value_for_comparison)
        valid_numeric_values = numeric_values.dropna()
        if valid_numeric_values.empty:
            continue
        best_numeric = valid_numeric_values.max() if is_higher else valid_numeric_values.min()
        for idx in result.index:
            original_val = result.loc[idx, col]
            numeric_val = numeric_values.loc[idx]
            if pd.notna(numeric_val) and np.isclose(numeric_val, best_numeric):
                result.loc[idx, col] = bold_string(original_val)
    return result


def bold_overall_best_in_group_df(df: pd.DataFrame, columns_to_consider: List[Any], higher_is_better: bool, N_best: int = 1) -> pd.DataFrame:
    """
    Bolds the top N_best values across specified columns in a DataFrame.

    Args:
        df (pd.DataFrame): The input DataFrame.
        columns_to_consider (List[Any]): List of column identifiers (can be tuples for MultiIndex)
                                         to consider for finding the best values.
        higher_is_better (bool): True if higher values are better, False otherwise.
        N_best (int): Number of top values to bold.

    Returns:
        pd.DataFrame: A new DataFrame with the top values bolded.
    """
    df_out = df.copy()
    if not columns_to_consider:
        return df_out
    all_values_with_loc = []
    for r_idx in df_out.index:
        for c_idx_or_tuple in columns_to_consider:
            if c_idx_or_tuple not in df_out.columns:
                continue
            val_str = df_out.loc[r_idx, c_idx_or_tuple]
            numeric_val = parse_value_for_comparison(val_str)
            if pd.notna(numeric_val):
                all_values_with_loc.append({"val": numeric_val, "r_idx": r_idx, "c_idx": c_idx_or_tuple, "orig_str": val_str})
    if not all_values_with_loc:
        return df_out
    all_values_with_loc.sort(key=lambda x: x["val"], reverse=higher_is_better)
    if N_best <= 0 or not all_values_with_loc:
        return df_out
    actual_N_best_limit = min(N_best, len(all_values_with_loc))
    cutoff_score = all_values_with_loc[actual_N_best_limit - 1]["val"]
    for item in all_values_with_loc:
        is_close_to_cutoff = np.isclose(item["val"], cutoff_score, equal_nan=False)
        should_bold = (item["val"] > cutoff_score or is_close_to_cutoff) if higher_is_better else (item["val"] < cutoff_score or is_close_to_cutoff)
        if should_bold:
            df_out.loc[item["r_idx"], item["c_idx"]] = bold_string(item["orig_str"])
    return df_out


def load_config(config_path: Path) -> Optional[Dict[str, Any]]:
    """
    Loads the main configuration file and updates global feature/pretty name mappings.

    Args:
        config_path (Path): Path to the main configuration file.

    Returns:
        Optional[Dict[str, Any]]: The loaded configuration dictionary, or None if loading fails.
    """
    global FEATURE_CONFIGS, FEATURE_SHORT_NAMES, PRETTY_NAMES
    FEATURE_CONFIGS.clear()
    FEATURE_SHORT_NAMES.clear()
    PRETTY_NAMES = PRETTY_NAMES.copy()
    try:
        with config_path.open("r") as f:
            config = yaml.safe_load(f)
        extractors = config.get("feature_extractors", [])
        FEATURE_CONFIGS.update({fc["name"]: fc for fc in extractors if "name" in fc})
        FEATURE_SHORT_NAMES.update({name: cfg.get("short_name", name) for name, cfg in FEATURE_CONFIGS.items()})
        logger.info("Loaded %d feature short names from %s", len(FEATURE_SHORT_NAMES), config_path.name)
        PRETTY_NAMES.update(config.get("table_generator_pretty_names", {}))
        return config
    except Exception as e:
        logger.error("Error loading config %s: %s", config_path, e)
        return None


def get_display_name(name: str, entity_type: str = "feature") -> str:
    """
    Gets the display name for a feature or other entity, using short names and pretty names.

    Args:
        name (str): The raw name of the entity.
        entity_type (str): The type of entity ('feature', 'distance', 'metric', 'subset', 'characteristic').

    Returns:
        str: The display name.
    """
    if entity_type == "feature" and name in FEATURE_SHORT_NAMES:
        return FEATURE_SHORT_NAMES[name]
    return PRETTY_NAMES.get(name, name)


def format_number(value: Any, precision: int = 1, is_percentage: bool = False) -> str:
    """
    Formats a numerical value to a string with specified precision, handling percentages and NaN.

    Args:
        value (Any): The numerical value.
        precision (int): The number of decimal places.
        is_percentage (bool): If True, the input value (assumed 0-1) is multiplied by 100 before formatting.

    Returns:
        str: The formatted string ("-" for NaN/None).
    """
    if pd.isna(value) or value is None:
        return "-"
    if isinstance(value, (int, float, np.number)):
        val_to_format = value * 100 if is_percentage else value
        formatted_str = f"{val_to_format:.{precision}f}"
        return formatted_str
    return str(value)


def find_latest_results_json(directory: Path) -> Optional[Path]:
    """
    Finds the path to the latest results JSON file in a given directory.

    Args:
        directory (Path): The directory to search within.

    Returns:
        Optional[Path]: The path to the latest JSON file, or None if no file is found.
    """
    if not directory.is_dir():
        logger.debug("Directory not found: %s", directory)
        return None
    files = sorted(directory.glob("*_results.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    if files:
        logger.info("Found results JSON in '%s': %s", directory.name, files[0].name)
        return files[0]
    logger.warning("No '*_results.json' file found in %s", directory)
    return None


def parse_benchmark_params(benchmark_str: str) -> Tuple[str, Dict[str, Any]]:
    """
    Parses a benchmark string (e.g., "MLP(alpha=0.01, max_iter=500)") into type and parameters.

    Args:
        benchmark_str (str): The input benchmark string.

    Returns:
        Tuple[str, Dict[str, Any]]: A tuple containing the benchmark type and a dictionary of parameters.
    """
    match = re.match(r"(\w+)\((.*)\)", benchmark_str)
    if not match:
        return benchmark_str, {}
    clf_type, params_str = match.group(1), match.group(2)
    params = {}
    param_pattern = re.compile(r"(\w+)\s*=\s*('[^']*'|\"[^\"]*\"|\[.*?\]|\(.*?,\s*\)|\(.*?\)|None|True|False|[\w\.-]+(?:e[+-]?\d+)?)")
    for p_match in param_pattern.finditer(params_str):
        key, val_str = p_match.group(1), p_match.group(2).strip()
        try:
            val = ast.literal_eval(val_str)
        except (ValueError, SyntaxError, TypeError):
            if val_str.lower() == "none":
                val = None
            elif val_str.lower() == "true":
                val = True
            elif val_str.lower() == "false":
                val = False
            elif val_str in ("auto", "adam", "relu", "adaptive", "balanced"):
                val = val_str
            else:
                try:
                    val_f = float(val_str)
                    val = int(val_f) if val_f.is_integer() else val_f
                except ValueError:
                    val = val_str
        params[key] = val
    return clf_type, params


def load_results_json(json_path: Path, subset_name: str) -> Optional[DataFrame]:
    """
    Loads and parses benchmark results from a JSON file into a pandas DataFrame.

    Args:
        json_path (Path): Path to the JSON results file.
        subset_name (str): The name of the subset these results belong to.

    Returns:
        Optional[DataFrame]: DataFrame containing parsed results, or None if loading/parsing fails.
    """
    if not json_path.is_file():
        logger.error("JSON file not found: %s", json_path)
        return None
    try:
        with json_path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        records = []
        for feature, feature_data in data.items():
            if not isinstance(feature_data, dict):
                if feature == "error":
                    records.append({"subset": subset_name, "feature": "FeatureProcessingError", "metric_type": "error", "distance": "N/A", "benchmark": "N/A", "error_details": str(feature_data)})
                continue
            if "error" in feature_data:
                records.append({"subset": subset_name, "feature": feature, "metric_type": "error", "distance": "N/A", "benchmark": "FeatureProcessing", "error_details": feature_data["error"]})
                continue
            for metric_type, metric_data in feature_data.items():
                if metric_type == "error":
                    records.append({"subset": subset_name, "feature": feature, "metric_type": metric_type, "distance": "N/A", "benchmark": "N/A", "error_details": str(metric_data)})
                    continue
                if not isinstance(metric_data, dict):
                    records.append({"subset": subset_name, "feature": feature, "metric_type": metric_type, "distance": "N/A", "benchmark": "N/A", "error_details": f"Invalid metric data: {metric_data}"})
                    continue
                base = {"subset": subset_name, "feature": feature, "metric_type": metric_type}
                if metric_type == "distance_based":
                    for distance, benchmarks in metric_data.items():
                        if not isinstance(benchmarks, dict):
                            continue
                        for bench_name, results in benchmarks.items():
                            record = {**base, "distance": distance, "benchmark": bench_name}
                            if bench_name == "ClassificationBenchmark" and isinstance(results, dict):
                                for clf_config, scores in results.items():
                                    records.append({**base, "distance": distance, "benchmark": clf_config, **(scores if isinstance(scores, dict) else {"value": scores})})
                            else:
                                records.append({**record, **(results if isinstance(results, dict) else {"value": results})})
                elif metric_type == "feature_based":
                    for bench_name, results in metric_data.items():
                        record = {**base, "distance": "N/A", "benchmark": bench_name}
                        if bench_name == "ClassificationBenchmark" and isinstance(results, dict):
                            for clf_config, scores in results.items():
                                records.append({**base, "distance": "N/A", "benchmark": clf_config, **(scores if isinstance(scores, dict) else {"value": scores})})
                        else:
                            records.append({**record, **(results if isinstance(results, dict) else {"value": results})})
        if not records:
            logger.warning("No records parsed from %s", json_path.name)
            return DataFrame()
        df = DataFrame(records)
        for col in ["subset", "feature", "metric_type", "distance", "benchmark"]:
            df[col] = df.get(col, pd.NA)
        return df
    except Exception as e:
        logger.error("Error parsing JSON %s: %s", json_path, e, exc_info=True)
        return None


def get_ordered_features(benchmark_only: bool = True) -> List[str]:
    """
    Gets a list of feature names from the loaded config, optionally filtering by 'benchmark_this'.

    Args:
        benchmark_only (bool): If True, return only features with 'benchmark_this: true'.

    Returns:
        List[str]: Ordered list of feature names.
    """
    return [name for name, cfg in FEATURE_CONFIGS.items() if cfg.get("benchmark_this", True) == benchmark_only or not benchmark_only]


def generate_vocsim_main_table(df: DataFrame) -> Optional[DataFrame]:
    """
    Generates the main VocSim results table (overall average performance).

    This table shows average performance metrics (P@1, P@5, etc.) across all
    subsets, primarily using Cosine distance, for each feature.

    Args:
        df (DataFrame): DataFrame containing benchmark results from the 'all' subset.

    Returns:
        Optional[DataFrame]: DataFrame formatted for the main table, or None if input is empty.
    """
    if df.empty:
        return None
    logger.info("Generating VocSim Main Table (Overall Results - Cosine Distance)")
    metrics = OrderedDict([("P@1", ("PrecisionAtK", "P@1")), ("P@5", ("PrecisionAtK", "P@5")), ("CSCF", ("CSCFBenchmark", "pccf")), ("CS", ("FValueBenchmark", "pairwise_f_value")), ("CSR", ("ClassSeparationRatio", "csr_score")), ("Weighted Purity", ("ClusteringPurity", "weighted_purity"))])
    table_data = []
    for feature in df["feature"].unique():
        feature_df = df[df["feature"] == feature]
        row = {"Feature": get_display_name(feature)}
        scores = []
        for metric_name, (bench, col) in metrics.items():
            score = np.nan
            if bench == "ClusteringPurity":
                bench_df_fb = feature_df[(feature_df["benchmark"] == bench) & (feature_df["metric_type"] == "feature_based")]
                if not bench_df_fb.empty and col in bench_df_fb.columns and pd.notna(bench_df_fb[col].iloc[0]):
                    score = bench_df_fb[col].iloc[0]
                else:
                    bench_df_db = feature_df[(feature_df["benchmark"] == bench) & (feature_df["metric_type"] == "distance_based") & (feature_df["distance"].str.lower() == "cosine")]
                    if not bench_df_db.empty and col in bench_df_db.columns and pd.notna(bench_df_db[col].iloc[0]):
                        score = bench_df_db[col].iloc[0]
            else:
                bench_df = feature_df[(feature_df["benchmark"] == bench) & (feature_df["metric_type"] == "distance_based") & (feature_df["distance"].str.lower() == "cosine")]
                if not bench_df.empty and col in bench_df.columns and pd.notna(bench_df[col].iloc[0]):
                    score = bench_df[col].iloc[0]
            row[metric_name] = score
            if pd.notna(score):
                scores.append(score if metric_name in ["P@1", "P@5", "Weighted Purity", "CS"] else (score + 1) / 2 if metric_name == "CSR" else 1 - score)
        row["_sort_score"] = np.mean(scores) if scores else -np.inf
        table_data.append(row)
    if not table_data:
        return None
    result = DataFrame(table_data).sort_values("_sort_score", ascending=False).drop(columns="_sort_score").set_index("Feature")
    ordered_cols = [col for col in metrics if col in result.columns]
    result = result[ordered_cols]
    for col_name_iter in result.columns:
        if col_name_iter in ["P@1", "P@5", "Weighted Purity"]:
            result[col_name_iter] = result[col_name_iter].apply(lambda x: format_number(x, 1, True))
        elif col_name_iter == "CSR":
            result[col_name_iter] = result[col_name_iter].apply(lambda x: format_number(((x + 1) / 2) * 100 if pd.notna(x) else x, 1, False))
        elif col_name_iter in ["CS", "CSCF"]:
            result[col_name_iter] = result[col_name_iter].apply(lambda x: format_number(x, 1, True))
        else:
            result[col_name_iter] = result[col_name_iter].apply(lambda x: format_number(x, 2, False))
    higher_is_better_map = {"P@1": True, "P@5": True, "CSCF": False, "CS": True, "CSR": True, "Weighted Purity": True}
    return bold_best_in_columns(result, ordered_cols, higher_is_better_map)


def generate_full_results_table(
    df: DataFrame,
    metric_name_key: str,
    benchmark_name_prefix: str,
    metric_column_in_json: str,
    is_metric_percentage: bool,
    is_higher_better: bool,
    target_subset_columns: List[str],
) -> Optional[DataFrame]:
    """
    Generates a DataFrame for a specific metric across features, distances, and specified subsets.

    This function is designed to produce data suitable for the strict VocSim appendix longtable format.

    Args:
        df (DataFrame): The full combined DataFrame from loading multiple JSON results.
        metric_name_key (str): The display name of the metric (e.g., "P@1"). Used for logging.
        benchmark_name_prefix (str): The benchmark class name prefix (e.g., "PrecisionAtK").
        metric_column_in_json (str): The exact column name in the parsed DataFrame for this metric (e.g., "P@1", "pccf").
        is_metric_percentage (bool): True if the raw metric value (0-1) needs to be multiplied by 100 for display.
        is_higher_better (bool): True if higher values of this metric indicate better performance.
        target_subset_columns (List[str]): A list of the exact subset keys (e.g., "BC", "BS1")
                                          that should form the data columns of the table, in order.

    Returns:
        Optional[DataFrame]: DataFrame indexed by (Method, Dist) with columns corresponding
                             to `target_subset_columns` + 'Avg' + 'Avg (Blind)',
                             containing pre-formatted string values ready for LaTeX S-columns.
                             Returns None if no data is found.
    """
    if df.empty:
        return None
    logger.info("Generating data for table: %s (across all distances and specified subsets)", metric_name_key)

    rows_data = []
    is_clustering_metric = benchmark_name_prefix == "ClusteringPurity"
    all_features_in_df = df["feature"].unique()
    distances_to_iterate = ["cosine", "euclidean", "spearman"]

    for feature_name_iter in all_features_in_df:
        df_feature_specific = df[df["feature"] == feature_name_iter]

        if is_clustering_metric:
            current_row_values = {"Method": get_display_name(feature_name_iter), "Dist": "-"}
            subset_scores_list = []
            blind_subset_scores_list = []
            has_any_data_for_row = False

            for vocsim_subset_key in target_subset_columns:
                df_vocsim_subset_specific = df_feature_specific[df_feature_specific["subset"] == vocsim_subset_key]
                score_val = np.nan
                if not df_vocsim_subset_specific.empty and metric_column_in_json in df_vocsim_subset_specific.columns:
                    val = df_vocsim_subset_specific[metric_column_in_json].iloc[0]
                    if pd.notna(val):
                        score_val = float(val)
                        has_any_data_for_row = True

                current_row_values[vocsim_subset_key] = score_val
                if pd.notna(score_val):
                    subset_scores_list.append(score_val)
                    if vocsim_subset_key in BLIND_TEST_SUBSETS:
                        blind_subset_scores_list.append(score_val)

            avg_score = np.mean(subset_scores_list) if subset_scores_list else np.nan
            avg_blind_score = np.mean(blind_subset_scores_list) if blind_subset_scores_list else np.nan
            current_row_values["Avg"] = avg_score
            current_row_values["Avg (Blind)"] = avg_blind_score

            sort_val = avg_blind_score if pd.notna(avg_blind_score) else avg_score if pd.notna(avg_score) else (-np.inf if is_higher_better else np.inf)
            current_row_values["_sort_score"] = sort_val * (1 if is_higher_better else -1)

            if has_any_data_for_row:
                rows_data.append(current_row_values)

        for dist_name in distances_to_iterate:
            current_row_values = {"Method": get_display_name(feature_name_iter), "Dist": get_display_name(dist_name, "distance")}
            subset_scores_list = []
            blind_subset_scores_list = []
            has_any_data_for_row = False

            for vocsim_subset_key in target_subset_columns:
                df_metric_specific = df_feature_specific[
                    (df_feature_specific["subset"] == vocsim_subset_key)
                    & (df_feature_specific["metric_type"] == "distance_based")
                    & (df_feature_specific["distance"].str.lower() == dist_name.lower())
                    & (df_feature_specific["benchmark"] == benchmark_name_prefix)
                ]
                score_val = np.nan
                if not df_metric_specific.empty and metric_column_in_json in df_metric_specific.columns:
                    val = df_metric_specific[metric_column_in_json].iloc[0]
                    if pd.notna(val):
                        score_val = float(val)
                        has_any_data_for_row = True

                current_row_values[vocsim_subset_key] = score_val
                if pd.notna(score_val):
                    subset_scores_list.append(score_val)
                    if vocsim_subset_key in BLIND_TEST_SUBSETS:
                        blind_subset_scores_list.append(score_val)

            avg_score = np.mean(subset_scores_list) if subset_scores_list else np.nan
            avg_blind_score = np.mean(blind_subset_scores_list) if blind_subset_scores_list else np.nan
            current_row_values["Avg"] = avg_score
            current_row_values["Avg (Blind)"] = avg_blind_score

            sort_val = avg_blind_score if pd.notna(avg_blind_score) else avg_score if pd.notna(avg_score) else (-np.inf if is_higher_better else np.inf)
            current_row_values["_sort_score"] = sort_val * (1 if is_higher_better else -1)

            if has_any_data_for_row:
                rows_data.append(current_row_values)

    if not rows_data:
        return None

    result_df = DataFrame(rows_data)
    if "_sort_score" not in result_df.columns:
        result_df["_sort_score"] = -np.inf if is_higher_better else np.inf

    result_df = result_df.sort_values(["_sort_score", "Method", "Dist"], ascending=[False, True, True])
    result_df = result_df.drop(columns="_sort_score")
    result_df = result_df.set_index(["Method", "Dist"])

    final_column_order = target_subset_columns + ["Avg", "Avg (Blind)"]
    result_df = result_df.reindex(columns=final_column_order)

    precision_for_format = 1
    if metric_name_key == "CSCF":
        precision_for_format = 2

    numeric_df_for_bolding = result_df.copy()
    if metric_name_key == "CSR":
        numeric_df_for_bolding = numeric_df_for_bolding.applymap(lambda x: ((x + 1) / 2) * 100 if pd.notna(x) else np.nan)
    elif is_metric_percentage:
        numeric_df_for_bolding = numeric_df_for_bolding.applymap(lambda x: x * 100 if pd.notna(x) else np.nan)

    for col_name_iter in result_df.columns:
        if metric_name_key == "CSR":
            result_df[col_name_iter] = result_df[col_name_iter].apply(lambda x: format_number(((x + 1) / 2) * 100 if pd.notna(x) else np.nan, precision_for_format, False))
        else:
            result_df[col_name_iter] = result_df[col_name_iter].apply(lambda x: format_number(x, precision_for_format, is_metric_percentage))

    for col in final_column_order:
        if col not in numeric_df_for_bolding.columns:
            continue

        current_col_numeric_values = numeric_df_for_bolding[col]
        valid_numeric_vals = current_col_numeric_values.dropna()
        if valid_numeric_vals.empty:
            continue

        best_numeric_val = valid_numeric_vals.max() if is_higher_better else valid_numeric_vals.min()

        for idx in result_df.index:
            original_str_val = result_df.loc[idx, col]
            numeric_comp_val = numeric_df_for_bolding.loc[idx, col]

            if pd.notna(numeric_comp_val) and np.isclose(numeric_comp_val, best_numeric_val):
                result_df.loc[idx, col] = bold_string(original_str_val)

    return result_df


def generate_avian_perception_table(df: DataFrame) -> Optional[DataFrame]:
    """
    Generates the Avian Perception table (Triplet Accuracy High).

    This table shows Triplet Accuracy (High Consistency) for each feature-distance
    combination and includes reference values from Zandberg et al. (2024).

    Args:
        df (DataFrame): DataFrame containing benchmark results for the 'avian_perception' subset.

    Returns:
        Optional[DataFrame]: DataFrame formatted for the Avian Perception table, or None if input is empty.
    """
    if df.empty:
        return None
    logger.info("Generating Avian Perception Table (Triplet Acc. High, All Distances)")
    data = []
    features = get_ordered_features() or df["feature"].unique()
    distances = ["cosine", "euclidean", "spearman"]
    for feature_name_iter in features:
        feature_df = df[df["feature"] == feature_name_iter]
        for dist_name_iter in distances:
            dist_df = feature_df[feature_df["distance"].str.lower() == dist_name_iter.lower()]
            if dist_df.empty:
                logger.debug("No data for '%s' with '%s' distance.", feature_name_iter, dist_name_iter)
                continue
            score_val = np.nan
            if "triplet_high_accuracy" in dist_df.columns:
                score_val = dist_df["triplet_high_accuracy"].iloc[0] if not dist_df.empty else np.nan
            else:
                logger.warning("'triplet_high_accuracy' column not found for feature '%s', dist '%s'.", feature_name_iter, dist_name_iter)

            if pd.notna(score_val):
                method_name = f"{get_display_name(feature_name_iter)} ({get_display_name(dist_name_iter, 'distance')})"
                data.append({"Method": method_name, "Triplet Acc. (High)": score_val})
    data.extend([{"Method": method_name_iter, "Triplet Acc. (High)": score_iter} for method_name_iter, score_iter in ZANDBERG_RESULTS.items()])
    if not data:
        return None
    result_df = DataFrame(data).sort_values("Triplet Acc. (High)", ascending=False).set_index("Method")
    result_df["Triplet Acc. (High)"] = result_df["Triplet Acc. (High)"].apply(lambda x: format_number(x, 1, True))
    return bold_best_in_columns(result_df, ["Triplet Acc. (High)"], {"Triplet Acc. (High)": True})


def _compare_params(parsed_val, target_val, atol=1e-6) -> bool:
    """
    Compares parsed parameters from benchmark string against target values.

    Args:
        parsed_val (Any): The value parsed from the benchmark string.
        target_val (Any): The target value from the configuration/definitions.
        atol (float): Absolute tolerance for float comparisons.

    Returns:
        bool: True if the values match (considering float tolerance and types), False otherwise.
    """
    if target_val is None:
        return parsed_val is None
    if parsed_val is None:
        return False
    if isinstance(target_val, float):
        return isinstance(parsed_val, (int, float)) and np.isclose(float(parsed_val), target_val, atol=atol)
    if isinstance(target_val, int):
        if isinstance(parsed_val, float):
            return parsed_val.is_integer() and int(parsed_val) == target_val
        return isinstance(parsed_val, int) and parsed_val == target_val
    if isinstance(target_val, (list, tuple)):
        if not isinstance(parsed_val, (list, tuple)) or len(target_val) != len(parsed_val):
            return False
        try:
            target_comp = [float(x) if isinstance(x, str) and x.replace(".", "", 1).lstrip("-").replace(".", "", 1).isdigit() else x for x in target_val]
            parsed_comp = [float(x) if isinstance(x, str) and x.replace(".", "", 1).lstrip("-").replace(".", "", 1).isdigit() else x for x in parsed_val]
            for p, t in zip(parsed_comp, target_comp):
                if isinstance(p, float) and isinstance(t, float):
                    if not np.isclose(p, t, atol=atol):
                        return False
                elif type(p) != type(t):
                    if isinstance(p, (int, float)) and isinstance(t, (int, float)):
                        if not np.isclose(float(p), float(t), atol=atol):
                            return False
                    else:
                        return False
                elif p != t:
                    return False
            return True
        except (ValueError, TypeError):
            return list(parsed_val) == list(target_val)
    return parsed_val == target_val


def format_float(x, precision=1, is_percentage=False):
    """
    Formats a float number to a string with a specific precision, handling NaN and percentages.

    Args:
        x: The float number.
        precision (int): Number of decimal places.
        is_percentage (bool): If True, multiplies by 100.

    Returns:
        str: The formatted string.
    """
    if pd.isna(x) or x is None:
        return "-"
    if isinstance(x, (int, float, np.number)):
        val = x * 100 if is_percentage else x
        return f"{val:.{precision}f}"
    return str(x)


def get_pretty_name(name: str, entity_type: str = "feature") -> str:
    """
    Gets the pretty display name for an entity type.

    Args:
        name (str): The raw name.
        entity_type (str): The entity type ('feature', 'distance', 'metric', 'subset').

    Returns:
        str: The pretty name.
    """
    if entity_type == "feature" and name in FEATURE_SHORT_NAMES:
        return FEATURE_SHORT_NAMES[name]
    return PRETTY_NAMES.get(name, name)


def generate_mouse_strain_table(
    df_strain_data: pd.DataFrame,
    your_target_feature_full_names: List[str],
    goffinet_target_feature_map: OrderedDict,
    classifier_column_config_map: OrderedDict,
    metric_name: str = "accuracy_mean",
    std_dev_name: str = "accuracy_std",
) -> Optional[pd.DataFrame]:
    """
    Generates the Mouse Strain classification accuracy table, combining own results with Goffinet et al. (2021).

    Args:
        df_strain_data (pd.DataFrame): DataFrame containing benchmark results for the 'mouse_strain' subset.
        your_target_feature_full_names (List[str]): List of full feature names from your results to include as rows.
        goffinet_target_feature_map (OrderedDict): Mapping from Goffinet et al. feature names to display names.
        classifier_column_config_map (OrderedDict): Configuration for classifier columns (groups and specific configs).
        metric_name (str): The name of the mean metric column (e.g., "accuracy_mean").
        std_dev_name (str): The name of the std dev metric column (e.g., "accuracy_std").

    Returns:
        Optional[pd.DataFrame]: DataFrame formatted for the Mouse Strain table, or None if no data is found.
    """
    logger.info("Generating Mouse Strain table with FEATURES AS ROWS, classifiers as columns.")
    goffinet_strain_raw = GOFFINET_STRAIN_DATA
    all_row_display_names = [get_pretty_name(f_full_name, "feature") for f_full_name in your_target_feature_full_names] if your_target_feature_full_names else []
    if goffinet_target_feature_map:
        for goffinet_display_name in goffinet_target_feature_map.values():
            if goffinet_display_name not in all_row_display_names:
                all_row_display_names.append(goffinet_display_name)
    if not all_row_display_names:
        logger.warning("No features defined for rows in strain table.")
        return None
    column_tuples = [(clf_group_name, specific_config_disp_name) for clf_group_name, specific_configs_map in classifier_column_config_map.items() for specific_config_disp_name in specific_configs_map.keys()]
    if not column_tuples:
        logger.warning("No classifier columns defined for strain table.")
        return None
    result_df = pd.DataFrame(index=all_row_display_names, columns=pd.MultiIndex.from_tuples(column_tuples))
    result_df.index.name = "Method"

    if df_strain_data is not None and not df_strain_data.empty and your_target_feature_full_names:
        for your_f_full_name in your_target_feature_full_names:
            your_f_display_name = get_pretty_name(your_f_full_name, "feature")
            if your_f_display_name not in result_df.index:
                continue
            df_feature_specific = df_strain_data[df_strain_data["feature"] == your_f_full_name]
            if df_feature_specific.empty:
                continue
            for clf_group_name, specific_configs_map in classifier_column_config_map.items():
                for specific_config_disp_name, clf_match_details in specific_configs_map.items():
                    target_clf_type, target_clf_params_to_match = clf_match_details["type_match"], clf_match_details["params_to_match"]
                    is_mlp_alpha_only_match = target_clf_type == "mlp" and "alpha" in target_clf_params_to_match and len(target_clf_params_to_match) == 1
                    candidate_runs_for_cell = []
                    for _, r_series in df_feature_specific.iterrows():
                        bench_str = r_series.get("benchmark", "")
                        parsed_type, parsed_params = parse_benchmark_params(bench_str)
                        if parsed_type == target_clf_type:
                            param_match = True
                            for p_key, p_val_target in target_clf_params_to_match.items():
                                if is_mlp_alpha_only_match and p_key == "hidden_layer_sizes":
                                    pass
                                elif not _compare_params(parsed_params.get(p_key), p_val_target):
                                    param_match = False
                                    break
                            if param_match:
                                candidate_runs_for_cell.append(r_series)
                    best_run_for_cell = None
                    if candidate_runs_for_cell:
                        if is_mlp_alpha_only_match and len(candidate_runs_for_cell) > 0:
                            best_score = -np.inf
                            for run in candidate_runs_for_cell:
                                score = run.get(metric_name, -np.inf)
                                if pd.notna(score) and score > best_score:
                                    best_score = score
                                    best_run_for_cell = run
                            if best_run_for_cell is None:
                                best_run_for_cell = candidate_runs_for_cell[0]
                        elif candidate_runs_for_cell:
                            best_run_for_cell = candidate_runs_for_cell[0]
                    if best_run_for_cell is not None:
                        mean_val_prop, std_val_prop = best_run_for_cell.get(metric_name, np.nan), best_run_for_cell.get(std_dev_name, np.nan)
                        mean_display = mean_val_prop if pd.notna(mean_val_prop) else np.nan
                        std_display = std_val_prop if pd.notna(std_val_prop) else np.nan
                        formatted_val = f"{format_float(mean_display, 1, False)} ({format_float(std_display, 1, False)})" if pd.notna(mean_display) and pd.notna(std_display) else ("-" if pd.isna(mean_display) else format_float(mean_display, 1, False))
                        result_df.loc[your_f_display_name, (clf_group_name, specific_config_disp_name)] = formatted_val
                    else:
                        result_df.loc[your_f_display_name, (clf_group_name, specific_config_disp_name)] = "-"
    if goffinet_target_feature_map:
        for goffinet_paper_feat_key, goffinet_display_name_row in goffinet_target_feature_map.items():
            if goffinet_display_name_row not in result_df.index:
                continue
            for clf_group_name, specific_configs_map in classifier_column_config_map.items():
                for specific_config_disp_name in specific_configs_map.keys():
                    goffinet_classifier_key = f"{clf_group_name} ({specific_config_disp_name})"
                    score_str = goffinet_strain_raw.get(goffinet_classifier_key, {}).get(goffinet_paper_feat_key, "-")
                    result_df.loc[goffinet_display_name_row, (clf_group_name, specific_config_disp_name)] = score_str
    result_df = result_df.fillna("-")
    all_data_cols_for_bouding = result_df.columns.tolist()
    if all_data_cols_for_bouding:
        result_df = bold_overall_best_in_group_df(result_df, all_data_cols_for_bouding, higher_is_better=True, N_best=1)
    return result_df


def generate_mouse_identity_table(df_mouse_identity_full: pd.DataFrame, target_feature_full_names_ours: List[str], mlp_alpha_configs_map: OrderedDict, goffinet_feature_display_map: OrderedDict) -> Optional[pd.DataFrame]:
    """
    Generates the Mouse Identity classification accuracy table, combining own results with Goffinet et al. (2021).

    Args:
        df_mouse_identity_full (pd.DataFrame): DataFrame containing benchmark results for the 'mouse_identity' subset.
        target_feature_full_names_ours (List[str]): List of full feature names from your results to include as rows.
        mlp_alpha_configs_map (OrderedDict): Configuration for MLP alpha columns.
        goffinet_feature_display_map (OrderedDict): Mapping from Goffinet et al. feature names to display names.

    Returns:
        Optional[pd.DataFrame]: DataFrame formatted for the Mouse Identity table, or None if no data is found.
    """
    if (df_mouse_identity_full is None or df_mouse_identity_full.empty) and not goffinet_feature_display_map:
        logger.warning("No own data and no Goffinet features for mouse identity table.")
        return None
    logger.info("Generating Mouse Identity Classification Table (Features as Rows, including Goffinet).")
    goffinet_identity_raw = GOFFINET_IDENTITY_DATA
    all_feature_rows_display_names = [get_pretty_name(f_full_name, "feature") for f_full_name in target_feature_full_names_ours] if target_feature_full_names_ours else []
    if goffinet_feature_display_map:
        for goffinet_feat_display_starred in goffinet_feature_display_map.values():
            if goffinet_feat_display_starred not in all_feature_rows_display_names:
                all_feature_rows_display_names.append(goffinet_feat_display_starred)
    if not all_feature_rows_display_names:
        logger.warning("No features (neither own nor Goffinet) to display for mouse identity table.")
        return None
    metric_types_display = ["Top-1 accuracy", "Top-5 accuracy"]
    column_tuples = [(mlp_disp_name, metric_type_disp) for mlp_disp_name in mlp_alpha_configs_map.keys() for metric_type_disp in metric_types_display]
    combined_df = pd.DataFrame(index=all_feature_rows_display_names, columns=pd.MultiIndex.from_tuples(column_tuples))
    combined_df.index.name = "Feature Set"

    if df_mouse_identity_full is not None and not df_mouse_identity_full.empty and target_feature_full_names_ours:
        for f_full_name_ours in target_feature_full_names_ours:
            f_display_name_ours = get_pretty_name(f_full_name_ours, "feature")
            if f_display_name_ours not in combined_df.index:
                continue
            df_feature_specific = df_mouse_identity_full[df_mouse_identity_full["feature"] == f_full_name_ours]
            if df_feature_specific.empty:
                continue
            for mlp_disp_name_col_group, mlp_params_to_match in mlp_alpha_configs_map.items():
                candidate_rows = []
                for _, r_series in df_feature_specific.iterrows():
                    bench_str = r_series.get("benchmark", "")
                    parsed_type, parsed_params = parse_benchmark_params(bench_str)
                    if parsed_type == "mlp":
                        param_match = all(_compare_params(parsed_params.get(p_key), p_val_target) for p_key, p_val_target in mlp_params_to_match.items())
                        if param_match:
                            candidate_rows.append(r_series)
                best_mlp_run_for_alpha = None
                if candidate_rows:
                    best_score_top1 = -np.inf
                    for cand_row in candidate_rows:
                        score_prop = cand_row.get("accuracy_mean", -np.inf)
                        if pd.notna(score_prop) and score_prop > best_score_top1:
                            best_score_top1 = score_prop
                            best_mlp_run_for_alpha = cand_row
                    if best_mlp_run_for_alpha is None and candidate_rows:
                        best_mlp_run_for_alpha = candidate_rows[0]
                if best_mlp_run_for_alpha is not None:
                    top1_mean_disp = best_mlp_run_for_alpha.get("accuracy_mean", np.nan)
                    top1_std_disp = best_mlp_run_for_alpha.get("accuracy_std", np.nan)
                    top5_mean_disp = best_mlp_run_for_alpha.get("top_5_accuracy_mean", np.nan)
                    top5_std_disp = best_mlp_run_for_alpha.get("top_5_accuracy_std", np.nan)
                    val_top1 = f"{format_float(top1_mean_disp, 1, False)} ({format_float(top1_std_disp, 1, False)})" if pd.notna(top1_mean_disp) and pd.notna(top1_std_disp) else ("-" if pd.isna(top1_mean_disp) else format_float(top1_mean_disp, 1, False))
                    val_top5 = f"{format_float(top5_mean_disp, 1, False)} ({format_float(top5_std_disp, 1, False)})" if pd.notna(top5_mean_disp) and pd.notna(top5_std_disp) else ("-" if pd.isna(top5_mean_disp) else format_float(top5_mean_disp, 1, False))
                    combined_df.loc[f_display_name_ours, (mlp_disp_name_col_group, "Top-1 accuracy")] = val_top1
                    combined_df.loc[f_display_name_ours, (mlp_disp_name_col_group, "Top-5 accuracy")] = val_top5
                else:
                    combined_df.loc[f_display_name_ours, (mlp_disp_name_col_group, "Top-1 accuracy")] = "-"
                    combined_df.loc[f_display_name_ours, (mlp_disp_name_col_group, "Top-5 accuracy")] = "-"
    if goffinet_feature_display_map:
        for goffinet_feat_paper_name_key, goffinet_feat_display_starred in goffinet_feature_display_map.items():
            if goffinet_feat_display_starred not in combined_df.index:
                continue
            for mlp_alpha_disp_name_col_group in mlp_alpha_configs_map.keys():
                score_str_top1 = goffinet_identity_raw["Top-1 accuracy"].get(mlp_alpha_disp_name_col_group, {}).get(goffinet_feat_paper_name_key, "-")
                combined_df.loc[goffinet_feat_display_starred, (mlp_alpha_disp_name_col_group, "Top-1 accuracy")] = score_str_top1
                score_str_top5 = goffinet_identity_raw["Top-5 accuracy"].get(mlp_alpha_disp_name_col_group, {}).get(goffinet_feat_paper_name_key, "-")
                combined_df.loc[goffinet_feat_display_starred, (mlp_alpha_disp_name_col_group, "Top-5 accuracy")] = score_str_top5
    combined_df = combined_df.fillna("-")
    top1_cols_to_consider = [(mlp_disp, "Top-1 accuracy") for mlp_disp in mlp_alpha_configs_map.keys() if (mlp_disp, "Top-1 accuracy") in combined_df.columns]
    if top1_cols_to_consider:
        combined_df = bold_overall_best_in_group_df(combined_df, top1_cols_to_consider, higher_is_better=True, N_best=1)
    top5_cols_to_consider = [(mlp_disp, "Top-5 accuracy") for mlp_disp in mlp_alpha_configs_map.keys() if (mlp_disp, "Top-5 accuracy") in combined_df.columns]
    if top5_cols_to_consider:
        combined_df = bold_overall_best_in_group_df(combined_df, top5_cols_to_consider, higher_is_better=True, N_best=1)
    return combined_df


def sanitize(text: Any) -> str:
    """
    Sanitizes text for LaTeX output, avoiding changes to likely LaTeX commands.

    Args:
        text (Any): Input text.

    Returns:
        str: Sanitized string.
    """
    if pd.isna(text):
        return "-"
    s_text = str(text)
    if any(cmd in s_text for cmd in ["\\textbf{", "\\makecell{", "\\multicolumn{", "\\textit{", "\\emph{", "\\texttt{"]):
        return s_text
    s_text = s_text.replace("&", r"\&").replace("%", r"\%").replace("$", r"\$")
    s_text = s_text.replace("#", r"\#").replace("_", r"\_").replace("{", r"\{")
    s_text = s_text.replace("}", r"\}").replace("~", r"\textasciitilde{}")
    s_text = s_text.replace("^", r"\^{}").replace("*", r"$^*$")
    s_text = s_text.replace("<", r"\textless{}").replace(">", r"\textgreater{}")
    if "\\" in s_text and not any(cmd in s_text for cmd in ["\\makecell", "\\textbf", "\\textit", "\\emph", "\\texttt", "\\&", "\\%", "\\$", "\\#", "\\_", "\\{", "\\}", "\\~", "\\^", "\\*", "\\textbackslash"]):
        s_text = s_text.replace("\\", r"\textbackslash{}")
    return s_text


def generate_vocsim_appendix_longtable_latex(
    df_data: pd.DataFrame, caption_text: str, table_label: str, output_file: Path
):
    """
    Generates and writes a VocSim appendix longtable using the STRICT format.

    Args:
        df_data (pd.DataFrame): DataFrame indexed by ('Method', 'Dist') with columns
                                matching VOCSIM_APPENDIX_S_COLUMN_ORDER, containing
                                pre-formatted string values.
        caption_text (str): Caption for the table.
        table_label (str): LaTeX label for the table.
        output_file (Path): Path to save the .tex file.
    """
    if df_data.empty:
        logger.warning("No data for VocSim Appendix table '%s'. Skipping file write.", caption_text)
        return

    num_data_columns_in_df = len(df_data.columns)
    expected_num_s_columns = 21

    if num_data_columns_in_df != expected_num_s_columns:
        logger.error(
            "FATAL: VocSim appendix table '%s' generator expects exactly %d data columns (as defined in VOCSIM_APPENDIX_S_COLUMN_ORDER),"
            " but found %d in the provided DataFrame. Columns found: %s. Expected: %s. Halting generation for this table.",
            caption_text,
            expected_num_s_columns,
            num_data_columns_in_df,
            list(df_data.columns),
            VOCSIM_APPENDIX_S_COLUMN_ORDER,
        )
        logger.error("Please ensure `generate_full_results_table` produces these columns in the correct order.")
        return

    latex_lines = []
    latex_lines.append("% Add to your LaTeX preamble: \\usepackage{longtable, booktabs, siunitx, makecell}")
    latex_lines.append("% WARNING: This table uses S[table-format=2.1]. Values like 100.0 or -10.0 will cause siunitx errors.")
    latex_lines.append("\\scriptsize")
    latex_lines.append("\\setlength{\\tabcolsep}{2pt}")

    s_caption = sanitize(caption_text)
    s_label = sanitize(table_label)

    latex_lines.append(f"\\begin{{longtable}}{{l l *{{{expected_num_s_columns}}}{{S[table-format=2.1]}}}}")
    latex_lines.append(f"\\caption{{{s_caption}}}\\label{{{s_label}}}\\\\")
    latex_lines.append("\\toprule")

    header_line_parts = ["Method", "Dist"]
    for s_col_name in VOCSIM_APPENDIX_S_COLUMN_ORDER:
        display_col_name = get_display_name(s_col_name, "subset")
        if display_col_name == "Avg (Blind)":
            header_line_parts.append("\\multicolumn{1}{c}{\\makecell{Avg\\\\(Blind)}}")
        else:
            header_line_parts.append(f"\\multicolumn{{1}}{{c}}{{{sanitize(display_col_name)}}}")

    header_full_line = " & ".join(header_line_parts) + " \\\\"

    latex_lines.append(header_full_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endfirsthead")
    latex_lines.append("")
    latex_lines.append(f"\\caption[]{{(Continued) {s_caption}}}\\\\")
    latex_lines.append("\\toprule")
    latex_lines.append(header_full_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endhead")
    latex_lines.append("")

    total_table_cols_inc_index = 2 + expected_num_s_columns
    latex_lines.append("\\midrule")
    latex_lines.append(f"\\multicolumn{{{total_table_cols_inc_index}}}{{r}}{{\\textit{{Continued on next page}}}}\\\\")
    latex_lines.append("\\endfoot")
    latex_lines.append("")
    latex_lines.append("\\bottomrule")
    latex_lines.append("\\endlastfoot")
    latex_lines.append("")

    for (method_val_idx, dist_val_idx), row_series_data in df_data.iterrows():
        row_str_parts = [sanitize(str(method_val_idx)), sanitize(str(dist_val_idx))]
        for cell_val_str in row_series_data:
            row_str_parts.append(str(cell_val_str))
        latex_lines.append(" & ".join(row_str_parts) + " \\\\")

    latex_lines.append("\\end{longtable}")

    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with output_file.open("w", encoding="utf-8") as f:
            f.write("\n".join(latex_lines))
        logger.info("VocSim Appendix LaTeX table '%s' saved to %s", s_caption, output_file)
    except Exception as e:
        logger.error("Error saving VocSim Appendix LaTeX table %s: %s", output_file, e)


def output_latex_table(
    df: DataFrame,
    caption: str,
    label: str,
    output_file: Path,
    column_format: Optional[str] = None,
    notes: Optional[List[str]] = None,
    is_longtable: bool = False,
) -> None:
    """
    Generates and writes a LaTeX table (standard or longtable) from a DataFrame.

    Args:
        df (DataFrame): The input DataFrame.
        caption (str): The caption for the table.
        label (str): The LaTeX label for the table.
        output_file (Path): Path to save the .tex file.
        column_format (Optional[str]): LaTeX column format string. Defaults to pandas inference.
        notes (Optional[List[str]]): List of strings to include as table notes.
        is_longtable (bool): If True, generates a longtable environment. Otherwise, a standard table.
    """
    if df.empty:
        logger.warning("No data for LaTeX table '%s'.", caption)
        return

    df_copy = df.copy()

    if df_copy.index.name or isinstance(df_copy.index, MultiIndex):
        sanitized_index_names = [sanitize(name) for name in df_copy.index.names]
        if isinstance(df_copy.index, MultiIndex):
            sanitized_index_tuples = [tuple(sanitize(level) for level in idx_tuple) for idx_tuple in df_copy.index.to_list()]
            df_copy.index = MultiIndex.from_tuples(sanitized_index_tuples, names=sanitized_index_names)
        else:
            sanitized_index_values = [sanitize(idx_val) for idx_val in df_copy.index.to_list()]
            df_copy.index = pd.Index(sanitized_index_values, name=sanitized_index_names[0] if sanitized_index_names else None)

    if isinstance(df_copy.columns, MultiIndex):
        sanitized_column_names = [sanitize(name) for name in df_copy.columns.names]
        sanitized_column_tuples = [tuple(sanitize(level) for level in col_tuple) for col_tuple in df_copy.columns.to_list()]
        df_copy.columns = MultiIndex.from_tuples(sanitized_column_tuples, names=sanitized_column_names)
    else:
        df_copy.columns = pd.Index([sanitize(col_name) for col_name in df_copy.columns.to_list()])

    for col in df_copy.columns:
        df_copy[col] = df_copy[col].apply(lambda x: sanitize(x) if not (isinstance(x, str) and x.startswith("\\")) else x)

    latex = []
    index_levels = df_copy.index.nlevels if (df_copy.index.name or isinstance(df_copy.index, MultiIndex)) else 0
    total_cols = len(df_copy.columns) + index_levels

    s_caption = sanitize(caption)
    s_label = sanitize(label)

    if is_longtable:
        col_fmt = column_format or ("l" * index_levels + "S[table-align-text-post=false]" * len(df_copy.columns))

        latex.append(f"% Longtable: {s_caption}")
        latex.append(f"\\begin{{longtable}}{{{col_fmt}}}")
        latex.append(f"\\caption{{{s_caption}}}\\label{{{s_label}}}\\\\")
        latex.append("\\toprule")

        header_parts = []
        if index_levels > 0:
            header_parts.extend(df_copy.index.names)

        if isinstance(df_copy.columns, MultiIndex):
            for i in range(df_copy.columns.nlevels):
                level_headers = []
                if index_levels > 0 and i == 0:
                    level_headers.extend([""] * index_levels)

                current_level_names = df_copy.columns.get_level_values(i)
                if i == 0:
                    spans = []
                    if len(current_level_names) > 0:
                        curr_name, curr_count = current_level_names[0], 1
                        for name_val in current_level_names[1:]:
                            if name_val == curr_name:
                                curr_count += 1
                            else:
                                spans.append((curr_name, curr_count))
                                curr_name, curr_count = name_val, 1
                        spans.append((curr_name, curr_count))
                    level_headers.extend([f"\\multicolumn{{{count}}}{{c}}{{{name}}}" for name, count in spans])
                else:
                    level_headers.extend(list(current_level_names))

                if i == 0 and index_levels > 0:
                    final_header_row_parts = list(df_copy.index.names) + level_headers[index_levels:]
                elif i > 0 and index_levels > 0:
                    final_header_row_parts = [""] * index_levels + level_headers
                else:
                    final_header_row_parts = level_headers

                latex.append(" & ".join(final_header_row_parts) + "\\\\")
        else:
            header_parts.extend(list(df_copy.columns))
            latex.append(" & ".join(header_parts) + "\\\\")

        latex.append("\\midrule")
        latex.append("\\endfirsthead")

        latex.append(f"\\caption[]{{(Continued) {s_caption}}}\\\\")
        latex.append("\\toprule")
        if isinstance(df_copy.columns, MultiIndex):
            for i in range(df_copy.columns.nlevels):
                level_headers = []
                if index_levels > 0 and i == 0:
                    level_headers.extend([""] * index_levels)
                current_level_names = df_copy.columns.get_level_values(i)
                if i == 0:
                    spans = []
                    if len(current_level_names) > 0:
                        curr_name, curr_count = current_level_names[0], 1
                        for name_val in current_level_names[1:]:
                            if name_val == curr_name:
                                curr_count += 1
                            else:
                                spans.append((curr_name, curr_count))
                                curr_name, curr_count = name_val, 1
                        spans.append((curr_name, curr_count))
                    level_headers.extend([f"\\multicolumn{{{count}}}{{c}}{{{name}}}" for name, count in spans])
                else:
                    level_headers.extend(list(current_level_names))
                if i == 0 and index_levels > 0:
                    final_header_row_parts = list(df_copy.index.names) + level_headers[index_levels:]
                elif i > 0 and index_levels > 0:
                    final_header_row_parts = [""] * index_levels + level_headers
                else:
                    final_header_row_parts = level_headers
                latex.append(" & ".join(final_header_row_parts) + "\\\\")
        else:
            repeated_header_parts = []
            if index_levels > 0:
                repeated_header_parts.extend(df_copy.index.names)
            repeated_header_parts.extend(list(df_copy.columns))
            latex.append(" & ".join(repeated_header_parts) + "\\\\")
        latex.append("\\midrule")
        latex.append("\\endhead")

        latex.append("\\midrule")
        latex.append(f"\\multicolumn{{{total_cols}}}{{r}}{{\\textit{{Continued on next page}}}}\\\\")
        latex.append("\\endfoot")

        latex.append("\\bottomrule")
        if notes:
            sanitized_notes = []
            for note_line in notes:
                if "\\cite{" in note_line:
                    parts = note_line.split("\\cite{")
                    processed_parts = [sanitize(parts[0])]
                    for part in parts[1:]:
                        cite_key_and_rest = part.split("}", 1)
                        processed_parts.append("\\cite{" + cite_key_and_rest[0] + "}")
                        if len(cite_key_and_rest) > 1:
                            processed_parts.append(sanitize(cite_key_and_rest[1]))
                    sanitized_notes.append("".join(processed_parts))
                else:
                    sanitized_notes.append(sanitize(note_line))

            notes_str = ('\\\\\n').join(sanitized_notes)
            latex.append(f"\\multicolumn{{{total_cols}}}{{p{{\\dimexpr\\linewidth-2\\tabcolsep\\relax}}}}{{\\footnotesize {notes_str}}}\\\\")
        latex.append("\\endlastfoot")

        for idx, row_series in df_copy.iterrows():
            row_data = []
            if isinstance(idx, tuple):
                row_data.extend([str(i) for i in idx])
            elif index_levels:
                row_data.append(str(idx))

            row_data.extend([str(val) for val in row_series])
            latex.append(" & ".join(row_data) + "\\\\")
        latex.append("\\end{longtable}")

    else:
        latex.append("% Standard table, not longtable")
        latex.append("\\begin{table}[htp!]\\centering")
        latex.append(f"\\caption{{{s_caption}}}\\label{{{s_label}}}")
        col_fmt_standard = column_format or ("l" * index_levels + "r" * len(df_copy.columns))

        latex_data_str = df_copy.to_latex(
            escape=False,
            na_rep="-",
            column_format=col_fmt_standard,
            index=bool(index_levels),
            header=True,
            multirow=True,
            multicolumn_format="c",
        )

        if "S[" in col_fmt_standard:
            logger.warning("Table '%s': Using S columns with pandas.to_latex. Headers might need manual adjustment or \\multicolumn.", s_caption)
            if not isinstance(df_copy.columns, MultiIndex):
                lines = latex_data_str.splitlines()
                header_idx = -1
                for i, line_txt in enumerate(lines):
                    if "\\midrule" in line_txt and header_idx == -1:
                        header_idx = i - 1
                        if header_idx >= 0 and "&" in lines[header_idx]:
                            header_content = lines[header_idx].split("&")
                            new_header_parts = []
                            col_specs = col_fmt_standard.replace(" ", "")

                            current_col_spec_idx = 0
                            if index_levels > 0:
                                new_header_parts.extend(header_content[:index_levels])
                                current_col_spec_idx += index_levels
                                header_content = header_content[index_levels:]

                            for i_h, head_item in enumerate(header_content):
                                spec_part = ""
                                if "S[" in col_specs[current_col_spec_idx:]:
                                    end_s = col_specs.find("]", current_col_spec_idx)
                                    if end_s != -1:
                                        spec_part = col_specs[current_col_spec_idx : end_s + 1]
                                        current_col_spec_idx = end_s + 1
                                else:
                                    spec_part = col_specs[current_col_spec_idx]
                                    current_col_spec_idx += 1

                                if "S[" in spec_part:
                                    new_header_parts.append(f"\\multicolumn{{1}}{{c}}{{{head_item.strip()}}}")
                                else:
                                    new_header_parts.append(head_item)
                            lines[header_idx] = " & ".join(new_header_parts)
                            latex_data_str = "\n".join(lines)
                            break

        latex.append(latex_data_str)
        if notes:
            sanitized_notes = []
            for note_line in notes:
                if "\\cite{" in note_line:
                    parts = note_line.split("\\cite{")
                    processed_parts = [sanitize(parts[0])]
                    for part in parts[1:]:
                        cite_key_and_rest = part.split("}", 1)
                        processed_parts.append("\\cite{" + cite_key_and_rest[0] + "}")
                        if len(cite_key_and_rest) > 1:
                            processed_parts.append(sanitize(cite_key_and_rest[1]))
                    sanitized_notes.append("".join(processed_parts))
                else:
                    sanitized_notes.append(sanitize(note_line))
            notes_str = ('\\\\\n').join(sanitized_notes)
            latex.append("\\smallskip\n\\begin{minipage}{\\textwidth}\\footnotesize")
            latex.append(notes_str)
            latex.append("\\end{minipage}")
        latex.append("\\end{table}")

    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with output_file.open("w", encoding="utf-8") as f:
            f.write("\n".join(latex))
        logger.info("Generic LaTeX table '%s' saved to %s", s_caption, output_file)
    except Exception as e:
        logger.error("Error saving generic LaTeX table %s: %s", output_file, e)


def main() -> None:
    """
    Main function to generate LaTeX tables for different benchmarks based on configuration and results JSONs.
    """
    parser = argparse.ArgumentParser(description="Generate LaTeX tables from VocSim JSON results.")
    parser.add_argument("--paper_configs", type=str, nargs="+", required=True, help="Paths to YAML configuration files.")
    parser.add_argument("--default_output_tables_dir_name", type=str, default="paper_tables_script_generated", help="Default subdirectory for tables.")
    args = parser.parse_args()

    for config_path_str in args.paper_configs:
        config_file = Path(config_path_str)
        if not config_file.is_file():
            logger.error("Config file not found: %s. Skipping.", config_file)
            continue
        logger.info("\n===== Processing Config: %s =====", config_file.name)
        cfg = load_config(config_file)
        if not cfg:
            continue

        project_root = Path(cfg.get("project_root", ".")).resolve()
        results_dir_base = Path(cfg.get("results_dir", project_root / "results")).resolve()
        output_dir = Path(cfg.get("output_tables_dir", results_dir_base / args.default_output_tables_dir_name)).resolve()
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Outputting tables for %s to: %s", config_file.name, output_dir)

        vocsim_data_subset_keys = [key for key in VOCSIM_APPENDIX_S_COLUMN_ORDER if key not in ["Avg", "Avg (Blind)"]]

        config_subsets_to_load_from_json = cfg.get("dataset", {}).get("subsets_to_run", ["all"])
        if not isinstance(config_subsets_to_load_from_json, list):
            config_subsets_to_load_from_json = [config_subsets_to_load_from_json]

        dfs_loaded = []
        for subset_key_to_load in config_subsets_to_load_from_json:
            json_dir_path = results_dir_base / subset_key_to_load
            json_file_path = find_latest_results_json(json_dir_path)
            if json_file_path:
                df_loaded_single = load_results_json(json_file_path, subset_key_to_load)
                if df_loaded_single is not None and not df_loaded_single.empty:
                    dfs_loaded.append(df_loaded_single)
            else:
                logger.warning("No JSON found for subset key '%s' in %s", subset_key_to_load, json_dir_path)

        if not dfs_loaded:
            logger.warning("No data loaded for %s from any specified subset. Skipping table generation.", config_file.name)
            continue
        combined_df_all_loaded = pd.concat(dfs_loaded, ignore_index=True)
        config_name_stem = config_file.stem.lower()

        if "vocsim_paper" in config_name_stem:
            df_for_vocsim_main_table = combined_df_all_loaded[combined_df_all_loaded["subset"] == "all"]
            if not df_for_vocsim_main_table.empty:
                table_vocsim_main = generate_vocsim_main_table(df_for_vocsim_main_table)
                if table_vocsim_main is not None:
                    output_latex_table(
                        table_vocsim_main,
                        "Performance Comparison on VocSim (Overall Average, Cosine Distance Preferred).",
                        "tab:main-results-comparison",
                        output_dir / "table_main_vocsim_results.tex",
                    )
            else:
                logger.warning("No 'all' subset data in loaded DFs for VocSim main table generation.")

            metrics_for_vocsim_appendix = OrderedDict([
                ("P@1", ("PrecisionAtK", "P@1", True, True)),
                ("P@5", ("PrecisionAtK", "P@5", True, True)),
                ("CSCF", ("CSCFBenchmark", "pccf", False, False)),
                ("CS", ("FValueBenchmark", "pairwise_f_value", True, True)),
                ("CSR", ("ClassSeparationRatio", "csr_score", True, False)),
                ("Weighted Purity", ("ClusteringPurity", "weighted_purity", True, True)),
            ])

            for metric_disp_name_iter, (bench_prefix_iter, json_col_iter, higher_bool_iter, is_perc_bool_iter) in metrics_for_vocsim_appendix.items():
                logger.info("Preparing data for VocSim Appendix table: %s", metric_disp_name_iter)

                table_data_df_for_appendix = generate_full_results_table(
                    combined_df_all_loaded, metric_disp_name_iter, bench_prefix_iter, json_col_iter, is_perc_bool_iter, higher_bool_iter, vocsim_data_subset_keys
                )

                if table_data_df_for_appendix is not None and not table_data_df_for_appendix.empty:
                    try:
                        table_data_df_for_appendix = table_data_df_for_appendix.reindex(columns=VOCSIM_APPENDIX_S_COLUMN_ORDER)
                    except Exception as e:
                        logger.error(
                            "Failed to reindex columns for %s table: %s. Columns present: %s. Expected: %s",
                            metric_disp_name_iter,
                            e,
                            table_data_df_for_appendix.columns,
                            VOCSIM_APPENDIX_S_COLUMN_ORDER,
                        )
                        continue

                    if table_data_df_for_appendix.isnull().all().all():
                        logger.warning("Data for %s resulted in all NaN columns after reordering. Skipping table generation.", metric_disp_name_iter)
                        continue

                    suffix_str_app = "($\\uparrow$ better)" if higher_bool_iter else "($\\downarrow$ better)"
                    table_title_app = f"{get_display_name(metric_disp_name_iter, 'metric')} Results Across Subsets and Distances {suffix_str_app}"
                    table_label_str_app = f"tab:appendix_{metric_disp_name_iter.lower().replace('@', '').replace(' ', '_').replace('(','').replace(')','')}_all_dists"
                    output_file_path_app = output_dir / f"table_appendix_{metric_disp_name_iter.lower().replace('@', '').replace(' ', '_').replace('(','').replace(')','')}_all_dists.tex"

                    logger.info("Ensuring generate_vocsim_appendix_longtable_latex is called for metric: %s", metric_disp_name_iter)
                    generate_vocsim_appendix_longtable_latex(table_data_df_for_appendix, table_title_app, table_label_str_app, output_file_path_app)
                else:
                    logger.warning("No data DataFrame generated by `generate_full_results_table` for metric: %s. Cannot create appendix table.", metric_disp_name_iter)

        elif "avian_paper" in config_name_stem:
            df_for_avian_table = combined_df_all_loaded[combined_df_all_loaded["subset"] == "avian_perception"]
            if df_for_avian_table.empty and "all" in combined_df_all_loaded["subset"].unique():
                df_for_avian_table = combined_df_all_loaded[combined_df_all_loaded["subset"] == "all"]

            if not df_for_avian_table.empty:
                table_avian = generate_avian_perception_table(df_for_avian_table)
                if table_avian is not None:
                    notes_avian = [
                        sanitize("\\textit{Comparison based on Triplet Accuracy (High), >70\\% consistency (Zandberg et al., 2024).}"),
                        sanitize("\\textit{Methods with * are reference values from \\cite{zandberg2024bird}.}"),
                    ]

                    logger.info("Generating Avian Perception table as a LONGTABLE.")
                    output_latex_table(
                        table_avian,
                        "Avian Perception Alignment: Triplet Accuracy (High Consistency)",
                        "tab:avian-perception-triplet-high",
                        output_dir / "table_avian_perception_triplet_high.tex",
                        column_format="lS[table-format=2.1]",
                        notes=notes_avian,
                        is_longtable=True,
                    )
            else:
                logger.warning("No 'avian_perception' or 'all' subset data found for Avian paper.")

        elif "mouse_strain_paper" in config_name_stem:
            df_for_strain_table = combined_df_all_loaded[combined_df_all_loaded["subset"] == "mouse_strain"]
            if df_for_strain_table.empty:
                logger.warning("No 'mouse_strain' data found for Mouse Strain paper.")
            else:
                desired_order_strain = ["EF", "ET", "ETF", "EMTF", "EF (D=30)", "EF (D=100)", "ET (D=30)", "ET (D=100)", "ETF (D=30)", "ETF (D=100)", "EMTF (D=30)", "EMTF (D=100)"]
                short_to_full_map_strain = {cfg.get("short_name", n): n for n, cfg in FEATURE_CONFIGS.items()}
                unique_features_strain = sorted(df_for_strain_table["feature"].unique())
                features_for_strain_table = [short_to_full_map_strain.get(s, s) for s in desired_order_strain if short_to_full_map_strain.get(s, s) in unique_features_strain]
                features_for_strain_table.extend([f for f in unique_features_strain if f not in features_for_strain_table])
                table_strain = generate_mouse_strain_table(df_for_strain_table, features_for_strain_table, GOFFINET_FEATURES, CLASSIFIERS)
                if table_strain is not None:
                    col_count_strain = len(table_strain.columns)
                    col_fmt_strain = "l" + "c" * col_count_strain
                    notes_strain = ["Results with (*) are from Goffinet et al. (2021). Values are Top-1 accuracy (\\%) with std."]
                    output_latex_table(
                        table_strain,
                        "Predicting mouse strain. Classification accuracy (Top-1, \\%) and std over 5 splits.",
                        "tab:mouse-strain-features-rows",
                        output_dir / "table_mouse_strain_features_rows.tex",
                        col_fmt_strain,
                        notes_strain,
                        is_longtable=True,
                    )

        elif "mouse_identity_paper" in config_name_stem:
            df_for_identity_table = combined_df_all_loaded[combined_df_all_loaded["subset"] == "mouse_identity"]
            if df_for_identity_table.empty:
                logger.warning("No 'mouse_identity' data found for Mouse Identity paper.")
            else:
                features_for_identity_table = sorted(df_for_identity_table["feature"].unique())
                table_identity = generate_mouse_identity_table(df_for_identity_table, features_for_identity_table, MLP_CONFIGS, GOFFINET_FEATURES)
                if table_identity is not None:
                    col_fmt_identity = "l" + ("cc" * len(MLP_CONFIGS))
                    notes_identity = ["Results with (*) are from Goffinet et al. (2021). Values are accuracy (\\%) with std."]
                    output_latex_table(
                        table_identity,
                        "Predicting mouse identity. Classification accuracy (\\%) and std over 5 splits.",
                        "tab:mouse-identity-combined",
                        output_dir / "table_mouse_identity_combined.tex",
                        col_fmt_identity,
                        notes_identity,
                        is_longtable=False,
                    )
        else:
            logger.warning("No specific table generation logic defined for config stem: %s", config_name_stem)

    logger.info("--- All configs processed ---")


if __name__ == "__main__":
    main()import logging
from pathlib import Path
import sys
import argparse
import os
from typing import List


try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[2]
    expected_vocsim_dir = project_root / "vocsim"
    if not expected_vocsim_dir.is_dir() and not (project_root / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARN: Assuming CWD project root: {project_root}")
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root: {project_root}")
except NameError:
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

from vocsim.runner import PipelineRunner
from utils.config_loader import load_config
from utils.logging_utils import setup_logging

CONFIG_NAME = "mouse_identity_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"


def main(steps_to_run: List[str]):
    """
    Main function to run the Mouse Identity pipeline stages.

    Args:
        steps_to_run (List[str]): A list of pipeline stage names to execute.
    """
    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        print(f"ERROR: Config not found: {config_path}")
        sys.exit(1)
    cfg = load_config(config_path, base_config_path=base_config_path if base_config_path.exists() else None)
    log_config = cfg.get("logging", {})
    log_config.setdefault("log_dir", project_root / "logs")
    log_config.setdefault("log_file", "mouse_identity_run.log")
    setup_logging(log_config)
    logger = logging.getLogger(__name__)
    logger.info(f"Loaded config from {config_path}" + (f" and merged with {base_config_path}" if base_config_path.exists() else ""))
    logger.info(f"Executing pipeline steps: {steps_to_run}")
    if Path.cwd() != project_root:
        logger.warning("Running from '%s', changing CWD to project root '%s'.", Path.cwd(), project_root)
        os.chdir(project_root)
    try:
        runner = PipelineRunner(cfg)
        runner.run(steps=steps_to_run)
        logger.info("Mouse Identity script finished successfully.")
    except Exception as e:
        logger.error("Error in Mouse Identity run: %s", e, exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Mouse Identity Pipeline Stages.")
    parser.add_argument("--steps", nargs="+", default=["benchmarks"], choices=["features", "benchmarks", "all"], help="Pipeline stages to run. 'all' runs features -> benchmarks.")
    args = parser.parse_args()
    if "all" in args.steps:
        selected_steps = ["benchmarks"]
    else:
        ordered_steps = []
        step_order = ["features", "benchmarks"]
        for step in step_order:
            if step in args.steps:
                ordered_steps.append(step)
        selected_steps = ordered_steps
    main(selected_steps)import logging
from pathlib import Path
import sys
import argparse
import os
from typing import List


try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[2]
    expected_vocsim_dir = project_root / "vocsim"
    if not expected_vocsim_dir.is_dir() and not (project_root / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARN: Assuming CWD project root: {project_root}")
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root: {project_root}")
except NameError:
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

from vocsim.runner import PipelineRunner
from utils.config_loader import load_config
from utils.logging_utils import setup_logging

CONFIG_NAME = "mouse_strain_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"


def main(steps_to_run: List[str]):
    """
    Main function to run the Mouse Strain pipeline stages.

    Args:
        steps_to_run (List[str]): A list of pipeline stage names to execute.
    """
    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        print(f"ERROR: Config not found: {config_path}")
        sys.exit(1)
    cfg = load_config(config_path, base_config_path=base_config_path if base_config_path.exists() else None)
    log_config = cfg.get("logging", {})
    log_config.setdefault("log_dir", project_root / "logs")
    log_config.setdefault("log_file", "mouse_strain_run.log")
    setup_logging(log_config)
    logger = logging.getLogger(__name__)
    logger.info(f"Loaded config from {config_path}" + (f" and merged with {base_config_path}" if base_config_path.exists() else ""))
    logger.info(f"Executing pipeline steps: {steps_to_run}")
    if Path.cwd() != project_root:
        logger.warning("Running from '%s', changing CWD to project root '%s'.", Path.cwd(), project_root)
        os.chdir(project_root)
    try:
        runner = PipelineRunner(cfg)
        runner.run(steps=steps_to_run)
        logger.info("Mouse Strain script finished successfully.")
    except Exception as e:
        logger.error("Error in Mouse Strain run: %s", e, exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    """
    Command-line entry point to run the Mouse Strain pipeline.
    """
    parser = argparse.ArgumentParser(description="Run Mouse Strain Pipeline Stages.")
    parser.add_argument("--steps", nargs="+", default=["benchmarks"], choices=["features", "benchmarks", "all"], help="Pipeline stages to run. 'all' runs features -> benchmarks.")
    args = parser.parse_args()
    if "all" in args.steps:
        selected_steps = ["features", "benchmarks"]
    else:
        ordered_steps = []
        step_order = ["features", "benchmarks"]
        for step in step_order:
            if step in args.steps:
                ordered_steps.append(step)
        selected_steps = ordered_steps
    main(selected_steps)# -*- coding: utf-8 -*-
"""
Handles loading of configurations and parsing of benchmark result files.
"""
import ast
import json
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from typing_extensions import OrderedDict

import pandas as pd
import yaml
from pandas import DataFrame

from .table_configs import PRETTY_NAMES

logger = logging.getLogger(__name__)


class ConfigManager:
    """Manages loading and accessing configuration for table generation."""

    def __init__(self, config_path: Path):
        self.config_path = config_path
        self.config: Dict[str, Any] = {}
        self.feature_configs: Dict[str, Any] = {}
        self.feature_short_names: Dict[str, str] = {}
        self.pretty_names = PRETTY_NAMES.copy()
        
        # This will be populated from the config file.
        self.METRICS_FOR_APPENDIX: OrderedDict[str, Tuple] = OrderedDict()

        self._load_config()

    def _load_config(self):
        """Loads the YAML config and populates instance attributes."""
        try:
            with self.config_path.open("r") as f:
                self.config = yaml.safe_load(f) or {}
            
            extractors = self.config.get("feature_extractors", [])
            self.feature_configs = {fc["name"]: fc for fc in extractors if "name" in fc}
            self.feature_short_names = {name: cfg.get("short_name", name) for name, cfg in self.feature_configs.items()}
            self.pretty_names.update(self.config.get("table_generator_pretty_names", {}))
            
            # Define metrics for appendix based on the main config file
            self.METRICS_FOR_APPENDIX = OrderedDict([
                ("P@1", ("PrecisionAtK", "P@1", True, True)),
                ("P@5", ("PrecisionAtK", "P@5", True, True)),
                ("GSR", ("GlobalSeparationRate", "gsr_score", True, True)),
                ("Sil", ("SilhouetteBenchmark", "silhouette_score", True, True)),
                ("CSR", ("ClassSeparationRatio", "csr_score", True, True)),
                ("CS", ("FValueBenchmark", "pairwise_f_value", False, True)),
                ("CSCF", ("CSCFBenchmark", "pccf", False, True)), 
                ("Weighted Purity", ("ClusteringPurity", "weighted_purity", True, True)),
            ])

            logger.info(f"Loaded {len(self.feature_short_names)} feature configs from {self.config_path.name}")
        except Exception as e:
            logger.error(f"Error loading config {self.config_path}: {e}")
            self.config = {}

    def get_display_name(self, name: str, entity_type: str = "feature") -> str:
        """Gets the display name for a feature or other entity."""
        if entity_type == "feature" and name in self.feature_short_names:
            return self.feature_short_names[name]
        return self.pretty_names.get(name, name)
    
    def get_ordered_features(self, benchmark_only: bool = True) -> List[str]:
        """Gets a list of feature names from the config."""
        return [
            name for name, cfg in self.feature_configs.items()
            if not benchmark_only or cfg.get("benchmark_this", True)
        ]


def find_latest_results_json(directory: Path) -> Optional[Path]:
    """Finds the path to the latest results JSON file in a directory."""
    if not directory.is_dir():
        logger.debug(f"Directory not found: {directory}")
        return None
    files = sorted(directory.glob("*_results.json"), key=lambda p: p.stat().st_mtime, reverse=True)
    if files:
        logger.info(f"Found results JSON in '{directory.name}': {files[0].name}")
        return files[0]
    logger.warning(f"No '*_results.json' file found in {directory}")
    return None

def parse_benchmark_params(benchmark_str: str) -> Tuple[str, Dict[str, Any]]:
    """Parses a benchmark string like 'MLP(alpha=0.01)' into type and params."""
    match = re.match(r"(\w+)\((.*)\)", benchmark_str)
    if not match:
        return benchmark_str, {}
    clf_type, params_str = match.groups()
    params = {}
    # A robust regex to capture different parameter value types
    param_pattern = re.compile(r"(\w+)\s*=\s*('[^']*'|\"[^\"]*\"|\[.*?\]|\(.*?,\s*\)|\(.*?\)|None|True|False|[\w\.-]+(?:e[+-]?\d+)?)")
    for p_match in param_pattern.finditer(params_str):
        key, val_str = p_match.groups()
        try:
            val = ast.literal_eval(val_str.strip())
        except (ValueError, SyntaxError, TypeError):
            val = val_str.strip()
    return clf_type, params

def load_results_json(json_path: Path, subset_name: str) -> Optional[DataFrame]:
    """Loads and parses benchmark results from a JSON file into a pandas DataFrame."""
    if not json_path.is_file(): return None
    try:
        with json_path.open("r", encoding="utf-8") as f: data = json.load(f)
        records = []
        for feature, feature_data in data.items():
            if not isinstance(feature_data, dict): continue
            for metric_type, metric_data in feature_data.items():
                if not isinstance(metric_data, dict): continue
                base = {"subset": subset_name, "feature": feature, "metric_type": metric_type}
                if metric_type == "distance_based":
                    for distance, benchmarks in metric_data.items():
                        for bench_name, results in benchmarks.items():
                            if bench_name == "ClassificationBenchmark" and isinstance(results, dict):
                                for clf_config, scores in results.items():
                                    records.append({**base, "distance": distance, "benchmark": clf_config, **(scores if isinstance(scores, dict) else {"value": scores})})
                            else:
                                records.append({**base, "distance": distance, "benchmark": bench_name, **(results if isinstance(results, dict) else {"value": results})})
                elif metric_type == "feature_based":
                    for bench_name, results in metric_data.items():
                        if bench_name == "ClassificationBenchmark" and isinstance(results, dict):
                            for clf_config, scores in results.items():
                                records.append({**base, "distance": "N/A", "benchmark": clf_config, **(scores if isinstance(scores, dict) else {"value": scores})})
                        else:
                            records.append({**base, "distance": "N/A", "benchmark": bench_name, **(results if isinstance(results, dict) else {"value": results})})
        return DataFrame(records) if records else DataFrame()
    except Exception as e:
        logger.error(f"Error parsing JSON {json_path}: {e}"); return None# In: reproducibility/scripts/paper/generate_paper_tables.py

# -*- coding: utf-8 -*-
"""
Main driver script to generate all LaTeX tables for the paper from VocSim JSON results.
This script orchestrates the loading of configurations and data, calls the specific
table generators for each benchmark (VocSim, Avian, Mouse, Correlation), and saves
the final .tex files.
"""
import argparse
import logging
from pathlib import Path

import pandas as pd
import numpy as np


from .data_loader import (ConfigManager, find_latest_results_json,
                          load_results_json)
from .latex_utils import output_latex_table, generate_vocsim_appendix_longtable_latex
from .table_configs import (CLASSIFIERS, GOFFINET_FEATURES, MLP_CONFIGS,
                            VOCSIM_APPENDIX_S_COLUMN_ORDER, BLIND_TEST_SUBSETS, METRICS_FOR_CORRELATION)
from .table_generators import (generate_avian_perception_table,
                               generate_averaged_correlation_table,
                               generate_mouse_identity_table,
                               generate_mouse_strain_table,
                               generate_vocsim_main_table,
                               generate_full_results_table)

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


def main() -> None:
    """
    Main function to generate LaTeX tables for different benchmarks based on
    configuration and results JSONs.
    """
    parser = argparse.ArgumentParser(description="Generate LaTeX tables from VocSim JSON results.")
    parser.add_argument("--paper_configs", type=str, nargs="+", required=True, help="Paths to YAML configuration files.")
    parser.add_argument("--default_output_tables_dir_name", type=str, default="paper_tables_script_generated", help="Default subdirectory for tables.")
    args = parser.parse_args()

    for config_path_str in args.paper_configs:
        config_file = Path(config_path_str)
        if not config_file.is_file():
            logger.error(f"Config file not found: {config_file}. Skipping.")
            continue

        logger.info(f"\n===== Processing Config: {config_file.name} =====")
        config_manager = ConfigManager(config_file)
        cfg = config_manager.config
        if not cfg:
            continue

        project_root = Path(cfg.get("project_root", ".")).resolve()
        results_dir_base = Path(cfg.get("results_dir", project_root / "results")).resolve()
        output_dir = Path(cfg.get("output_tables_dir", results_dir_base / args.default_output_tables_dir_name)).resolve()
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Outputting tables for {config_file.name} to: {output_dir}")

        subsets_to_load = cfg.get("dataset", {}).get("subsets_to_run", ["all"])
        if not isinstance(subsets_to_load, list):
            subsets_to_load = [subsets_to_load]

        dfs_loaded = []
        for subset_key in subsets_to_load:
            json_dir = results_dir_base / subset_key
            json_file = find_latest_results_json(json_dir)
            if json_file:
                df_single = load_results_json(json_file, subset_key)
                if df_single is not None and not df_single.empty:
                    dfs_loaded.append(df_single)
            else:
                logger.warning(f"No JSON found for subset '{subset_key}' in {json_dir}")

        if not dfs_loaded:
            logger.warning(f"No data loaded for {config_file.name}. Skipping table generation.")
            continue

        combined_df = pd.concat(dfs_loaded, ignore_index=True)
        config_name_stem = config_file.stem.lower()

        # --- Generate Tables Based on Config File Name ---
        if "vocsim_paper" in config_name_stem:
            df_for_main = combined_df[combined_df["subset"] == "all"]
            if not df_for_main.empty:
                logger.info("--- Generating VocSim Main Table ---")
                table_main = generate_vocsim_main_table(df_for_main, config_manager)
                if table_main is not None:
                    output_latex_table(
                        table_main,
                        "Performance Comparison on VocSim (Overall Average, Cosine Distance Preferred).",
                        "tab:main-results-comparison",
                        output_dir / "table_main_vocsim_results.tex",
                    )

            logger.info("--- Generating Metric Correlation Tables ---")
            
            # 1. Generate correlation table for ALL methods
            logger.info("Calculating correlation for ALL methods...")
            correlation_table_all = generate_averaged_correlation_table(combined_df, config_manager)
            if correlation_table_all is not None:
                output_latex_table(
                    correlation_table_all,
                    caption="Average Spearman Rank Correlation ($\\rho$) of Key Metrics (All Methods)",
                    label="tab:metric-correlations-all",
                    output_file=output_dir / "table_metric_correlations_all.tex",
                    column_format="l" + "c" * len(correlation_table_all.columns)
                )

            # 2. Identify the "best performing" methods to generate a second table
            logger.info("Identifying top 20% of methods for focused correlation analysis...")
            public_df = combined_df[~combined_df['subset'].isin(BLIND_TEST_SUBSETS)].copy()
            public_df['method'] = public_df['feature'] + ' | ' + public_df['distance']
            
            method_scores = {}
            for method in public_df['method'].unique():
                method_df = public_df[public_df['method'] == method]
                scores = []
                for metric_name, config in METRICS_FOR_CORRELATION.items():
                    score_df = method_df[method_df['benchmark'] == config['benchmark']]
                    if not score_df.empty and config['column'] in score_df.columns:
                        # Average the transformed score across public subsets
                        avg_score = score_df[config['column']].apply(config['transform']).mean()
                        if pd.notna(avg_score):
                            scores.append(avg_score)
                if scores:
                    method_scores[method] = np.mean(scores)
            
            if method_scores:
                sorted_methods = sorted(method_scores.items(), key=lambda item: item[1], reverse=True)
                top_20_percent_count = int(len(sorted_methods) * 0.2)
                best_methods = [method for method, score in sorted_methods[:top_20_percent_count]]
                logger.info(f"Identified {len(best_methods)} best-performing methods.")

                # 3. Generate correlation table for ONLY the best methods
                logger.info("Calculating correlation for BEST PERFORMING methods...")
                correlation_table_best = generate_averaged_correlation_table(combined_df, config_manager, methods_to_include=best_methods)
                if correlation_table_best is not None:
                    output_latex_table(
                        correlation_table_best,
                        caption="Average Spearman Rank Correlation ($\\rho$) of Key Metrics (Top 20\\% of Methods)",
                        label="tab:metric-correlations-best",
                        output_file=output_dir / "table_metric_correlations_best.tex",
                        column_format="l" + "c" * len(correlation_table_best.columns)
                    )
            else:
                logger.warning("Could not determine best performing methods.")


            logger.info("--- Generating VocSim Appendix Tables ---")
            metrics_for_appendix = config_manager.METRICS_FOR_APPENDIX
            vocsim_subsets = [k for k in VOCSIM_APPENDIX_S_COLUMN_ORDER if k not in ["Avg", "Avg (Blind)"]]
            for metric_name, (bench, col, higher_is_better, is_percent) in metrics_for_appendix.items():
                table_appendix = generate_full_results_table(
                    combined_df, metric_name, bench, col, is_percent, higher_is_better, vocsim_subsets, config_manager
                )
                if table_appendix is not None:
                    suffix = "($\\uparrow$ better)" if higher_is_better else "($\\downarrow$ better)"
                    caption = f"{config_manager.get_display_name(metric_name, 'metric')} Results Across Subsets and Distances {suffix}"
                    label = f"tab:appendix_{metric_name.lower().replace('@', '')}"
                    generate_vocsim_appendix_longtable_latex(table_appendix, caption, label, output_dir / f"table_appendix_{metric_name.lower().replace('@','')}.tex")

        elif "avian_paper" in config_name_stem:
            df_for_avian = combined_df[combined_df["subset"] == "avian_perception"]
            if not df_for_avian.empty:
                table_avian = generate_avian_perception_table(df_for_avian, config_manager)
                if table_avian is not None:
                    notes = ["\\textit{Comparison based on Triplet Accuracy (High), >70\\% consistency (Zandberg et al., 2024).}", "\\textit{Methods with * are reference values from \\cite{zandberg2024bird}.}"]
                    output_latex_table(
                        table_avian,
                        "Avian Perception Alignment: Triplet Accuracy (High Consistency)",
                        "tab:avian-perception-triplet-high",
                        output_dir / "table_avian_perception_triplet_high.tex",
                        column_format="lS[table-format=2.1]",
                        notes=notes,
                        is_longtable=True
                    )

        elif "mouse_strain_paper" in config_name_stem:
            df_for_strain = combined_df[combined_df["subset"] == "mouse_strain"]
            if not df_for_strain.empty:
                features_for_table = config_manager.get_ordered_features()
                table_strain = generate_mouse_strain_table(df_for_strain, features_for_table, GOFFINET_FEATURES, CLASSIFIERS, config_manager)
                if table_strain is not None:
                    notes = ["Results with (*) are from Goffinet et al. (2021). Values are Top-1 accuracy (\\%) with std."]
                    output_latex_table(
                        table_strain,
                        "Predicting mouse strain. Classification accuracy (Top-1, \\%) and std over 5 splits.",
                        "tab:mouse-strain-features-rows",
                        output_dir / "table_mouse_strain_features_rows.tex",
                        notes=notes,
                        is_longtable=True
                    )

        elif "mouse_identity_paper" in config_name_stem:
            df_for_identity = combined_df[combined_df["subset"] == "mouse_identity"]
            if not df_for_identity.empty:
                features_for_table = config_manager.get_ordered_features()
                table_identity = generate_mouse_identity_table(df_for_identity, features_for_table, MLP_CONFIGS, GOFFINET_FEATURES, config_manager)
                if table_identity is not None:
                    notes = ["Results with (*) are from Goffinet et al. (2021). Values are accuracy (\\%) with std."]
                    output_latex_table(
                        table_identity,
                        "Predicting mouse identity. Classification accuracy (\\%) and std over 5 splits.",
                        "tab:mouse-identity-combined",
                        output_dir / "table_mouse_identity_combined.tex",
                        notes=notes,
                        is_longtable=False
                    )

    logger.info("--- All configs processed ---")


if __name__ == "__main__":
    main()# -*- coding: utf-8 -*-
"""
Utility functions for generating and formatting LaTeX tables.
"""
import logging
from pathlib import Path
import re
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from pandas import DataFrame, MultiIndex

from .table_configs import PRETTY_NAMES, VOCSIM_APPENDIX_S_COLUMN_ORDER
from .data_loader import ConfigManager

logger = logging.getLogger(__name__)


def sanitize(text: Any) -> str:
    """Sanitizes text for LaTeX output, avoiding changes to likely LaTeX commands."""
    if pd.isna(text):
        return "-"
    s_text = str(text)
    # Don't sanitize if it looks like a LaTeX command
    if any(cmd in s_text for cmd in ["\\textbf{", "\\makecell{", "\\multicolumn{", "\\textit{", "\\emph{", "\\texttt{"]):
        return s_text
    
    replacements = {
        "&": r"\&", "%": r"\%", "$": r"\$", "#": r"\#", "_": r"\_",
        "{": r"\{", "}": r"\}", "~": r"\textasciitilde{}", "^": r"\^{}",
        "\\": r"\\", "*": r"$^*$", "<": r"\textless{}", ">": r"\textgreater{}"
    }
    for old, new in replacements.items():
        s_text = s_text.replace(old, new)
    return s_text

def format_number(value: Any, precision: int = 1, is_percentage: bool = False) -> str:
    """Formats a numerical value to a string with specified precision."""
    if pd.isna(value) or value is None:
        return "-"
    if isinstance(value, (int, float, np.number)):
        val_to_format = value * 100 if is_percentage else value
        return f"{val_to_format:.{precision}f}"
    return str(value)

def parse_value_for_comparison(value: Any) -> float:
    """Parses a formatted string (e.g., with bolding) back to a float."""
    if pd.isna(value) or not isinstance(value, str) or value == "-":
        return np.nan
    cleaned = value.replace("\\textbf{", "").replace("}", "").replace("%", "").strip()
    match = re.match(r"^\s*(-?(?:\d+\.\d+|\d+))", cleaned)
    return float(match.group(1)) if match else np.nan

def bold_string(value: Any) -> str:
    """Wraps a string in LaTeX bold command if not already bolded."""
    str_value = str(value)
    if pd.isna(value) or str_value == "-":
        return str_value
    if str_value.startswith("\\textbf{") and str_value.endswith("}"):
        return str_value
    return f"\\textbf{{{str_value}}}"

def bold_best_in_columns(df: DataFrame, columns: List[str], higher_is_better: Dict[str, bool]) -> DataFrame:
    """Bolds the best values in specified columns of a DataFrame."""
    result = df.copy()
    for col in columns:
        if col not in result.columns:
            continue
        is_higher = higher_is_better.get(col, True)
        numeric_values = result[col].apply(parse_value_for_comparison)
        valid_numeric_values = numeric_values.dropna()
        if valid_numeric_values.empty:
            continue
        best_numeric = valid_numeric_values.max() if is_higher else valid_numeric_values.min()
        for idx in result.index:
            if pd.notna(numeric_values.loc[idx]) and np.isclose(numeric_values.loc[idx], best_numeric):
                result.loc[idx, col] = bold_string(result.loc[idx, col])
    return result

def bold_overall_best_in_group_df(df: pd.DataFrame, columns_to_consider: List[Any], higher_is_better: bool, n_best: int = 1) -> pd.DataFrame:
    """Bolds the top N values across a group of columns in a DataFrame."""
    df_out = df.copy()
    if not columns_to_consider:
        return df_out
    
    all_values = []
    for r_idx in df_out.index:
        for c_idx in columns_to_consider:
            if c_idx not in df_out.columns: continue
            val_str = df_out.loc[r_idx, c_idx]
            numeric_val = parse_value_for_comparison(val_str)
            if pd.notna(numeric_val):
                all_values.append({'val': numeric_val, 'r_idx': r_idx, 'c_idx': c_idx, 'orig_str': val_str})
    
    if not all_values: return df_out
    
    all_values.sort(key=lambda x: x['val'], reverse=higher_is_better)
    
    if n_best > 0 and all_values:
        cutoff_score = all_values[min(n_best, len(all_values)) - 1]['val']
        for item in all_values:
            is_close = np.isclose(item['val'], cutoff_score)
            should_bold = (higher_is_better and (item['val'] > cutoff_score or is_close)) or \
                          (not higher_is_better and (item['val'] < cutoff_score or is_close))
            if should_bold:
                df_out.loc[item['r_idx'], item['c_idx']] = bold_string(item['orig_str'])
    return df_out


def output_latex_table(df: DataFrame, caption: str, label: str, output_file: Path, column_format: Optional[str] = None, notes: Optional[List[str]] = None, is_longtable: bool = False):
    """Generates and writes a LaTeX table (standard or longtable) from a DataFrame."""
    if df.empty:
        logger.warning(f"DataFrame for '{caption}' is empty. Skipping LaTeX output.")
        return

    df_copy = df.copy()
    
    # Sanitize index and columns
    if df_copy.index.name or isinstance(df_copy.index, MultiIndex):
        df_copy.index.names = [sanitize(name) for name in df_copy.index.names]
        if isinstance(df_copy.index, MultiIndex):
            df_copy.index = MultiIndex.from_tuples([tuple(sanitize(level) for level in idx) for idx in df_copy.index])
        else:
            df_copy.index = [sanitize(idx) for idx in df_copy.index]

    if isinstance(df_copy.columns, MultiIndex):
        df_copy.columns.names = [sanitize(name) for name in df_copy.columns.names]
        df_copy.columns = MultiIndex.from_tuples([tuple(sanitize(level) for level in col) for col in df_copy.columns])
    else:
        df_copy.columns = [sanitize(col) for col in df_copy.columns]

    for col in df_copy.columns:
        df_copy[col] = df_copy[col].apply(lambda x: sanitize(x) if not (isinstance(x, str) and x.startswith("\\")) else x)

    latex_str = df_copy.to_latex(escape=False, na_rep="-", column_format=column_format, index=True, header=True, multirow=True, multicolumn_format="c")
    
    table_env = "longtable" if is_longtable else "table"
    
    latex = [f"\\begin{{{table_env}}}{{'[ht!]' if not is_longtable else ''}}"]
    if not is_longtable:
        latex.append("\\centering")
    latex.append(f"\\caption{{{sanitize(caption)}}}\\label{{{sanitize(label)}}}")
    if is_longtable:
        latex.append("\\\\") # Required for longtable caption
    latex.append("\\small")
    
    if is_longtable:
        # Add longtable headers and footers
        lines = latex_str.splitlines()
        header = "\n".join(lines[2:lines.index("\\midrule") + 1])
        latex.append(header)
        latex.append("\\endfirsthead")
        latex.append(f"\\caption[]{{(Continued) {sanitize(caption)}}}\\\\")
        latex.append(header)
        latex.append("\\endhead")
        latex.append("\\bottomrule")
        latex.append(f"\\multicolumn{{{len(df.columns) + df.index.nlevels}}}{{r}}{{\\textit{{Continued on next page}}}}\\\\")
        latex.append("\\endfoot")
        latex.append("\\bottomrule")
        if notes:
            notes_str = '\\\\\n'.join([sanitize(note) for note in notes])
            latex.append(f"\\multicolumn{{{len(df.columns) + df.index.nlevels}}}{{p{{\\linewidth-2\\tabcolsep}}}}{{\\footnotesize {notes_str}}}\\\\")
        latex.append("\\endlastfoot")
        latex.append("\n".join(lines[lines.index("\\midrule") + 1:-2])) # table body
    else:
        latex.append(latex_str)
        if notes:
            notes_str = '\\\\\n'.join([sanitize(note) for note in notes])
            latex.append("\\smallskip\n\\begin{minipage}{\\textwidth}\\footnotesize")
            latex.append(notes_str)
            latex.append("\\end{minipage}")
        
    latex.append(f"\\end{{{table_env}}}")
    
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with output_file.open("w", encoding="utf-8") as f:
        f.write("\n".join(latex))
    logger.info(f"LaTeX table '{caption}' saved to {output_file}")


def generate_vocsim_appendix_longtable_latex(df_data: pd.DataFrame, caption_text: str, table_label: str, output_file: Path):
    """
    Generates and writes a VocSim appendix longtable using the STRICT format,
    now compatible with siunitx S columns.
    """
    if df_data.empty:
        logger.warning(f"No data for VocSim Appendix table '{caption_text}'. Skipping file write.")
        return

    expected_cols = 21
    if len(df_data.columns) != expected_cols:
        logger.error(
            f"FATAL: VocSim appendix table expects {expected_cols} data columns, "
            f"but got {len(df_data.columns)}. Halting generation for this table."
        )
        return

    latex_lines = []
    latex_lines.append("% Add to your LaTeX preamble: \\usepackage{longtable, booktabs, siunitx, makecell}")
    latex_lines.append("\\scriptsize")
    latex_lines.append("\\setlength{\\tabcolsep}{2pt}")

    s_caption = sanitize(caption_text)
    s_label = sanitize(table_label)

    # Note: table-parse-only used to allow non-numeric content in braces {}
    latex_lines.append(f"\\begin{{longtable}}{{l l *{{{expected_cols}}}{{S[table-format=2.1, table-parse-only]}}}}")
    latex_lines.append(f"\\caption{{{s_caption}}}\\label{{{s_label}}}\\\\")
    latex_lines.append("\\toprule")

    header_parts = ["Method", "Dist"]
    for s_col_name in VOCSIM_APPENDIX_S_COLUMN_ORDER:
        display_col_name = PRETTY_NAMES.get(s_col_name, s_col_name)
        if display_col_name == "Avg (Blind)":
            header_parts.append("\\multicolumn{1}{c}{\\makecell{Avg\\\\(Blind)}}")
        else:
            header_parts.append(f"\\multicolumn{{1}}{{c}}{{{sanitize(display_col_name)}}}")

    header_full_line = " & ".join(header_parts) + " \\\\"

    latex_lines.append(header_full_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endfirsthead")
    latex_lines.append("")
    latex_lines.append(f"\\caption[]{{(Continued) {s_caption}}}\\\\")
    latex_lines.append("\\toprule")
    latex_lines.append(header_full_line)
    latex_lines.append("\\midrule")
    latex_lines.append("\\endhead")
    latex_lines.append("")
    latex_lines.append("\\bottomrule")
    latex_lines.append("\\endlastfoot")
    latex_lines.append("")

    for (method_val_idx, dist_val_idx), row_series_data in df_data.iterrows():
        row_str_parts = [sanitize(str(method_val_idx)), sanitize(str(dist_val_idx))]
        for cell_val_str in row_series_data:
            cell_content = str(cell_val_str)
            is_plain_number = False
            try:
                float(cell_content)
                is_plain_number = True
            except (ValueError, TypeError):
                is_plain_number = False

            if is_plain_number:
                row_str_parts.append(cell_content)
            else:
                row_str_parts.append(f"{{{cell_content}}}")
                
        latex_lines.append(" & ".join(row_str_parts) + " \\\\")

    latex_lines.append("\\end{longtable}")

    try:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with output_file.open("w", encoding="utf-8") as f:
            f.write("\n".join(latex_lines))
        logger.info(f"VocSim Appendix LaTeX table '{s_caption}' saved to {output_file}")
    except Exception as e:
        logger.error(f"Error saving VocSim Appendix LaTeX table {output_file}: {e}")from collections import OrderedDict

# Default pretty names for various keys. Can be overridden by config.
PRETTY_NAMES: dict[str, str] = {
    "P@1": "P@1", "P@5": "P@5", "pairwise_f_value": "CS", "pccf": "CSCF", "gsr_score": "GSR",
    "weighted_purity": "Purity (W)", "num_clusters_found": "Clusters", "csr_score": "CSR",
    "accuracy_mean": "Accuracy", "top_5_accuracy_mean": "Top-5 Acc.",
    "cosine": "C", "euclidean": "E", "spearman": "S",
    "all": "Overall", "avian_perception": "Avian Perc.", "mouse_strain": "Mouse Strain", "mouse_identity": "Mouse ID",
    "BS1": "BS1", "BS2": "BS2", "BS3": "BS3", "BS4": "BS4", "BS5": "BS5", "BC": "BC", "ES1": "ES1",
    "HP": "HP", "HS1": "HS1", "HS2": "HS2", "HU1": "HU1", "HU2": "HU2", "HU3": "HU3", "HU4": "HU4",
    "HW1": "HW1", "HW2": "HW2", "HW3": "HW3", "HW4": "HW4", "OC1": "OC1",
}

# Column order for the large VocSim appendix tables
VOCSIM_APPENDIX_S_COLUMN_ORDER = [
    "BC", "BS1", "BS2", "BS3", "BS4", "BS5", "ES1", "HP", "HS1", "HS2", "HU1", "HU2",
    "HU3", "HU4", "HW1", "HW2", "HW3", "HW4", "OC1", "Avg", "Avg (Blind)",
]

# Subsets reserved for blind testing
BLIND_TEST_SUBSETS = ["HU3", "HU4", "HW3", "HW4"]

# Configuration for metrics to be included in the correlation analysis
METRICS_FOR_CORRELATION = OrderedDict([
    ('GSR', {'benchmark': 'GlobalSeparationRate', 'column': 'gsr_score', 'transform': lambda x: x}),
    ('Silhouette', {'benchmark': 'SilhouetteBenchmark', 'column': 'silhouette_score', 'transform': lambda x: x}),
    ('P@1', {'benchmark': 'PrecisionAtK', 'column': 'P@1', 'transform': lambda x: x}),
    ('P@5', {'benchmark': 'PrecisionAtK', 'column': 'P@5', 'transform': lambda x: x}),
    ('CSR', {'benchmark': 'ClassSeparationRatio', 'column': 'csr_score', 'transform': lambda x: x}),
    ('CS', {'benchmark': 'FValueBenchmark', 'column': 'pairwise_f_value', 'transform': lambda x: 1 - x}), 
    ('CSCF', {'benchmark': 'CSCFBenchmark', 'column': 'pccf', 'transform': lambda x: 1 - x}),
])

# Reference data from Goffinet et al. (2021)
GOFFINET_FEATURES = OrderedDict([("Spectrogram D=10", "Spectrogram D=10*"), ("Spectrogram D=30", "Spectrogram D=30*"), ("Spectrogram D=100", "Spectrogram D=100*"), ("MUPET D=9", "MUPET D=9*"), ("DeepSqueak D=10", "DeepSqueak D=10*"), ("Latent D=7", "Latent D=7*"), ("Latent D=8", "Latent D=8*")])
GOFFINET_STRAIN_DATA = { "k-NN (k=3)": {"Spectrogram D=10": "68.1 (0.2)", "Spectrogram D=30": "76.4 (0.3)", "Spectrogram D=100": "82.3 (0.5)", "MUPET D=9": "86.1 (0.2)", "DeepSqueak D=10": "79.0 (0.3)", "Latent D=7": "89.8 (0.2)"}, "k-NN (k=10)": {"Spectrogram D=10": "71.0 (0.3)", "Spectrogram D=30": "78.2 (0.1)", "Spectrogram D=100": "82.7 (0.6)", "MUPET D=9": "87.0 (0.1)", "DeepSqueak D=10": "80.7 (0.3)", "Latent D=7": "90.7 (0.4)"}, "k-NN (k=30)": {"Spectrogram D=10": "72.8 (0.3)", "Spectrogram D=30": "78.5 (0.2)", "Spectrogram D=100": "81.3 (0.5)", "MUPET D=9": "86.8 (0.2)", "DeepSqueak D=10": "81.0 (0.2)", "Latent D=7": "90.3 (0.4)"}, "RF (depth=10)": {"Spectrogram D=10": "72.8 (0.2)", "Spectrogram D=30": "76.6 (0.2)", "Spectrogram D=100": "79.1 (0.3)", "MUPET D=9": "87.4 (0.5)", "DeepSqueak D=10": "81.2 (0.4)", "Latent D=7": "88.1 (0.5)"}, "RF (depth=15)": {"Spectrogram D=10": "73.1 (0.3)", "Spectrogram D=30": "78.0 (0.3)", "Spectrogram D=100": "80.5 (0.2)", "MUPET D=9": "87.9 (0.4)", "DeepSqueak D=10": "82.1 (0.3)", "Latent D=7": "89.6 (0.4)"}, "RF (depth=20)": {"Spectrogram D=10": "73.2 (0.2)", "Spectrogram D=30": "78.3 (0.2)", "Spectrogram D=100": "80.7 (0.3)", "MUPET D=9": "87.9 (0.4)", "DeepSqueak D=10": "81.9 (0.3)", "Latent D=7": "89.6 (0.4)"}, "MLP (α=0.1)": {"Spectrogram D=10": "72.4 (0.3)", "Spectrogram D=30": "79.1 (0.4)", "Spectrogram D=100": "84.5 (0.3)", "MUPET D=9": "87.8 (0.2)", "DeepSqueak D=10": "82.1 (0.4)", "Latent D=7": "90.1 (0.3)"}, "MLP (α=0.01)": {"Spectrogram D=10": "72.3 (0.4)", "Spectrogram D=30": "78.6 (0.3)", "Spectrogram D=100": "82.9 (0.4)", "MUPET D=9": "88.1 (0.3)", "DeepSqueak D=10": "82.4 (0.4)", "Latent D=7": "90.0 (0.4)"}, "MLP (α=0.001)": {"Spectrogram D=10": "72.4 (0.4)", "Spectrogram D=30": "78.5 (0.8)", "Spectrogram D=100": "82.8 (0.1)", "MUPET D=9": "87.9 (0.2)", "DeepSqueak D=10": "81.0 (0.2)", "Latent D=7": "90.4 (0.3)"}}
GOFFINET_IDENTITY_DATA = {"Top-1 accuracy": {"MLP (α=0.01)": {"Spectrogram D=10": "9.9 (0.2)", "Spectrogram D=30": "14.9 (0.2)", "Spectrogram D=100": "20.4 (0.4)", "MUPET D=9": "14.7 (0.2)", "Latent D=8": "17.0 (0.3)"}, "MLP (α=0.001)": {"Spectrogram D=10": "10.8 (0.1)", "Spectrogram D=30": "17.3 (0.4)", "Spectrogram D=100": "25.3 (0.3)", "MUPET D=9": "19.0 (0.3)", "Latent D=8": "22.7 (0.5)"}, "MLP (α=0.0001)": {"Spectrogram D=10": "10.7 (0.2)", "Spectrogram D=30": "17.3 (0.3)", "Spectrogram D=100": "25.1 (0.3)", "MUPET D=9": "20.6 (0.4)", "Latent D=8": "24.0 (0.2)"}}, "Top-5 accuracy": {"MLP (α=0.01)": {"Spectrogram D=10": "36.6 (0.4)", "Spectrogram D=30": "45.1 (0.5)", "Spectrogram D=100": "55.0 (0.3)", "MUPET D=9": "46.5 (0.3)", "Latent D=8": "49.9 (0.4)"}, "MLP (α=0.001)": {"Spectrogram D=10": "38.6 (0.2)", "Spectrogram D=30": "50.7 (0.6)", "Spectrogram D=100": "62.9 (0.4)", "MUPET D=9": "54.0 (0.2)", "Latent D=8": "59.2 (0.6)"}, "MLP (α=0.0001)": {"Spectrogram D=10": "38.7 (0.5)", "Spectrogram D=30": "50.8 (0.3)", "Spectrogram D=100": "63.2 (0.4)", "MUPET D=9": "57.3 (0.4)", "Latent D=8": "61.6 (0.4)"}}}

# Reference data from Zandberg et al. (2024)
ZANDBERG_RESULTS = {"EMB-LUA (Zandberg et al.)": 0.727, "Luscinia-U (Zandberg et al.)": 0.698, "Luscinia (Zandberg et al.)": 0.66, "SAP (Zandberg et al.)": 0.64, "Raven (Zandberg et al.)": 0.57}

# Definitions for matching classifier results from benchmark strings
CLASSIFIERS = OrderedDict([("k-NN", OrderedDict([("k=3", {"type_match": "knn", "params_to_match": {"n_neighbors": 3}}), ("k=10", {"type_match": "knn", "params_to_match": {"n_neighbors": 10}}), ("k=30", {"type_match": "knn", "params_to_match": {"n_neighbors": 30}})])), ("RF", OrderedDict([("depth=10", {"type_match": "rf", "params_to_match": {"max_depth": 10, "class_weight": "balanced"}}), ("depth=15", {"type_match": "rf", "params_to_match": {"max_depth": 15, "class_weight": "balanced"}}), ("depth=20", {"type_match": "rf", "params_to_match": {"max_depth": 20, "class_weight": "balanced"}})])), ("MLP", OrderedDict([("α=0.1", {"type_match": "mlp", "params_to_match": {"alpha": 0.1}}), ("α=0.01", {"type_match": "mlp", "params_to_match": {"alpha": 0.01}}), ("α=0.001", {"type_match": "mlp", "params_to_match": {"alpha": 0.001}})]))])
MLP_CONFIGS = OrderedDict([("MLP (α=0.01)", {"alpha": 0.01}), ("MLP (α=0.001)", {"alpha": 0.001}), ("MLP (α=0.0001)", {"alpha": 0.0001})])# In: reproducibility/scripts/paper/table_generators.py

# -*- coding: utf-8 -*-
"""
Functions for generating the specific pandas DataFrames for each paper table.
This includes the main VocSim summary, detailed appendix tables, application-specific
tables (Avian, Mouse), and the metric correlation matrix.
"""
import logging
from collections import OrderedDict
from typing import Any, Dict, List, Optional

import numpy as np
import pandas as pd
from pandas import DataFrame

from .data_loader import ConfigManager, parse_benchmark_params
from .latex_utils import bold_best_in_columns, bold_overall_best_in_group_df, bold_string, format_number
from .table_configs import (BLIND_TEST_SUBSETS, GOFFINET_IDENTITY_DATA,
                            GOFFINET_STRAIN_DATA, METRICS_FOR_CORRELATION,
                            VOCSIM_APPENDIX_S_COLUMN_ORDER, ZANDBERG_RESULTS)

logger = logging.getLogger(__name__)

def _compare_params(parsed_val: Any, target_val: Any, atol: float = 1e-6) -> bool:
    """Helper to robustly compare parsed parameters against target values."""
    if target_val is None: return parsed_val is None
    if parsed_val is None: return False
    if isinstance(target_val, float):
        return isinstance(parsed_val, (int, float)) and np.isclose(float(parsed_val), target_val, atol=atol)
    if isinstance(target_val, int):
        if isinstance(parsed_val, float):
            return parsed_val.is_integer() and int(parsed_val) == target_val
        return isinstance(parsed_val, int) and parsed_val == target_val
    if isinstance(target_val, (list, tuple)):
        if not isinstance(parsed_val, (list, tuple)) or len(target_val) != len(parsed_val):
            return False
        return all(_compare_params(p, t, atol) for p, t in zip(parsed_val, target_val))
    return parsed_val == target_val


def generate_metric_correlation_table(df: DataFrame) -> Optional[DataFrame]:
    """Generates a Spearman correlation matrix table for key performance metrics."""
    if df.empty: return None
    logger.info("Generating Metric Correlation Table...")

    df['method'] = df['feature'] + ' | ' + df['distance']
    public_df = df[~df['subset'].isin(BLIND_TEST_SUBSETS)].copy()

    agg_data = []
    for method in public_df['method'].unique():
        method_df = public_df[public_df['method'] == method]
        method_scores = {'Method': method}
        for metric_name, config in METRICS_FOR_CORRELATION.items():
            score_df = method_df[method_df['benchmark'] == config['benchmark']]
            if not score_df.empty and config['column'] in score_df.columns:
                raw_score = score_df[config['column']].mean()
                if pd.notna(raw_score):
                    method_scores[metric_name] = config['transform'](raw_score)
                else:
                    method_scores[metric_name] = np.nan
            else:
                method_scores[metric_name] = np.nan
        agg_data.append(method_scores)

    scores_df = pd.DataFrame(agg_data).set_index('Method').dropna()
    if scores_df.empty:
        logger.warning("No methods with complete scores found. Cannot generate correlation table.")
        return None

    correlation_matrix = scores_df.corr(method='spearman')
    formatted_matrix = correlation_matrix.applymap(lambda x: f"{x:.2f}")
    formatted_matrix.index.name = "Metric"
    return formatted_matrix


# In reproducibility/scripts/paper/table_generators.py

def generate_vocsim_main_table(df: DataFrame, config_manager: ConfigManager) -> Optional[DataFrame]:
    """Generates the main VocSim results table."""
    if df.empty: return None
    logger.info("Generating VocSim Main Table (Overall Results - Cosine Distance)")
    metrics = OrderedDict([
        ("GSR", ("GlobalSeparationRate", "gsr_score")),
        ("P@1", ("PrecisionAtK", "P@1")), 
        ("P@5", ("PrecisionAtK", "P@5")), 
        ("CSR", ("ClassSeparationRatio", "csr_score")), 
        ("CS", ("FValueBenchmark", "pairwise_f_value")), 
        ("CSCF", ("CSCFBenchmark", "pccf")), 
        ("Weighted Purity", ("ClusteringPurity", "weighted_purity"))
    ])
    
    table_data = []
    for feature in df["feature"].unique():
        feature_df = df[df["feature"] == feature]
        row = {"Feature": config_manager.get_display_name(feature)}
        scores_for_sorting = [] # This list will hold "higher is better" scores for ranking
        for metric_name, (bench, col) in metrics.items():
            score = np.nan
            bench_df = feature_df[(feature_df["benchmark"] == bench) & (feature_df["metric_type"] == "distance_based") & (feature_df["distance"].str.lower() == "cosine")]
            if not bench_df.empty and col in bench_df.columns and pd.notna(bench_df[col].iloc[0]): score = bench_df[col].iloc[0]
            elif bench == "ClusteringPurity": # Special case for feature-based metric
                bench_df_fb = feature_df[(feature_df["benchmark"] == bench) & (feature_df["metric_type"] == "feature_based")]
                if not bench_df_fb.empty and col in bench_df_fb.columns and pd.notna(bench_df_fb[col].iloc[0]): score = bench_df_fb[col].iloc[0]
            
            row[metric_name] = score 
            
            if pd.notna(score):
                if metric_name in ["GSR", "P@1", "P@5", "Weighted Purity", "CSR"]:
                    scores_for_sorting.append(score) # Already 0-1, higher is better
                elif metric_name in ["CS", "CSCF"]:
                    scores_for_sorting.append(1 - score) # Transform 0-1, lower is better -> higher is better
        
        row["_sort_score"] = np.mean(scores_for_sorting) if scores_for_sorting else -np.inf
        table_data.append(row)

    if not table_data: return None
    result = pd.DataFrame(table_data).sort_values("_sort_score", ascending=False).drop(columns="_sort_score").set_index("Feature")
    ordered_cols = [col for col in metrics if col in result.columns]
    result = result[ordered_cols]
    
    for col_name in result.columns:
        if col_name in ["GSR", "P@1", "P@5", "Weighted Purity", "CSR"]:
            result[col_name] = result[col_name].apply(lambda x: format_number(x, 1, True))
        elif col_name in ["CS", "CSCF"]:
            result[col_name] = result[col_name].apply(lambda x: format_number(1 - x if pd.notna(x) else np.nan, 1, True))

    # All metrics are now "higher is better" for bolding
    return bold_best_in_columns(result, ordered_cols, {k: True for k in result.columns})
def generate_full_results_table(
    df: DataFrame,
    metric_name_key: str,
    benchmark_name: str,
    metric_col: str,
    is_percent: bool,
    is_higher_better: bool,
    subsets: List[str],
    config_manager: "ConfigManager",
) -> Optional[DataFrame]:
    """
    Generates a DataFrame for a specific metric across features, distances, and subsets.
    This function handles both distance-based and feature-based metrics correctly.
    """
    if df.empty:
        return None
    logger.info(f"Generating Appendix Table for: {metric_name_key}")

    rows_data = []
    features = config_manager.get_ordered_features()
    distances = ["cosine", "euclidean", "spearman"]
    
    # Determine if the benchmark is exclusively feature-based
    # A simple heuristic: check if it *ever* appears as feature_based in the data
    is_feature_based_metric = not df[df["benchmark"] == benchmark_name]["distance"].dropna().any()

    for feature_name in features:
        df_feature = df[df["feature"] == feature_name]
        if df_feature.empty:
            continue

        if is_feature_based_metric:
            # Special handling for feature-based metrics like ClusteringPurity
            row = {
                "Method": config_manager.get_display_name(feature_name),
                "Dist": "-",
            }
            subset_scores, blind_scores, has_data = [], [], False
            for subset in subsets:
                df_metric = df_feature[
                    (df_feature["subset"] == subset) &
                    (df_feature["metric_type"] == "feature_based") &
                    (df_feature["benchmark"] == benchmark_name)
                ]
                score = np.nan
                if not df_metric.empty and metric_col in df_metric.columns:
                    val = df_metric[metric_col].iloc[0]
                    if pd.notna(val):
                        score = float(val)
                        has_data = True
                row[subset] = score
                if pd.notna(score):
                    subset_scores.append(score)
                    if subset in BLIND_TEST_SUBSETS:
                        blind_scores.append(score)
            
            if has_data:
                row["Avg"] = np.mean(subset_scores) if subset_scores else np.nan
                row["Avg (Blind)"] = np.mean(blind_scores) if blind_scores else np.nan
                sort_val = row["Avg (Blind)"] if pd.notna(row["Avg (Blind)"]) else row["Avg"] if pd.notna(row["Avg"]) else (-np.inf if is_higher_better else np.inf)
                row["_sort_score"] = sort_val * (1 if is_higher_better else -1)
                rows_data.append(row)
        else:
            # Standard handling for distance-based metrics
            for dist_name in distances:
                row = {
                    "Method": config_manager.get_display_name(feature_name),
                    "Dist": config_manager.get_display_name(dist_name, "distance"),
                }
                subset_scores, blind_scores, has_data = [], [], False
                for subset in subsets:
                    df_metric = df_feature[
                        (df_feature["subset"] == subset) &
                        (df_feature["metric_type"] == "distance_based") &
                        (df_feature["distance"].str.lower() == dist_name.lower()) &
                        (df_feature["benchmark"] == benchmark_name)
                    ]
                    score = np.nan
                    if not df_metric.empty and metric_col in df_metric.columns:
                        val = df_metric[metric_col].iloc[0]
                        if pd.notna(val):
                            score = float(val)
                            has_data = True
                    row[subset] = score
                    if pd.notna(score):
                        subset_scores.append(score)
                        if subset in BLIND_TEST_SUBSETS:
                            blind_scores.append(score)
                
                if has_data:
                    row["Avg"] = np.mean(subset_scores) if subset_scores else np.nan
                    row["Avg (Blind)"] = np.mean(blind_scores) if blind_scores else np.nan
                    sort_val = row["Avg (Blind)"] if pd.notna(row["Avg (Blind)"]) else row["Avg"] if pd.notna(row["Avg"]) else (-np.inf if is_higher_better else np.inf)
                    row["_sort_score"] = sort_val * (1 if is_higher_better else -1)
                    rows_data.append(row)

    if not rows_data:
        return None

    result_df = pd.DataFrame(rows_data).sort_values("_sort_score", ascending=False).drop(columns="_sort_score").set_index(["Method", "Dist"])
    result_df = result_df.reindex(columns=VOCSIM_APPENDIX_S_COLUMN_ORDER)

    # Create a numeric version for bolding calculations
    numeric_df = result_df.copy()
    if is_percent:
        numeric_df = numeric_df.applymap(lambda x: x * 100 if pd.notna(x) else np.nan)

    # Format the display DataFrame
    for col in result_df.columns:
        result_df[col] = result_df[col].apply(lambda x: format_number(x, 1, is_percent))

    # Apply bolding based on the numeric DataFrame
    for col in VOCSIM_APPENDIX_S_COLUMN_ORDER:
        if col not in numeric_df.columns:
            continue
        valid_vals = numeric_df[col].dropna()
        if valid_vals.empty:
            continue
        best_val = valid_vals.max() if is_higher_better else valid_vals.min()
        for idx in result_df.index:
            if pd.notna(numeric_df.loc[idx, col]) and np.isclose(numeric_df.loc[idx, col], best_val):
                result_df.loc[idx, col] = bold_string(result_df.loc[idx, col])

    return result_df

def generate_avian_perception_table(df: DataFrame, config_manager: ConfigManager) -> Optional[DataFrame]:
    if df.empty: return None
    logger.info("Generating Avian Perception Table (Triplet Acc. High)")
    data = []
    features = config_manager.get_ordered_features()
    distances = ["cosine", "euclidean", "spearman"]
    for feature_name in features:
        feature_df = df[df["feature"] == feature_name]
        for dist_name in distances:
            dist_df = feature_df[feature_df["distance"].str.lower() == dist_name.lower()]
            if dist_df.empty: continue
            score = dist_df["triplet_high_accuracy"].iloc[0] if "triplet_high_accuracy" in dist_df.columns else np.nan
            if pd.notna(score):
                method_name = f"{config_manager.get_display_name(feature_name)} ({config_manager.get_display_name(dist_name, 'distance')})"
                data.append({"Method": method_name, "Triplet Acc. (High)": score})
    data.extend([{"Method": k, "Triplet Acc. (High)": v} for k, v in ZANDBERG_RESULTS.items()])
    if not data: return None
    result_df = pd.DataFrame(data).sort_values("Triplet Acc. (High)", ascending=False).set_index("Method")
    result_df["Triplet Acc. (High)"] = result_df["Triplet Acc. (High)"].apply(lambda x: format_number(x, 1, True))
    return bold_best_in_columns(result_df, ["Triplet Acc. (High)"], {"Triplet Acc. (High)": True})


def generate_mouse_strain_table(df_strain: pd.DataFrame, features: List[str], goffinet_features: Dict, classifiers: Dict, config_manager: ConfigManager, metric_name="accuracy_mean", std_dev_name="accuracy_std") -> Optional[DataFrame]:
    logger.info("Generating Mouse Strain table with FEATURES AS ROWS, classifiers as columns.")
    all_rows = [config_manager.get_display_name(f) for f in features] + list(goffinet_features.values())
    all_rows = sorted(list(set(all_rows)))
    
    column_tuples = [(g, s) for g, s_map in classifiers.items() for s in s_map.keys()]
    result_df = pd.DataFrame(index=all_rows, columns=pd.MultiIndex.from_tuples(column_tuples))
    result_df.index.name = "Method"

    # Populate with our results
    for f_name in features:
        f_display = config_manager.get_display_name(f_name)
        df_feature = df_strain[df_strain["feature"] == f_name]
        if df_feature.empty: continue
        for clf_group, specific_configs in classifiers.items():
            for specific_name, match_details in specific_configs.items():
                target_type, target_params = match_details["type_match"], match_details["params_to_match"]
                
                best_run = None
                best_score = -np.inf
                for _, row in df_feature.iterrows():
                    parsed_type, parsed_params = parse_benchmark_params(row.get("benchmark", ""))
                    if parsed_type == target_type:
                        is_match = all(_compare_params(parsed_params.get(k), v) for k, v in target_params.items())
                        if is_match:
                            score = row.get(metric_name, -np.inf)
                            if pd.notna(score) and score > best_score:
                                best_score = score
                                best_run = row
                
                if best_run is not None:
                    mean_val = best_run.get(metric_name, np.nan)
                    std_val = best_run.get(std_dev_name, np.nan)
                    val_str = f"{format_number(mean_val, 1)} ({format_number(std_val, 1)})" if pd.notna(mean_val) and pd.notna(std_val) else format_number(mean_val, 1)
                    result_df.loc[f_display, (clf_group, specific_name)] = val_str

    # Populate with Goffinet's results
    for goffinet_key, goffinet_display in goffinet_features.items():
        for clf_group, specific_configs in classifiers.items():
            for specific_name in specific_configs.keys():
                goffinet_clf_key = f"{clf_group} ({specific_name})"
                score_str = GOFFINET_STRAIN_DATA.get(goffinet_clf_key, {}).get(goffinet_key, "-")
                result_df.loc[goffinet_display, (clf_group, specific_name)] = score_str
                
    result_df = result_df.fillna("-")
    return bold_overall_best_in_group_df(result_df, result_df.columns.tolist(), higher_is_better=True, n_best=1)


def generate_mouse_identity_table(df_identity: pd.DataFrame, features: List[str], mlp_configs: Dict, goffinet_features: Dict, config_manager: ConfigManager) -> Optional[DataFrame]:
    logger.info("Generating Mouse Identity Classification Table.")
    all_rows = [config_manager.get_display_name(f) for f in features] + list(goffinet_features.values())
    all_rows = sorted(list(set(all_rows)))

    metric_types = ["Top-1 accuracy", "Top-5 accuracy"]
    column_tuples = [(mlp_disp, metric_disp) for mlp_disp in mlp_configs.keys() for metric_disp in metric_types]
    result_df = pd.DataFrame(index=all_rows, columns=pd.MultiIndex.from_tuples(column_tuples))
    result_df.index.name = "Feature Set"

    # Populate with our results
    for f_name in features:
        f_display = config_manager.get_display_name(f_name)
        df_feature = df_identity[df_identity["feature"] == f_name]
        if df_feature.empty: continue
        for mlp_disp, mlp_params in mlp_configs.items():
            best_run = None
            best_score = -np.inf
            for _, row in df_feature.iterrows():
                parsed_type, parsed_params = parse_benchmark_params(row.get("benchmark", ""))
                if parsed_type == "mlp":
                    is_match = all(_compare_params(parsed_params.get(k), v) for k, v in mlp_params.items())
                    if is_match:
                        score = row.get("accuracy_mean", -np.inf)
                        if pd.notna(score) and score > best_score:
                            best_score = score
                            best_run = row
            if best_run is not None:
                top1_mean = best_run.get("accuracy_mean", np.nan)
                top1_std = best_run.get("accuracy_std", np.nan)
                top5_mean = best_run.get("top_5_accuracy_mean", np.nan)
                top5_std = best_run.get("top_5_accuracy_std", np.nan)
                val_top1 = f"{format_number(top1_mean, 1)} ({format_number(top1_std, 1)})" if pd.notna(top1_mean) and pd.notna(top1_std) else format_number(top1_mean, 1)
                val_top5 = f"{format_number(top5_mean, 1)} ({format_number(top5_std, 1)})" if pd.notna(top5_mean) and pd.notna(top5_std) else format_number(top5_mean, 1)
                result_df.loc[f_display, (mlp_disp, "Top-1 accuracy")] = val_top1
                result_df.loc[f_display, (mlp_disp, "Top-5 accuracy")] = val_top5
    
    # Populate with Goffinet's results
    for goffinet_key, goffinet_display in goffinet_features.items():
        for mlp_disp in mlp_configs.keys():
            score_top1 = GOFFINET_IDENTITY_DATA["Top-1 accuracy"].get(mlp_disp, {}).get(goffinet_key, "-")
            score_top5 = GOFFINET_IDENTITY_DATA["Top-5 accuracy"].get(mlp_disp, {}).get(goffinet_key, "-")
            result_df.loc[goffinet_display, (mlp_disp, "Top-1 accuracy")] = score_top1
            result_df.loc[goffinet_display, (mlp_disp, "Top-5 accuracy")] = score_top5

    result_df = result_df.fillna("-")
    top1_cols = [(mlp_disp, "Top-1 accuracy") for mlp_disp in mlp_configs.keys() if (mlp_disp, "Top-1 accuracy") in result_df.columns]
    top5_cols = [(mlp_disp, "Top-5 accuracy") for mlp_disp in mlp_configs.keys() if (mlp_disp, "Top-5 accuracy") in result_df.columns]
    result_df = bold_overall_best_in_group_df(result_df, top1_cols, higher_is_better=True, n_best=1)
    result_df = bold_overall_best_in_group_df(result_df, top5_cols, higher_is_better=True, n_best=1)
    return result_df

def generate_averaged_correlation_table(df: DataFrame, config_manager: ConfigManager, methods_to_include: Optional[List[str]] = None) -> Optional[DataFrame]:
    """
    Generates a Spearman correlation matrix by averaging correlations from each public subset.
    Can be filtered to only include a specific list of methods.
    """
    if df.empty: return None
    logger.info("Generating averaged metric correlation table...")

    # Create a consistent 'method' identifier for filtering
    df['method'] = df['feature'] + ' | ' + df['distance']
    
    public_subsets = [s for s in df['subset'].unique() if s not in BLIND_TEST_SUBSETS]
    if not public_subsets:
        logger.warning("No public subsets found to calculate correlations.")
        return None

    subset_corr_matrices = []
    for subset in public_subsets:
        subset_df = df[df['subset'] == subset].copy()

        # If a list of methods is provided, filter for them
        if methods_to_include:
            subset_df = subset_df[subset_df['method'].isin(methods_to_include)]

        if subset_df.empty:
            continue

        # Aggregate scores for each method within this subset
        agg_data = []
        for method in subset_df['method'].unique():
            method_df = subset_df[subset_df['method'] == method]
            method_scores = {'Method': method}
            has_any_score = False
            for metric_name, config in METRICS_FOR_CORRELATION.items():
                # Find the score for this specific benchmark and column
                score_df = method_df[method_df['benchmark'] == config['benchmark']]
                
                # Special handling for feature-based metrics like Clustering Purity
                if config['benchmark'] == 'ClusteringPurity':
                    score_df = method_df[
                        (method_df['benchmark'] == config['benchmark']) &
                        (method_df['metric_type'] == 'feature_based')
                    ]
                
                if not score_df.empty and config['column'] in score_df.columns:
                    raw_score = score_df[config['column']].iloc[0]
                    if pd.notna(raw_score):
                        # Apply the transformation (e.g., 1 - x)
                        method_scores[metric_name] = config['transform'](raw_score)
                        has_any_score = True
                    else:
                        method_scores[metric_name] = np.nan
                else:
                    method_scores[metric_name] = np.nan
            
            if has_any_score:
                agg_data.append(method_scores)

        if not agg_data:
            continue

        scores_df = pd.DataFrame(agg_data).set_index('Method')
        
        # Drop methods that don't have scores for all metrics, ensuring a square matrix
        scores_df.dropna(inplace=True)

        if len(scores_df) > 1: # Need at least 2 data points to correlate
            corr_matrix = scores_df.corr(method='spearman')
            subset_corr_matrices.append(corr_matrix)

    if not subset_corr_matrices:
        logger.warning("Could not generate correlation matrices for any subset.")
        return None

    # Average all the collected correlation matrices element-wise
    average_corr_matrix = pd.concat(subset_corr_matrices).groupby(level=0).mean()
    
    # Reorder to match the defined order
    ordered_metrics = list(METRICS_FOR_CORRELATION.keys())
    average_corr_matrix = average_corr_matrix.reindex(index=ordered_metrics, columns=ordered_metrics)
    
    formatted_matrix = average_corr_matrix.applymap(lambda x: f"{x:.2f}")
    formatted_matrix.index.name = "Metric"
    return formatted_matrix
import logging
from pathlib import Path
import sys
import argparse
from typing import List
import os

try:
    script_path = Path(__file__).resolve()
    project_root = script_path.parents[2]
    expected_vocsim_dir = project_root / "vocsim"
    if not expected_vocsim_dir.is_dir() and not (project_root / "vocsim" / "runner.py").exists():
        project_root = Path.cwd()
        print(f"WARN: Assuming CWD project root: {project_root}")
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
        print(f"INFO: Added project root: {project_root}")
except NameError:
    project_root = Path.cwd()
    sys.path.insert(0, str(project_root))
    print(f"INFO: Assuming CWD project root for interactive session: {project_root}")

from vocsim.runner import PipelineRunner
from utils.config_loader import load_config
from utils.logging_utils import setup_logging

CONFIG_NAME = "vocsim_paper.yaml"
BASE_CONFIG_NAME = "base.yaml"
CONFIG_DIR = project_root / "reproducibility" / "configs"
BASE_CONFIG_DIR = project_root / "configs"


def main(steps_to_run: List[str]):
    """
    Main function to run the VocSim pipeline stages.
    """
    print("DEBUG: main() function started.")  

    config_path = CONFIG_DIR / CONFIG_NAME
    base_config_path = BASE_CONFIG_DIR / BASE_CONFIG_NAME
    if not config_path.exists():
        print(f"ERROR: Config not found: {config_path}")
        sys.exit(1)

    print(f"DEBUG: Attempting to load config: {config_path}")  
    cfg = load_config(config_path, base_config_path=base_config_path if base_config_path.exists() else None)
    print("DEBUG: Config loaded successfully.")  

    log_config = cfg.get("logging", {})
    log_config.setdefault("log_dir", project_root / "logs")
    log_config.setdefault("log_file", "vocsim_run.log")

    print("DEBUG: Attempting to set up logging.")  
    setup_logging(log_config)
    print("DEBUG: Logging setup complete.")  

    logger = logging.getLogger(__name__)
    logger.info(f"Loaded config from {config_path}" + (f" and merged with {base_config_path}" if base_config_path.exists() else ""))
    logger.info(f"Executing pipeline steps: {steps_to_run}")

    if Path.cwd() != project_root:
        logger.warning("Running from '%s', changing CWD to project root '%s'.", Path.cwd(), project_root)
        os.chdir(project_root)

    try:
        print("DEBUG: Initializing PipelineRunner.")  
        runner = PipelineRunner(cfg)
        print("DEBUG: PipelineRunner initialized. Starting run...")  
        runner.run(steps=steps_to_run)
        logger.info("VocSim script finished successfully.")
        print("DEBUG: Pipeline run finished successfully.") 
    except Exception as e:
        logger.error("Error in VocSim run: %s", e, exc_info=True)
        print(f"DEBUG: An exception occurred: {e}") 
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run VocSim Pipeline Stages.")
    parser.add_argument(
        "--steps",
        nargs="+",
        default=["benchmarks"],
        choices=["train", "features", "distances", "benchmarks", "all"],
        help=(
            "Which pipeline stages to run. 'train': Train models. 'features': Extract/cache features."
            " 'distances': Compute/cache distances. 'benchmarks': Run benchmarks. 'all': Run all stages"
            " (train, features, distances, benchmarks). Steps are executed in a fixed order (train -> features ->"
            " distances -> benchmarks). If a step is skipped, subsequent steps will try to use cached outputs."
        ),
    )
    args = parser.parse_args()

    if "all" in args.steps:
        selected_steps = ["train", "features", "distances", "benchmarks"]
    else:
        ordered_steps = []
        step_order = ["train", "features", "distances", "benchmarks"]
        for step in step_order:
            if step in args.steps:
                ordered_steps.append(step)
        selected_steps = ordered_steps

    main(selected_steps)# -*- coding: utf-8 -*-
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm.auto import tqdm

from reproducibility.models.autoencoder import Autoencoder
from trainers.base import Trainer

try:
    from utils.torch_utils import check_tensor
except ImportError:
    logger = logging.getLogger(__name__)
    logger.warning("utils.torch_utils.check_tensor not found. NaN checks disabled.")

    def check_tensor(tensor, name):
        return True


logger = logging.getLogger(__name__)


def ae_collate_fn(batch: List[Dict[str, Any]]) -> Tuple[Optional[torch.Tensor], List[Optional[Any]]]:
    """
    Collate function for the Autoencoder DataLoader.

    Pads audio tensors to the maximum length in the batch and collects metadata.
    Handles potential errors or missing data in items.

    Args:
        batch (List[Dict[str, Any]]): A list of dataset items (dictionaries).

    Returns:
        Tuple[Optional[torch.Tensor], List[Optional[Any]]]: A tuple containing:
            - Padded audio tensor [batch_size, max_len], or empty tensor if no valid audio.
            - List of metadata/IDs corresponding to items in the batch.
    """
    audio_list = []
    metadata_list = []
    max_len = 0
    for item in batch:
        audio_info = item.get("audio")
        meta = item.get("metadata_or_id", None)
        if not audio_info or "array" not in audio_info or audio_info["array"] is None:
            metadata_list.append(meta)
            continue
        audio_data = audio_info["array"]
        if not isinstance(audio_data, torch.Tensor):
            if isinstance(audio_data, np.ndarray):
                if audio_data.dtype != np.float32:
                    audio_data = audio_data.astype(np.float32)
                audio_data = torch.from_numpy(audio_data)
            elif isinstance(audio_data, list):
                audio_data = torch.tensor(audio_data, dtype=torch.float32)
            else:
                metadata_list.append(meta)
                continue
        elif audio_data.dtype != torch.float32:
            audio_data = audio_data.float()
        if audio_data.ndim > 1:
            if audio_data.shape[0] > 1 and audio_data.shape[1] > 1:
                audio_data = torch.mean(audio_data, dim=0)
            elif audio_data.shape[1] == 1:
                audio_data = audio_data.squeeze(1)
            elif audio_data.shape[0] == 1:
                audio_data = audio_data.squeeze(0)
        if audio_data.ndim != 1:
            logger.warning("Audio bad dims %d. Skip.", audio_data.ndim)
            metadata_list.append(meta)
            continue
        audio_list.append(audio_data)
        metadata_list.append(meta)
        if len(audio_data) > max_len:
            max_len = len(audio_data)
    if not audio_list:
        return torch.empty((0, 0), dtype=torch.float32), []
    try:
        padded_audio = pad_sequence([t.cpu() for t in audio_list], batch_first=True, padding_value=0.0)
        return padded_audio, metadata_list
    except Exception as e:
        logger.error("Pad failed: %s", e, exc_info=True)
        return torch.empty((0, 0), dtype=torch.float32), []


class PaperAutoencoderTrainer(Trainer):
    """
    Trains the specific Autoencoder architecture used in the paper.
    """

    def _initialize_components(self):
        """
        Initializes optimizer, scheduler, criterion, and TensorBoard writer.
        """
        logger.info("Initializing components for PaperAutoencoderTrainer...")
        lr = self.config.get("learning_rate", 0.0003)
        weight_decay = self.config.get("weight_decay", 0.01)
        betas = tuple(self.config.get("betas", (0.9, 0.999)))
        scheduler_mode = self.config.get("scheduler_mode", "min")
        scheduler_factor = self.config.get("scheduler_factor", 0.5)
        scheduler_patience = self.config.get("scheduler_patience", 5)
        self.mixed_precision = self.config.get("mixed_precision", True)
        self.grad_accum_steps = self.config.get("gradient_accumulation_steps", 1)
        self.reg_weight = self.config.get("regularization_weight", 0.01)
        self.model.to(self.device)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)
        logger.debug("Optimizer: AdamW (lr=%f, wd=%f, betas=%s)", lr, weight_decay, betas)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode=scheduler_mode, factor=scheduler_factor, patience=scheduler_patience, verbose=True
        )
        logger.debug(
            "Scheduler: ReduceLROnPlateau (mode=%s, factor=%f, patience=%d)",
            scheduler_mode,
            scheduler_factor,
            scheduler_patience,
        )
        self.criterion = nn.L1Loss()
        logger.debug("Criterion: L1Loss")
        if self.mixed_precision and self.device.type == "cuda":
            self.scaler = torch.cuda.amp.GradScaler()
            logger.debug("Mixed precision enabled with GradScaler.")
        else:
            self.scaler = None
        self.current_epoch = 0
        self.best_loss = float("inf")
        self.early_stopping_counter = 0
        self.writer = SummaryWriter(log_dir=str(self.logs_dir))

    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):
        """
        Execute the training loop for the paper's Autoencoder.

        Args:
            train_loader (DataLoader): DataLoader for the training dataset.
            val_loader (Optional[DataLoader]): DataLoader for the validation dataset.
        """
        num_epochs = self.config.get("num_epochs", 50)
        early_stopping_patience = self.config.get("early_stopping_patience", 10)

        logger.info("Starting AE training for %d epochs...", num_epochs)
        if hasattr(train_loader, "batch_size"):
            logger.info("Batch size: %d, Grad Accum: %d", train_loader.batch_size, self.grad_accum_steps)

        nan_batches_skipped_train = 0

        for epoch in range(self.current_epoch, num_epochs):
            self.current_epoch = epoch
            start_time = time.time()

            self.model.train()
            total_train_loss = 0.0
            total_recon_loss = 0.0
            total_reg_loss = 0.0
            processed_batches = 0

            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
            for i, batch in enumerate(pbar):
                if batch is None:
                    continue
                try:
                    audio, _ = batch
                    if audio is None or audio.numel() == 0:
                        continue
                    audio = audio.to(self.device)
                    if not check_tensor(audio, f"Input Batch {i}"):
                        logger.warning("Skipping batch %d due to NaN/Inf in input audio.", i)
                        nan_batches_skipped_train += 1
                        continue
                except Exception as e:
                    logger.error("Error processing batch %d: %s. Skipping.", i, e, exc_info=True)
                    continue

                try:
                    with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                        reconstructed, encoded = self.model(audio)
                        with torch.no_grad():
                            target_mel_spec = self.model.frontend(audio)

                        valid_target = check_tensor(target_mel_spec, f"Target Batch {i}")
                        valid_recon = check_tensor(reconstructed, f"Recon Batch {i}")
                        valid_encoded = check_tensor(encoded, f"Encoded Batch {i}")

                        if not (valid_target and valid_recon and valid_encoded):
                            logger.warning("Skipping batch %d due to NaN/Inf in model outputs.", i)
                            nan_batches_skipped_train += 1
                            if (i + 1) % self.grad_accum_steps != 0:
                                self.optimizer.zero_grad(set_to_none=True)
                            continue

                        recon_loss = self.criterion(reconstructed, target_mel_spec)
                        reg_loss = self.reg_weight * torch.mean(torch.abs(encoded))
                        loss = recon_loss + reg_loss
                        loss = loss / self.grad_accum_steps

                    if self.scaler:
                        self.scaler.scale(loss).backward()
                    else:
                        loss.backward()

                    if (i + 1) % self.grad_accum_steps == 0 or (i + 1) == len(train_loader):
                        if self.scaler:
                            self.scaler.unscale_(self.optimizer)
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                            self.scaler.step(self.optimizer)
                            self.scaler.update()
                        else:
                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                            self.optimizer.step()
                        self.optimizer.zero_grad(set_to_none=True)

                    total_train_loss += loss.item() * self.grad_accum_steps
                    total_recon_loss += recon_loss.item()
                    total_reg_loss += reg_loss.item()
                    processed_batches += 1
                    if processed_batches > 0:
                        pbar.set_postfix(
                            {
                                "loss": f"{total_train_loss / processed_batches:.4f}",
                                "recon": f"{total_recon_loss / processed_batches:.4f}",
                                "reg": f"{total_reg_loss / processed_batches:.4f}",
                                "lr": f"{self.optimizer.param_groups[0]['lr']:.2e}",
                            }
                        )

                except Exception as model_err:
                    logger.error("Error during training step %d: %s. Skipping batch.", i, model_err, exc_info=True)
                    if (i + 1) % self.grad_accum_steps != 0:
                        try:
                            self.optimizer.zero_grad(set_to_none=True)
                        except Exception:
                            pass
                    continue

            if nan_batches_skipped_train > 0:
                logger.warning("Skipped %d batches in TRAIN epoch %d due to NaN/Inf.", nan_batches_skipped_train, epoch + 1)
                nan_batches_skipped_train = 0

            avg_train_loss = total_train_loss / processed_batches if processed_batches > 0 else float("nan")
            avg_recon_loss = total_recon_loss / processed_batches if processed_batches > 0 else float("nan")
            avg_reg_loss = total_reg_loss / processed_batches if processed_batches > 0 else float("nan")
            epoch_time = time.time() - start_time

            logger.info(
                "Epoch %d Train Summary | Time: %.2fs | Avg Loss: %.4f | Recon Loss: %.4f | Reg Loss: %.4f",
                epoch + 1,
                epoch_time,
                avg_train_loss,
                avg_recon_loss,
                avg_reg_loss,
            )
            self.writer.add_scalar("train/loss", avg_train_loss, epoch)
            self.writer.add_scalar("train/recon_loss", avg_recon_loss, epoch)
            self.writer.add_scalar("train/reg_loss", avg_reg_loss, epoch)
            self.writer.add_scalar("train/learning_rate", self.optimizer.param_groups[0]["lr"], epoch)

            avg_val_loss = float("inf")
            nan_batches_skipped_val = 0
            if val_loader:
                self.model.eval()
                total_val_loss = 0.0
                processed_val_batches = 0
                with torch.no_grad():
                    pbar_val = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
                    for batch in pbar_val:
                        if batch is None:
                            continue
                        try:
                            audio, _ = batch
                            if audio is None or audio.numel() == 0:
                                continue
                            audio = audio.to(self.device)
                            if not check_tensor(audio, "Val Input"):
                                logger.warning("NaN/Inf in val input. Skipping batch.")
                                nan_batches_skipped_val += 1
                                continue

                            with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                                reconstructed, encoded = self.model(audio)
                                target_mel_spec = self.model.frontend(audio)

                                if not check_tensor(target_mel_spec, "Val Target") or not check_tensor(reconstructed, "Val Recon") or not check_tensor(encoded, "Val Encoded"):
                                    logger.warning("NaN/Inf in val outputs. Skipping batch.")
                                    nan_batches_skipped_val += 1
                                    continue

                                recon_loss = self.criterion(reconstructed, target_mel_spec)
                                reg_loss = self.reg_weight * torch.mean(torch.abs(encoded))
                                loss = recon_loss + reg_loss
                            total_val_loss += loss.item()
                            processed_val_batches += 1
                            if processed_val_batches > 0:
                                pbar_val.set_postfix({"loss": f"{total_val_loss / processed_val_batches:.4f}"})
                        except Exception as val_err:
                            logger.error("Error during validation batch: %s. Skipping.", val_err, exc_info=True)
                            continue

                if nan_batches_skipped_val > 0:
                    logger.warning("Skipped %d batches in VAL epoch %d due to NaN/Inf.", nan_batches_skipped_val, epoch + 1)

                avg_val_loss = total_val_loss / processed_val_batches if processed_val_batches > 0 else float("nan")
                logger.info("Epoch %d Val Summary | Avg Loss: %.4f", epoch + 1, avg_val_loss)
                self.writer.add_scalar("val/loss", avg_val_loss, epoch)
            else:
                avg_val_loss = avg_train_loss

            if not np.isnan(avg_val_loss) and not np.isinf(avg_val_loss):
                self.scheduler.step(avg_val_loss)
                is_best = avg_val_loss < self.best_loss
                if is_best:
                    self.best_loss = avg_val_loss
                    self.save_model(epoch=epoch + 1, is_best=True)
                    self.early_stopping_counter = 0
                    logger.info("New best model at epoch %d, loss %.4f", epoch + 1, self.best_loss)
                else:
                    self.early_stopping_counter += 1
            else:
                logger.warning("Skipping scheduler/best model check due to non-finite val loss (%f)", avg_val_loss)
                self.early_stopping_counter += 1

            save_freq = self.config.get("save_frequency_epochs", 10)
            if save_freq > 0 and (epoch + 1) % save_freq == 0:
                self.save_model(epoch=epoch + 1, is_best=False)

            if early_stopping_patience > 0 and self.early_stopping_counter >= early_stopping_patience:
                logger.info("Early stopping after %d epochs.", early_stopping_patience)
                break

        logger.info("AE Training finished.")
        self.save_model(epoch=self.current_epoch + 1, is_best=False, final=True)
        self.writer.close()

    def save_model(self, epoch: Optional[int] = None, is_best: bool = False, final: bool = False):
        """
        Saves the model checkpoint.

        Args:
            epoch (Optional[int]): The current epoch number.
            is_best (bool): True if this is the best model so far.
            final (bool): True if this is the final model after training completes.
        """
        if final:
            filename = "final_model.pt"
            super()._save_checkpoint(filename=filename, epoch=epoch, is_best=True)
            return
        elif is_best:
            filename = f"checkpoint_epoch_{epoch}.pt"
            super()._save_checkpoint(filename=filename, epoch=epoch, is_best=True)
            return
        else:
            filename = f"checkpoint_epoch_{epoch}.pt"
            super()._save_checkpoint(filename=filename, epoch=epoch, is_best=False)import functools
import logging
import os
import time
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import yaml
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
from tqdm.auto import tqdm

from reproducibility.models.vae import VariationalAutoencoder, preprocess_vae_input
from trainers.base import Trainer


logger = logging.getLogger(__name__)

X_DIM = 128 * 128


def vae_collate_fn(
    batch: List[Dict[str, Any]],
    target_sr: int,
    n_fft: int,
    hop_length: int,
    win_length: int,
    window_fn_str: str,
    spec_height: int,
    spec_width: int,
    window_samples: int,
    hop_samples: int,
) -> Tuple[Optional[torch.Tensor], List[Optional[Any]]]:
    """
    Collate function for VAE training.

    Args:
        batch (List[Dict[str, Any]]): A list of dataset items (dictionaries).
        target_sr (int): Target sample rate for resampling.
        n_fft (int): FFT size for spectrogram calculation.
        hop_length (int): Hop length for spectrogram calculation.
        win_length (int): Window length for spectrogram calculation.
        window_fn_str (str): Name of the window function ('hann', 'hamming').
        spec_height (int): Target spectrogram height (number of Mel bins).
        spec_width (int): Target spectrogram width (number of time frames).
        window_samples (int): Number of audio samples corresponding to the desired spectrogram window width.
        hop_samples (int): Number of audio samples corresponding to the desired hop between spectrogram windows.

    Returns:
        Tuple[Optional[torch.Tensor], List[Optional[Any]]]: A tuple containing:
            - Padded spectrogram chunks tensor [batch_size, spec_height, spec_width], or empty tensor.
            - List of metadata/IDs corresponding to original items.
    """
    all_processed_chunks = []
    metadata_list = []
    preprocess_device = torch.device("cpu")

    for item in batch:
        audio_info = item.get("audio")
        metadata = item.get("metadata_or_id", None)
        if not audio_info or "array" not in audio_info or "sampling_rate" not in audio_info:
            metadata_list.append(metadata)
            continue
        audio_data = audio_info["array"]
        sample_rate = audio_info["sampling_rate"]
        if audio_data is None or sample_rate is None:
            metadata_list.append(metadata)
            continue

        try:
            processed_chunks = preprocess_vae_input(
                audio_tensor=audio_data,
                sample_rate=sample_rate,
                target_sr=target_sr,
                n_fft=n_fft,
                hop_length=hop_length,
                win_length=win_length,
                window_fn_str=window_fn_str,
                spec_height=spec_height,
                spec_width=spec_width,
                window_samples=window_samples,
                hop_samples=hop_samples,
                device=preprocess_device,
            )
            all_processed_chunks.extend(processed_chunks)
            metadata_list.extend([metadata] * len(processed_chunks))
        except Exception as e:
            metadata_list.append(metadata)
            continue

    if not all_processed_chunks:
        return (
            torch.empty((0, spec_height, spec_width), device=preprocess_device),
            [],
        )

    try:
        stacked_chunks = torch.cat(all_processed_chunks, dim=0)
        return stacked_chunks, metadata_list
    except Exception as e:
        return (
            torch.empty((0, spec_height, spec_width), device=preprocess_device),
            [],
        )


class PaperVAETrainer(Trainer):
    """
    Trains the specific Variational Autoencoder architecture used in the paper.
    """

    def _initialize_components(self):
        """
        Initialize optimizer, loss (ELBO), etc. for the paper's VAE.
        """
        logger.info("Initializing components for PaperVAETrainer...")

        self.vae_params = self.config.get("vae_frontend_params", {})
        self.target_sr = self.vae_params.get("target_sr", 16000)
        self.n_fft = self.vae_params.get("n_fft", 512)
        self.hop_length = self.vae_params.get("hop_length", 256)
        self.win_length = self.vae_params.get("win_length", self.n_fft)
        self.window_fn_str = self.vae_params.get("window_fn_str", "hann")
        self.spec_height = self.vae_params.get("spec_height", 128)
        self.spec_width = self.vae_params.get("spec_width", 128)
        window_overlap = self.vae_params.get("window_overlap", 0.5)
        self.window_samples = (self.spec_width - 1) * self.hop_length
        self.hop_samples = int(self.window_samples * (1 - window_overlap))

        lr = self.config.get("learning_rate", 1e-3)
        self.test_freq = self.config.get("test_frequency", 25)
        self.save_freq = self.config.get("save_frequency_epochs", 10)
        self.mixed_precision = self.config.get("mixed_precision", False)

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
        logger.debug("Optimizer: Adam (lr=%f)", lr)
        self.criterion = None
        logger.debug("Criterion: Negative ELBO (calculated in VAE forward)")

        if self.mixed_precision and self.device.type == "cuda":
            self.scaler = torch.cuda.amp.GradScaler()
            logger.debug("Mixed precision enabled with GradScaler.")
        else:
            self.scaler = None
            if self.mixed_precision and self.device.type == "cpu":
                logger.warning("Mixed precision requested but device is CPU. Disabling.")
                self.mixed_precision = False

        self.current_epoch = 0
        self.best_val_loss = float("inf")
        self.loss_history = {"train": {}, "val": {}}
        self.writer = SummaryWriter(log_dir=str(self.logs_dir))

    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):
        """
        Execute the training loop for the paper's VAE.

        Args:
            train_loader (DataLoader): DataLoader for the training dataset.
            val_loader (Optional[DataLoader]): DataLoader for the validation dataset.
        """
        num_epochs = self.config.get("num_epochs", 50)

        logger.info("Starting VAE training for %d epochs...", num_epochs)
        if hasattr(train_loader, "batch_size"):
            logger.info("Batch size: %d", train_loader.batch_size)

        for epoch in range(self.current_epoch, num_epochs):
            self.current_epoch = epoch
            start_time = time.time()

            self.model.train()
            total_train_loss = 0.0
            total_train_samples = 0

            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
            for batch_idx, batch in enumerate(pbar):
                data, _ = batch
                if data is None or data.numel() == 0:
                    continue

                data = data.to(self.device)

                batch_num_samples = data.size(0)
                if batch_num_samples == 0:
                    continue
                total_train_samples += batch_num_samples

                self.optimizer.zero_grad()

                with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                    loss = self.model(data)

                if self.scaler:
                    self.scaler.scale(loss).backward()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    loss.backward()
                    self.optimizer.step()

                total_train_loss += loss.item() * batch_num_samples
                if total_train_samples > 0:
                    pbar.set_postfix({"avg_chunk_loss": f"{total_train_loss / total_train_samples:.4f}"})

            avg_train_loss = total_train_loss / total_train_samples if total_train_samples > 0 else float("inf")
            self.loss_history["train"][epoch] = avg_train_loss

            epoch_time = time.time() - start_time
            logger.info(
                "Epoch %d Train Summary | Time: %.2fs | Avg Loss (Neg ELBO per chunk): %.4f",
                epoch + 1,
                epoch_time,
                avg_train_loss,
            )
            self.writer.add_scalar("train/loss_per_chunk", avg_train_loss, epoch)
            self.writer.add_scalar("train/learning_rate", self.optimizer.param_groups[0]["lr"], epoch)

            if val_loader and (epoch + 1) % self.test_freq == 0:
                self.model.eval()
                total_val_loss = 0.0
                total_val_samples = 0
                with torch.no_grad():
                    pbar_val = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
                    for batch in pbar_val:
                        data, _ = batch
                        if data is None or data.numel() == 0:
                            continue
                        data = data.to(self.device)
                        batch_num_samples = data.size(0)
                        if batch_num_samples == 0:
                            continue
                        total_val_samples += batch_num_samples

                        with torch.cuda.amp.autocast(enabled=self.mixed_precision):
                            loss = self.model(data)

                        total_val_loss += loss.item() * batch_num_samples
                        if total_val_samples > 0:
                            pbar_val.set_postfix({"avg_chunk_loss": f"{total_val_loss / total_val_samples:.4f}"})

                avg_val_loss = total_val_loss / total_val_samples if total_val_samples > 0 else float("inf")
                self.loss_history["val"][epoch] = avg_val_loss
                logger.info("Epoch %d Val Summary | Avg Loss (Neg ELBO per chunk): %.4f", epoch + 1, avg_val_loss)
                self.writer.add_scalar("val/loss_per_chunk", avg_val_loss, epoch)

                if avg_val_loss < self.best_val_loss:
                    self.best_val_loss = avg_val_loss
                    logger.info("New best validation loss: %.4f", self.best_val_loss)
                    self.save_model(epoch=epoch + 1, is_best=True)

            if (epoch + 1) % self.save_freq == 0 and epoch > 0:
                self.save_model(epoch=epoch + 1, is_best=False)

        logger.info("VAE Training finished.")
        self.save_model(epoch=self.current_epoch + 1, is_best=False, final=True)
        self.writer.close()

    def save_model(self, epoch: Optional[int] = None, is_best: bool = False, final: bool = False):
        """
        Saves the VAE model checkpoint.

        Args:
            epoch (Optional[int]): The current epoch number.
            is_best (bool): True if this is the best model so far.
            final (bool): True if this is the final model after training completes.
        """
        filename = "final_model.pt" if final else f"checkpoint_epoch_{epoch}.pt"
        checkpoint_data = {
            "z_dim": self.model.z_dim,
            "model_precision": self.model.model_precision,
            "lr": self.model.lr,
            "vae_frontend_params": {
                "target_sr": self.target_sr,
                "n_fft": self.n_fft,
                "hop_length": self.hop_length,
                "win_length": self.win_length,
                "window_fn_str": self.window_fn_str,
                "spec_height": self.spec_height,
                "spec_width": self.spec_width,
                "window_overlap": self.config.get("vae_frontend_params", {}).get("window_overlap", 0.5),
            },
        }
        self._save_checkpoint(filename, epoch, is_best, extra_data=checkpoint_data)

    def _save_checkpoint(
        self,
        filename: str,
        epoch: Optional[int],
        is_best: bool,
        extra_data: Optional[Dict] = None,
    ):
        """
        Helper method to save a checkpoint dictionary including VAE params.

        Args:
            filename (str): The name of the checkpoint file.
            epoch (Optional[int]): The current epoch number.
            is_best (bool): True if this is the best model checkpoint.
            extra_data (Optional[Dict]): Additional data to include in the checkpoint.
        """
        if self.optimizer is None:
            logger.warning("Optimizer not initialized, cannot save its state.")

        base_checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict() if self.optimizer else None,
            "scheduler_state_dict": None,
            "config": self.config,
            "scaler_state_dict": self.scaler.state_dict() if self.scaler else None,
            "best_val_loss": self.best_val_loss,
            "loss_history": self.loss_history,
        }
        if extra_data:
            base_checkpoint.update(extra_data)

        filepath = self.checkpoints_dir / filename
        torch.save(base_checkpoint, filepath)
        logger.info("Checkpoint saved to %s", filepath)

        if is_best:
            best_filepath = self.output_dir / "final_model.pt"
            torch.save(base_checkpoint, best_filepath)
            logger.info("Best model checkpoint saved to %s", best_filepath)# -*- coding: utf-8 -*-
"""
VocSim Benchmark: Paper Reproducibility Model Trainers.

This package contains trainer implementations specific to the
models trained in the original paper (e.g., AE, VAE).
"""

from .autoencoder import PaperAutoencoderTrainer
from .vae import PaperVAETrainer

__all__ = [
    "PaperAutoencoderTrainer",
    "PaperVAETrainer",
]import logging
from abc import ABC, abstractmethod
from typing import Any, Optional, Dict, Union
from pathlib import Path
import torch
from torch.utils.data import DataLoader


logger = logging.getLogger(__name__)


class Trainer(ABC):
    """
    Abstract Base Class for all model trainers.
    """

    def __init__(self, model: torch.nn.Module, config: Dict[str, Any], output_dir: Union[str, Path], device: str = "cpu"):
        """
        Initialize the base trainer.

        Args:
            model (torch.nn.Module): The model to be trained.
            config (Dict[str, Any]): Training configuration parameters (e.g., lr, epochs, batch_size).
            output_dir (Union[str, Path]): Directory to save models and logs.
            device (str): The device to run training on ('cpu' or 'cuda').
        """
        self.model = model.to(torch.device(device))
        self.config = config
        self.output_dir = Path(output_dir)
        self.device = torch.device(device)
        self.checkpoints_dir = self.output_dir / "checkpoints"
        self.logs_dir = self.output_dir / "logs"

        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)

        self._initialize_components()
        logger.info(f"{self.__class__.__name__} initialized. Output Dir: {self.output_dir}, Device: {self.device}")

    def _initialize_components(self):
        """
        Initialize components like optimizer, scheduler, loss function, etc.
        """
        self.optimizer: Optional[torch.optim.Optimizer] = None
        self.scheduler: Optional[torch.optim.lr_scheduler._LRScheduler] = None
        self.criterion: Optional[torch.nn.Module] = None
        self.scaler: Optional[torch.cuda.amp.GradScaler] = None

        logger.warning("Base Trainer _initialize_components called. Subclasses should override this.")

    @abstractmethod
    def train(self, train_loader: DataLoader, val_loader: Optional[DataLoader] = None):
        """
        Execute the training loop.

        Args:
            train_loader (DataLoader): DataLoader for the training dataset.
            val_loader (Optional[DataLoader]): DataLoader for the validation dataset.
        """
        pass

    @abstractmethod
    def save_model(self, epoch: Optional[int] = None, is_best: bool = False):
        """
        Save the current state of the model.

        Args:
            epoch (Optional[int]): The epoch number (used for checkpoint naming).
            is_best (bool): Flag indicating if this is the best model found so far.
        """
        pass

    def _save_checkpoint(self, filename: str, epoch: Optional[int], is_best: bool):
        """
        Helper method to save a checkpoint dictionary.

        Args:
            filename (str): The name of the checkpoint file.
            epoch (Optional[int]): The current epoch number.
            is_best (bool): True if this is the best model checkpoint.
        """
        if self.optimizer is None:
            logger.warning("Optimizer not initialized, cannot save its state.")

        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict() if self.optimizer else None,
            "scheduler_state_dict": self.scheduler.state_dict() if self.scheduler else None,
            "config": self.config,
            "scaler_state_dict": self.scaler.state_dict() if self.scaler else None,
        }
        filepath = self.checkpoints_dir / filename
        torch.save(checkpoint, filepath)
        logger.info("Checkpoint saved to %s", filepath)

        if is_best:
            best_filepath = self.output_dir / "final_model.pt"
            torch.save(checkpoint, best_filepath)
            logger.info("Best model checkpoint saved to %s", best_filepath)# -*- coding: utf-8 -*-
"""
VocSim Benchmark: General Model Training Modules.
"""

from .base import Trainer
__all__ = [
    "Trainer",
]import logging
import yaml
from pathlib import Path
from typing import Dict, Any, Union, Optional, List
import os

logger = logging.getLogger(__name__)


def _deep_update(base_dict: Dict, update_dict: Dict) -> Dict:
    """
    Recursively updates a dictionary with values from another dictionary.
    List values in update_dict completely replace list values in base_dict.

    Args:
        base_dict (Dict): The dictionary to update.
        update_dict (Dict): The dictionary containing updates.

    Returns:
        Dict: The updated dictionary.
    """
    for key, value in update_dict.items():
        if isinstance(value, dict) and key in base_dict and isinstance(base_dict[key], dict):
            base_dict[key] = _deep_update(base_dict[key], value)
        else:
            base_dict[key] = value
    return base_dict


def load_config(config_path: Union[str, Path], base_config_path: Optional[Union[str, Path]] = None) -> Dict[str, Any]:
    """
    Loads the main configuration file and optionally merges it with a base configuration.

    Args:
        config_path (Union[str, Path]): Path to the main YAML configuration file.
        base_config_path (Optional[Union[str, Path]]): Path to a base YAML configuration file.
                                                       Values in the main config override base config values.

    Returns:
        Dict[str, Any]: The loaded (and potentially merged) configuration dictionary with paths resolved.

    Raises:
        FileNotFoundError: If the specified configuration file(s) do not exist.
        yaml.YAMLError: If there is an error parsing the YAML file(s).
    """
    main_path = Path(config_path)
    if not main_path.exists():
        raise FileNotFoundError(f"Main configuration file not found: {main_path}")

    final_config = {}

    if base_config_path:
        base_path = Path(base_config_path)
        if base_path.exists():
            try:
                with open(base_path, "r", encoding="utf-8") as f:
                    base_loaded = yaml.safe_load(f)
                    if base_loaded:
                        final_config = base_loaded
                logger.debug("Loaded base configuration from %s", base_path)
            except yaml.YAMLError as e:
                logger.error("Error parsing base YAML file %s: %s", base_path, e)
                raise
            except Exception as e:
                logger.error("Error reading base config file %s: %s", base_path, e)
                raise
        else:
            logger.warning("Base configuration file specified but not found: %s", base_path)

    try:
        with open(main_path, "r", encoding="utf-8") as f:
            main_config = yaml.safe_load(f)
            if main_config is None:
                main_config = {}
        logger.debug("Loaded main configuration from %s", main_path)

        final_config = _deep_update(final_config, main_config)

    except yaml.YAMLError as e:
        logger.error("Error parsing main YAML file %s: %s", main_path, e)
        raise
    except Exception as e:
        logger.error("Error reading main config file %s: %s", main_path, e)
        raise

    project_root_str = final_config.get("project_root", ".")
    project_root = (main_path.parent / project_root_str).resolve()
    final_config["project_root"] = str(project_root)

    path_keys = ["results_dir", "features_dir", "models_dir", "log_dir", "data_dir", "probe_csv_path", "triplet_csv_path", "model_path"]

    def resolve_path_values(cfg_part):
        if isinstance(cfg_part, dict):
            resolved_dict = {}
            for k, v in cfg_part.items():
                if any(path_key in k for path_key in path_keys) and isinstance(v, str):
                    p = Path(v)
                    if p.is_absolute():
                        resolved_path = p.resolve()
                    else:
                        resolved_path = (project_root / p).resolve()
                    resolved_dict[k] = str(resolved_path)
                else:
                    resolved_dict[k] = resolve_path_values(v)
            return resolved_dict
        elif isinstance(cfg_part, list):
            return [resolve_path_values(elem) for elem in cfg_part]
        else:
            return cfg_part

    final_config = resolve_path_values(final_config)
    logger.debug("Resolved relative paths in configuration.")

    return final_configimport logging
import os
from pathlib import Path
import shutil
from typing import Any, Dict, List, Optional, Tuple, Union
import scipy.io as sio
import torchaudio
import pandas as pd
from datasets import Dataset as HFDataset
from datasets import Audio, Value, ClassLabel, Features
import numpy as np
from tqdm.auto import tqdm
import torch

logger = logging.getLogger(__name__)


def _get_unique_filename(dest_dir: Path, filename: str) -> str:
    """
    Generates a unique filename if a conflict occurs in the destination directory.

    Args:
        dest_dir (Path): The directory where the file will be saved.
        filename (str): The desired filename.

    Returns:
        str: A unique filename.
    """
    base, ext = os.path.splitext(filename)
    counter = 1
    new_filename = filename
    while (dest_dir / new_filename).exists():
        new_filename = f"{base}_{counter}{ext}"
        counter += 1
    return new_filename


def _flatten_mouse_identity_raw_structure(raw_data_dir: Path, target_dir: Path) -> None:
    """
    Flattens the directory structure from the original MUPET mouse identity data.

    Moves all .mat and .wav files from subdirectories into the target_dir.
    This is often necessary before processing MAT files if they reference WAV files
    assuming a flat structure.

    Args:
        raw_data_dir (Path): Path to the directory containing the raw MUPET data.
        target_dir (Path): Path to the directory where files will be moved (flattened structure).
    """
    logger.info("Flattening directory structure from %s to %s...", raw_data_dir, target_dir)
    target_dir.mkdir(parents=True, exist_ok=True)
    moved_count = 0
    skipped_count = 0

    for dirpath, _, filenames in os.walk(raw_data_dir):
        current_dir = Path(dirpath)
        if current_dir == raw_data_dir or current_dir == target_dir:
            continue

        for filename in filenames:
            if filename.endswith((".mat", ".wav")):
                src_path = current_dir / filename
                dest_filename = _get_unique_filename(target_dir, filename)
                dest_path = target_dir / dest_filename
                try:
                    shutil.move(str(src_path), str(dest_path))
                    logger.debug("Moved: %s -> %s", src_path, dest_path)
                    moved_count += 1
                except Exception as e:
                    logger.error("Error moving %s: %s", src_path, e)
                    skipped_count += 1
            else:
                skipped_count += 1
                logger.debug("Skipping non-mat/wav file: %s", current_dir / filename)

    logger.info("Flattening complete. Moved %d files, skipped %d.", moved_count, skipped_count)


def _process_single_mat_file(mat_path: Path, audio_base_dir: Path, output_audio_dir: Path) -> List[Dict[str, Any]]:
    """
    Processes a single MUPET .mat file to extract syllable audio segments.

    Args:
        mat_path (Path): Path to the .mat file.
        audio_base_dir (Path): Base directory where corresponding .wav files are located.
        output_audio_dir (Path): Directory to save the extracted syllable .wav files.

    Returns:
        List[Dict[str, Any]]: A list of dictionaries, each representing a syllable,
                              containing metadata like file path, label (identity), speaker (identity), etc.
    """
    syllable_records = []
    try:
        mat_data = sio.loadmat(mat_path)
        syllable_data = mat_data.get("syllable_data")
        syllable_stats = mat_data.get("syllable_stats")
        filestats = mat_data.get("filestats")

        if syllable_data is None or syllable_stats is None or filestats is None:
            logger.warning("Skipping %s: Missing required data fields (syllable_data, syllable_stats, or filestats).", mat_path.name)
            return []
        if syllable_data.shape[1] == 0 or syllable_stats.shape[1] == 0:
            logger.warning("Skipping %s: No syllables found in data/stats.", mat_path.name)
            return []
        if syllable_data.shape[1] != syllable_stats.shape[1]:
            logger.warning(
                "Skipping %s: Mismatch between syllable_data (%d) and syllable_stats (%d) counts.",
                mat_path.name,
                syllable_data.shape[1],
                syllable_stats.shape[1],
            )
            return []

        try:
            wav_filename = syllable_data[0, 0][0]
            wav_path = audio_base_dir / wav_filename
            if not wav_path.exists():
                potential_paths = list(audio_base_dir.glob(f"**/{wav_filename}"))
                if not potential_paths:
                    logger.error("WAV file '%s' referenced in %s not found in %s.", wav_filename, mat_path.name, audio_base_dir)
                    return []
                wav_path = potential_paths[0]
                logger.debug("Found WAV file at %s", wav_path)

            fs = filestats[0, 0]["fs"][0, 0]
        except (IndexError, KeyError, TypeError) as e:
            logger.error("Error extracting metadata from %s: %s. Check MAT file structure.", mat_path.name, e, exc_info=True)
            return []

        try:
            audio, wav_fs = torchaudio.load(wav_path)
            if wav_fs != fs:
                logger.warning("Sample rate mismatch for %s: MAT=%d, WAV=%d. Using WAV's sample rate.", wav_path.name, fs, wav_fs)
                fs = wav_fs
            audio = audio[0].numpy()
            audio = audio.astype(np.float32)
            max_abs = np.max(np.abs(audio))
            if max_abs > 1e-6:
                audio = audio / max_abs
        except Exception as e:
            logger.error("Error loading or processing WAV file %s: %s", wav_path, e, exc_info=True)
            return []

        try:
            onsets_ms = syllable_stats[3, :]
            durations_ms = syllable_stats[1, :]
        except IndexError:
            logger.error("Error accessing onset/duration rows in syllable_stats for %s. Check MAT file structure.", mat_path.name)
            return []

        onsets_samples = (onsets_ms * fs / 1000).astype(int)
        durations_samples = (durations_ms * fs / 1000).astype(int)
        offsets_samples = onsets_samples + durations_samples

        identity = mat_path.stem.split("_")[0]
        file_id = mat_path.stem

        identity_output_dir = output_audio_dir / identity
        identity_output_dir.mkdir(parents=True, exist_ok=True)

        num_syllables = len(onsets_samples)
        for i in range(num_syllables):
            onset, offset = onsets_samples[i], offsets_samples[i]

            if onset < 0 or offset > len(audio) or onset >= offset:
                logger.warning("Skipping syllable %d in %s: Invalid boundaries [%d, %d] for audio length %d.", i + 1, mat_path.name, onset, offset, len(audio))
                continue

            syllable = audio[onset:offset]

            syllable_filename = f"{identity}_{file_id}_syllable{i+1}.wav"
            output_path = identity_output_dir / syllable_filename
            try:
                syllable_tensor = torch.from_numpy(syllable).unsqueeze(0)
                torchaudio.save(output_path, syllable_tensor, fs)

                syllable_records.append({
                    "audio": str(output_path),
                    "label": identity,
                    "speaker": identity,
                    "source_mat": str(mat_path),
                    "source_wav": str(wav_path),
                    "syllable_index": i + 1,
                    "onset_ms": onsets_ms[i],
                    "duration_ms": durations_ms[i],
                    "sampling_rate": fs,
                    "subset": "mouse_identity",
                })
            except Exception as e:
                logger.error("Error saving syllable %d from %s to %s: %s", i + 1, mat_path.name, output_path, e, exc_info=True)

    except Exception as e:
        logger.error("Critical error processing MAT file %s: %s", mat_path, e, exc_info=True)

    return syllable_records


def convert_mouse_identity_data(
    raw_data_dir: Union[str, Path], output_dir: Union[str, Path], flatten_first: bool = True
) -> Optional[str]:
    """
    Converts the raw MUPET mouse identity dataset (.mat files) into a structured
    format with individual syllable audio files and metadata.

    Args:
        raw_data_dir (Union[str, Path]): Path to the directory containing the raw
                                         MUPET data (potentially nested).
        output_dir (Union[str, Path]): Path to the directory where the processed data
                                       (syllable audio files, metadata JSON/HF dataset) will be saved.
        flatten_first (bool): If True, first flatten the raw_data_dir structure by moving
                              all .mat/.wav files to a single directory (`output_dir`/raw_flat).

    Returns:
        str: Path to the saved Hugging Face dataset directory, or None if conversion failed.
    """
    raw_data_dir = Path(raw_data_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    processed_audio_dir = output_dir / "audio_syllables"
    processed_audio_dir.mkdir(parents=True, exist_ok=True)
    hf_dataset_dir = output_dir / "hf_dataset"

    if flatten_first:
        flat_dir = output_dir / "raw_flat"
        _flatten_mouse_identity_raw_structure(raw_data_dir, flat_dir)
        mat_search_dir = flat_dir
        wav_base_dir = flat_dir
    else:
        logger.warning("Not flattening directory structure. MAT file processing might fail if WAV paths are relative.")
        mat_search_dir = raw_data_dir
        wav_base_dir = raw_data_dir

    mat_files = list(mat_search_dir.glob("**/*.mat"))
    if not mat_files:
        logger.error("No .mat files found in %s. Cannot proceed.", mat_search_dir)
        return None
    logger.info("Found %d .mat files to process.", len(mat_files))

    all_syllable_data = []
    for mat_file in tqdm(mat_files, desc="Processing MAT files"):
        syllable_records = _process_single_mat_file(mat_file, wav_base_dir, processed_audio_dir)
        all_syllable_data.extend(syllable_records)

    if not all_syllable_data:
        logger.error("No syllables were successfully extracted from any MAT file.")
        return None

    logger.info("Successfully extracted %d syllables.", len(all_syllable_data))

    try:
        features = Features({
            "audio": Audio(sampling_rate=all_syllable_data[0]["sampling_rate"]),
            "label": Value(dtype="string"),
            "speaker": Value(dtype="string"),
            "source_mat": Value(dtype="string"),
            "source_wav": Value(dtype="string"),
            "syllable_index": Value(dtype="int32"),
            "onset_ms": Value(dtype="float32"),
            "duration_ms": Value(dtype="float32"),
            "sampling_rate": Value(dtype="int32"),
            "subset": Value(dtype="string"),
        })

        def generator():
            for record in all_syllable_data:
                audio_path = record["audio"]
                record["audio"] = audio_path
                yield record

        hf_dataset = HFDataset.from_generator(generator, features=features)

        hf_dataset.save_to_disk(str(hf_dataset_dir))
        logger.info("Successfully created and saved Hugging Face dataset to %s", hf_dataset_dir)

        if flatten_first and flat_dir.exists():
            logger.info("Intermediate flat directory left at %s. Remove manually if desired.", flat_dir)

        return str(hf_dataset_dir)

    except Exception as e:
        logger.error("Failed to create or save Hugging Face dataset: %s", e, exc_info=True)
        return Noneimport logging
from typing import Union, Optional, Dict, List, Tuple, Any, Iterator
import torch
import numpy as np
from sklearn.decomposition import IncrementalPCA
from tqdm.auto import tqdm
from itertools import chain, tee
import gc

logger = logging.getLogger(__name__)

def apply_averaging(features_np: np.ndarray, method: Optional[str]) -> Optional[np.ndarray]:
    """
    Applies a specified averaging method to a NumPy feature array.

    Args:
        features_np (np.ndarray): Input feature array. Can be multi-dimensional.
        method (Optional[str]): The averaging method to apply. Supported methods:
                                'mean_row_col', 'first_row_col', 'first_row', 'first_col',
                                'mean_time_dim'. If None or unsupported, input is flattened.

    Returns:
        Optional[np.ndarray]: The averaged feature array (always 1D) or None on error.
    """
    original_shape = features_np.shape
    ndim = features_np.ndim
    func_name = "apply_averaging"
    
    allowed_methods = {"mean_row_col", "first_row_col", "first_row", "first_col", "mean_time_dim"}

    if method is None or method not in allowed_methods:
        if method is not None:
            logger.warning("[%s] Invalid method '%s'. Allowed: %s. Flattening input.", func_name, method, allowed_methods)
        else:
            logger.debug("[%s] No averaging method. Flattening input shape %s.", func_name, original_shape)
        if ndim == 0:
            return features_np.reshape(-1).astype(np.float32)
        return features_np.flatten().astype(np.float32)

    logger.debug("[%s] Applying method '%s' to input shape %s", func_name, method, original_shape)

    if method != 'mean_time_dim' and ndim != 2:
        logger.warning(
            "[%s] Method '%s' expects 2D input, got %dD shape %s. Reshaping to 2D.",
            func_name, method, ndim, original_shape
        )
        if any(s == 0 for s in original_shape):
            return np.empty((0,), dtype=np.float32)
        
        first_dim_size = np.prod(original_shape[:-1])
        last_dim_size = original_shape[-1]
        features_np = features_np.reshape(first_dim_size, last_dim_size)
        logger.debug("[%s] Reshaped input to %s", func_name, features_np.shape)

    result_1d = None
    try:
        if method == "mean_time_dim":
            if ndim < 1:
                 raise ValueError("Cannot apply mean_time_dim to a scalar.")
            result_1d = np.mean(features_np, axis=-1, dtype=np.float32)
        elif method == "mean_row_col":
            row_means = np.mean(features_np, axis=1, dtype=np.float32)
            col_means = np.mean(features_np, axis=0, dtype=np.float32)
            result_1d = np.concatenate([row_means, col_means])
        elif method == "first_row_col":
            first_row = features_np[0, :].astype(np.float32)
            first_col = features_np[:, 0].astype(np.float32)
            result_1d = np.concatenate([first_row, first_col])
        elif method == "first_row":
            result_1d = features_np[0, :].astype(np.float32)
        elif method == "first_col":
            result_1d = features_np[:, 0].astype(np.float32)

    except IndexError as e:
        logger.error("[%s] IndexError applying '%s' to shape %s: %s. Flattening.", func_name, method, features_np.shape, e)
        return features_np.flatten().astype(np.float32)
    except Exception as e:
        logger.error("[%s] Error applying '%s' to shape %s: %s. Flattening.", func_name, method, features_np.shape, e, exc_info=True)
        return features_np.flatten().astype(np.float32)

    if result_1d is None:
        logger.error("[%s] Result is None after applying method '%s'. Flattening.", func_name, method)
        return features_np.flatten().astype(np.float32)

    final_result = result_1d.flatten().astype(np.float32)
    logger.debug("[%s] Method '%s' successful. Input %s -> Output %s", func_name, method, original_shape, final_result.shape)
    return final_result


def apply_pca(
    features_iterator: Iterator[np.ndarray],
    n_components: int,
    pca_model: Optional[IncrementalPCA] = None,
    batch_size_hint: int = 1024,
) -> Tuple[Optional[List[np.ndarray]], Optional[IncrementalPCA]]:
    """
    Applies IncrementalPCA to features yielded by an iterator.

    Args:
        features_iterator: An iterator yielding NumPy arrays of shape [batch_size, feature_dim].
                           The iterator MUST yield 2D arrays.
        n_components: Number of PCA components. Must be positive.
        pca_model: A pre-fitted IncrementalPCA model to use for transformation, or None to fit a new one.
        batch_size_hint: Hint for IncrementalPCA's internal processing batch size during fitting.

    Returns:
        Tuple containing:
            - Optional[List[np.ndarray]]: List of transformed feature batches (if transforming).
                                         None if fitting or error.
            - Optional[IncrementalPCA]: The fitted or passed PCA model. None if fitting failed.
    """
    func_name = "apply_pca"
    if n_components <= 0:
        raise ValueError("n_components must be positive.")

    try:
        iter_peek, features_iterator_orig = tee(features_iterator)
        first_batch = next(iter_peek)
        features_iterator = chain([first_batch], iter_peek)
        if not isinstance(first_batch, np.ndarray) or first_batch.ndim != 2:
            logger.error(
                "[%s] Iterator must yield 2D NumPy arrays. Got %s with shape %s. Aborting PCA.",
                func_name,
                type(first_batch),
                getattr(first_batch, "shape", "N/A"),
            )
            return None, pca_model
        input_dim = first_batch.shape[1]
        actual_batch_size = first_batch.shape[0]
        logger.debug("[%s] Peeked first batch shape: %s. Input Dim: %d", func_name, first_batch.shape, input_dim)
    except StopIteration:
        logger.warning("[%s] Input iterator is empty.", func_name)
        return [], pca_model
    except Exception as peek_err:
        logger.warning("[%s] Failed to peek at iterator: %s. Cannot proceed without dimensionality.", func_name, peek_err)
        return None, pca_model

    if pca_model is None:
        logger.info("[%s] Fitting IncrementalPCA (n=%d, hint=%d)...", func_name, n_components, batch_size_hint)
        if n_components > input_dim:
            logger.warning("Requested n_components (%d) > feature dimension (%d). Using n_components=%d.", n_components, input_dim, input_dim)
            n_components = input_dim

        pca_model_inc = IncrementalPCA(n_components=n_components, batch_size=batch_size_hint)
        samples_processed = 0
        batch_num_fit = 0
        try:
            pbar_fit = tqdm(features_iterator, desc="Fitting IncrementalPCA", leave=False, unit="batch")
            for batch_2d_np in pbar_fit:
                batch_num_fit += 1
                if not isinstance(batch_2d_np, np.ndarray) or batch_2d_np.ndim != 2:
                    logger.warning(
                        "[%s] FIT: Skipping invalid batch %d (Expected 2D np.ndarray, got %s shape %s).",
                        func_name,
                        batch_num_fit,
                        type(batch_2d_np),
                        getattr(batch_2d_np, "shape", "N/A"),
                    )
                    continue
                batch_samples = batch_2d_np.shape[0]
                if batch_samples == 0:
                    continue

                has_nan = np.isnan(batch_2d_np).any()
                has_inf = np.isinf(batch_2d_np).any()
                if has_nan or has_inf:
                    logger.error("[%s] FIT: NaN/Inf in batch %d. Skipping!", func_name, batch_num_fit)
                    continue
                if batch_2d_np.shape[1] != input_dim:
                    logger.error("[%s] FIT: Inconsistent feature dim in batch %d (%d != %d). Skipping!", func_name, batch_num_fit, batch_2d_np.shape[1], input_dim)
                    continue
                if batch_2d_np.dtype != np.float32:
                    batch_2d_np = batch_2d_np.astype(np.float32)

                pca_model_inc.partial_fit(batch_2d_np)
                samples_processed += batch_samples
                fitted_components = getattr(pca_model_inc, "n_components_", 0)
                pbar_fit.set_postfix({"fitted_comp": fitted_components, "samples": samples_processed})

            if samples_processed == 0:
                logger.error("[%s] IncrementalPCA: No valid data processed during fit.", func_name)
                return None, None
            final_fitted_components = getattr(pca_model_inc, "n_components_", 0)
            if final_fitted_components == 0:
                logger.error("[%s] IncrementalPCA failed to fit any components.", func_name)
                return None, None
            var_sum = np.sum(pca_model_inc.explained_variance_ratio_) if hasattr(pca_model_inc, "explained_variance_ratio_") and pca_model_inc.explained_variance_ratio_ is not None else np.nan
            if final_fitted_components < n_components:
                logger.warning("[%s] PCA fitted %d/%d components. Var: %.4f", func_name, final_fitted_components, n_components, var_sum)
            else:
                logger.info("[%s] PCA fitted %d components from %d samples. Var: %.4f", func_name, final_fitted_components, samples_processed, var_sum)
            return None, pca_model_inc

        except ValueError as ve:
            logger.error("[%s] IncrementalPCA fitting failed (ValueError): %s", func_name, ve, exc_info=True)
            return None, None
        except Exception as e:
            logger.error("[%s] IncrementalPCA fitting failed unexpectedly: %s", func_name, e, exc_info=True)
            return None, None

    else:
        logger.info("[%s] Applying pre-fitted IncrementalPCA model batch-by-batch...", func_name)
        if not isinstance(pca_model, IncrementalPCA):
            logger.error("[%s] Expected IncrementalPCA model!", func_name)
            return None, pca_model

        transformed_data_batches: List[np.ndarray] = []
        n_features_expected = getattr(pca_model, "n_features_in_", -1)
        fitted_comps = getattr(pca_model, "n_components_", n_components)
        pca_out_dim = min(n_components, fitted_comps)

        if n_features_expected != -1 and n_features_expected != input_dim:
            logger.error("[%s] Input data dim (%d) does not match pre-fitted PCA model expected dim (%d). Aborting transform.", func_name, input_dim, n_features_expected)
            return None, pca_model

        if n_components > fitted_comps:
            logger.warning("Requested %d PCA components, but model only has %d. Outputting %d.", n_components, fitted_comps, pca_out_dim)
        elif pca_out_dim < n_components:
            logger.warning("[%s] Limiting transform output to %d components (Min of requested %d and fitted %d).", func_name, pca_out_dim, n_components, fitted_comps)

        samples_processed = 0
        batch_num_transform = 0
        try:
            pbar_transform = tqdm(features_iterator, desc="Transforming (IncrementalPCA)", leave=False, unit="batch")
            for batch_2d_np in pbar_transform:
                batch_num_transform += 1
                if not isinstance(batch_2d_np, np.ndarray) or batch_2d_np.ndim != 2:
                    logger.warning(
                        "[%s] Transform: Skipping invalid batch %d (Expected 2D np.ndarray, got %s shape %s).",
                        func_name,
                        batch_num_transform,
                        type(batch_2d_np),
                        getattr(batch_2d_np, "shape", "N/A"),
                    )
                    continue
                batch_samples = batch_2d_np.shape[0]
                if batch_samples == 0:
                    continue

                has_nan = np.isnan(batch_2d_np).any()
                has_inf = np.isinf(batch_2d_np).any()
                if has_nan or has_inf:
                    logger.error("[%s] Transform: NaN/Inf in batch %d. Skipping!", func_name, batch_num_transform)
                    continue
                if batch_2d_np.shape[1] != input_dim:
                    logger.error("[%s] Transform: Inconsistent feature dim in batch %d (%d != %d). Skipping!", func_name, batch_num_transform, batch_2d_np.shape[1], input_dim)
                    continue
                if batch_2d_np.dtype != np.float32:
                    batch_2d_np = batch_2d_np.astype(np.float32)

                transformed_batch = pca_model.transform(batch_2d_np)
                if transformed_batch.shape[1] != pca_out_dim:
                    logger.warning("PCA transform output dim %d != expected %d. Adjusting.", transformed_batch.shape[1], pca_out_dim)
                    transformed_batch = transformed_batch[:, :pca_out_dim]

                transformed_data_batches.append(transformed_batch.astype(np.float32))
                samples_processed += batch_samples
                pbar_transform.set_postfix({"samples": samples_processed})

            logger.info("[%s] PCA transform complete (%d batches, %d valid samples). Output dim: %d", func_name, len(transformed_data_batches), samples_processed, pca_out_dim)
            return transformed_data_batches, pca_model

        except ValueError as ve:
            logger.error("[%s] IncrementalPCA transform failed (ValueError): %s", func_name, ve, exc_info=True)
            return None, pca_model
        except Exception as e:
            logger.error("[%s] IncrementalPCA transform failed unexpectedly: %s", func_name, e, exc_info=True)
            return None, pca_modelimport hashlib
import json
import logging
import pickle
from pathlib import Path
from typing import Any, Dict, Optional, Union, List
import numpy as np
import pandas as pd
import torch
from sklearn.decomposition import PCA, IncrementalPCA
import h5py
import os


logger = logging.getLogger(__name__)
HDF5_DATASET_NAME = "features"
HDF5_INDICES_NAME = "original_indices"


class NpEncoder(json.JSONEncoder):
    """Helper class for JSON encoding NumPy types, Path, Tensors etc."""

    def default(self, obj):
        """
        Encodes various object types into JSON-serializable formats.

        Args:
            obj: The object to encode.

        Returns:
            JSON-serializable representation of the object.
        """
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, np.floating):
            return float(obj) if np.isfinite(obj) else None
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        if isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        if isinstance(obj, (torch.Tensor)):
            return obj.detach().cpu().numpy().tolist()
        if isinstance(obj, Path):
            return str(obj)
        if obj is None:
            return None
        if isinstance(obj, set):
            return list(obj)
        return super().default(obj)


def ensure_dir_exists(path_obj: Union[str, Path]):
    """
    Checks if a directory exists at the given path, and creates it if not.

    Args:
        path_obj: The path to the directory (string or Path object).

    Raises:
        OSError: If the path exists but is a file, or if directory creation fails.
    """
    if not isinstance(path_obj, Path):
        path_obj = Path(path_obj)

    try:
        if path_obj.exists():
            if not path_obj.is_dir():
                error_msg = f"Path exists but is not a directory: {path_obj}"
                logger.error(error_msg)
                raise OSError(error_msg)
        else:
            logger.info("Creating directory: %s", path_obj)
            path_obj.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.error("Failed to ensure directory exists at %s: %s", path_obj, e, exc_info=True)
        raise


def _generate_config_hash(config: Dict[str, Any], length: int = 10) -> str:
    """
    Generates a deterministic hash for a configuration dictionary.

    Args:
        config (Dict[str, Any]): The configuration dictionary.
        length (int): The desired length of the hash string.

    Returns:
        str: The configuration hash.
    """
    hasher = hashlib.md5()
    relevant_keys = [
        "name",
        "module",
        "class",
        "base_extractor",
        "averaging",
        "pca",
        "pca_load_chunks",
        "distance_name",
        "feature_config",
        "distance_config",
        "benchmark_config",
        "params",
    ]
    relevant_config = {}
    for key in relevant_keys:
        value = config.get(key)
        if value is not None:
            if isinstance(value, dict):
                try:
                    relevant_config[key] = json.dumps(value, sort_keys=True, cls=NpEncoder)
                except TypeError:
                    relevant_config[key] = repr(sorted(value.items()))
            elif isinstance(value, (list, tuple, set)):
                try:
                    try:
                        processed_list = sorted(list(value)) if isinstance(value, set) else sorted(value)
                        relevant_config[key] = json.dumps(processed_list, cls=NpEncoder)
                    except TypeError:
                        relevant_config[key] = repr(value)
                except TypeError:
                    relevant_config[key] = repr(value)
            else:
                relevant_config[key] = value

    try:
        serialized = json.dumps(relevant_config, sort_keys=True, cls=NpEncoder).encode("utf-8")
        hasher.update(serialized)
    except TypeError as e:
        logger.warning("JSON hash failed: %s. Using repr.", e)
        hasher.update(repr(sorted(relevant_config.items())).encode("utf-8"))

    return hasher.hexdigest()[:length]


def _get_safe_path_part(name: Optional[str], default="unknown") -> str:
    """
    Sanitizes a string to be safe for use in file paths.

    Args:
        name (Optional[str]): The input string.
        default (str): The default string to use if the input is None or becomes empty after sanitization.

    Returns:
        str: The sanitized string.
    """
    if name is None:
        name = default
    name = str(name)
    chars_to_replace = r'<>:"/\|?*' + "".join(map(chr, range(32)))
    safe_name = name
    for char in chars_to_replace:
        safe_name = safe_name.replace(char, "_")
    safe_name = safe_name.strip(" .")
    return safe_name or default


def get_cache_path(cache_dir: Path, prefix: str, dataset_cache_id: str, config_dict: Dict[str, Any], extra_suffix: Optional[str] = None) -> Path:
    """
    Generates the deterministic cache file path based on configuration details and prefix.

    Args:
        cache_dir (Path): The base directory for caching.
        prefix (str): Prefix indicating data type (e.g., 'features', 'distances_cosine').
        dataset_cache_id (str): Unique identifier for the dataset subset/split.
        config_dict (Dict[str, Any]): Configuration dictionary for hash generation and name extraction.
        extra_suffix (Optional[str]): Optional extra string used in the filename.

    Returns:
        Path: The generated cache file path.
    """
    cache_dir.mkdir(parents=True, exist_ok=True)
    safe_dataset_id = _get_safe_path_part(dataset_cache_id, "unknown_dataset")
    config_hash = _generate_config_hash(config_dict)

    if prefix.startswith("distances_"):
        feature_conf = config_dict.get("feature_config", {})
        item_name = feature_conf.get("name", "unknown_feature")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "features":
        item_name = config_dict.get("name", "unknown_feature")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "intermediate":
        item_name = config_dict.get("name", "unknown_intermediate")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "pca_model":
        item_name = config_dict.get("name", "unknown_pcamodel")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".pkl"
    elif prefix == "bench_item":
        item_name = config_dict.get("name", "unknown_benchitem")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".json"
    elif prefix == "bench_results_summary":
        summary_name = f"{prefix}_{safe_dataset_id}"
        if extra_suffix:
            summary_name += f"_{_get_safe_path_part(extra_suffix)}"
        return (cache_dir / summary_name).with_suffix(".csv")
    elif prefix == "bench_results":
        results_name = f"{prefix}_{safe_dataset_id}"
        if extra_suffix:
            results_name += f"_{_get_safe_path_part(extra_suffix)}"
        return (cache_dir / results_name).with_suffix(".json")
    else:
        logger.warning("Using fallback path construction for unknown prefix: '%s'", prefix)
        item_name = config_dict.get("name", prefix)
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts = [prefix, safe_dataset_id]
        if item_name != prefix and safe_item_name:
            base_name_parts.append(safe_item_name)
        suffix = ".dat"

    if extra_suffix:
        base_name_parts.append(_get_safe_path_part(extra_suffix))

    base_name_parts.append(config_hash)
    base_name = "_".join(filter(None, base_name_parts))

    return (cache_dir / base_name).with_suffix(suffix)


def find_cache_path(cache_dir: Path, prefix: str, dataset_cache_id: str, config_dict: Dict[str, Any], extra_suffix: Optional[str] = None) -> Optional[Path]:
    """
    Finds a cache file path, first trying the exact path with hash,
    then falling back to a looser match based on names (without hash).

    Args:
        cache_dir: The base directory for caching.
        prefix: Prefix indicating data type (e.g., 'features', 'distances_cosine').
        dataset_cache_id: Unique identifier for the dataset subset/split.
        config_dict: Configuration dictionary for hash generation and name extraction.
                     For distances, expects 'feature_config'.
        extra_suffix: Optional extra string used in the filename.

    Returns:
        The Path object if a matching file is found, otherwise None.
    """
    cache_dir = Path(cache_dir)
    if not cache_dir.is_dir():
        logger.debug("Cache directory does not exist: %s", cache_dir)
        return None

    exact_path = get_cache_path(cache_dir, prefix, dataset_cache_id, config_dict, extra_suffix)

    if exact_path.is_file():
        logger.debug("Found exact cache match: %s", exact_path.name)
        return exact_path

    logger.debug("Exact path %s not found. Trying loose match...", exact_path.name)

    safe_dataset_id = _get_safe_path_part(dataset_cache_id, "unknown_dataset")
    item_name = "unknown"
    base_name_parts_loose = []
    suffix = ".h5"

    if prefix.startswith("distances_"):
        feature_conf = config_dict.get("feature_config", {})
        item_name = feature_conf.get("name", "unknown_feature")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "features":
        item_name = config_dict.get("name", "unknown_feature")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "intermediate":
        item_name = config_dict.get("name", "unknown_intermediate")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".h5"
    elif prefix == "pca_model":
        item_name = config_dict.get("name", "unknown_pcamodel")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".pkl"
    elif prefix == "bench_item":
        item_name = config_dict.get("name", "unknown_benchitem")
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".json"
    else:
        logger.warning("Using fallback construction for unknown prefix: '%s'", prefix)
        item_name = config_dict.get("name", prefix)
        safe_item_name = _get_safe_path_part(item_name)
        base_name_parts_loose = [prefix, safe_dataset_id, safe_item_name]
        suffix = ".dat"

    if extra_suffix:
        base_name_parts_loose.append(_get_safe_path_part(extra_suffix))

    loose_filename_base = "_".join(filter(None, base_name_parts_loose))
    loose_filename_pattern = f"{loose_filename_base}_*{suffix}"
    

    search_pattern = str(cache_dir / loose_filename_pattern)

    logger.debug("Searching for loose match with pattern: %s", loose_filename_pattern)

    relative_pattern = loose_filename_pattern
    found_files = list(cache_dir.glob(relative_pattern))

    if found_files:
        found_files.sort(key=os.path.getmtime, reverse=True)
        selected_file = found_files[0]

        if len(found_files) > 1:
            logger.warning("Found %d loose matches for %s/%s. Using the most recent: %s", len(found_files), prefix, item_name, selected_file.name)
        else:
            logger.info("Found loose cache match: %s", selected_file.name)
        return selected_file

    logger.info("No cache file found (exact or loose) for %s/%s", prefix, item_name)
    return None

def load_pickle(filepath: Path, log_prefix: str = "Data") -> Optional[Any]:
    """
    Loads data from a pickle file if it exists.

    Args:
        filepath (Path): Path to the pickle file.
        log_prefix (str): Prefix for logging messages.

    Returns:
        Optional[Any]: The loaded data, or None if loading fails.
    """
    if not filepath.is_file():
        logger.debug("%s pickle cache file not found: %s", log_prefix, filepath.name)
        return None
    try:
        with open(filepath, "rb") as f:
            data = pickle.load(f)
        logger.info("Loaded %s from cache: %s", log_prefix.lower(), filepath.name)
        return data
    except (EOFError, pickle.UnpicklingError, ImportError, ModuleNotFoundError, AttributeError) as e:
        logger.error("Corrupt/incompatible pickle file %s: %s. Deleting.", filepath.name, e)
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return None
    except Exception as e:
        logger.error("Failed load %s from %s: %s", log_prefix.lower(), filepath.name, e)
        return None


def load_hdf5(filepath: Path, dataset_name: str = "data", log_prefix: str = "Data") -> Optional[np.ndarray]:
    """
    Loads a NumPy array from an HDF5 file if it exists.

    Args:
        filepath (Path): Path to the HDF5 file.
        dataset_name (str): Name of the dataset within the HDF5 file.
        log_prefix (str): Prefix for logging messages.

    Returns:
        Optional[np.ndarray]: The loaded array, or None if loading fails.
    """
    if not filepath.is_file():
        logger.debug("%s HDF5 cache file not found: %s", log_prefix, filepath.name)
        return None
    try:
        with h5py.File(filepath, "r") as f:
            if dataset_name not in f:
                logger.error("Dataset '%s' not found in HDF5 file %s.", dataset_name, filepath.name)
                return None
            data = f[dataset_name][:]
            logger.info("Loaded %s from HDF5 cache: %s (Shape: %s)", log_prefix.lower(), filepath.name, data.shape)
            return data
    except OSError as e:
        logger.error("Error opening/reading HDF5 %s: %s.", filepath.name, e)
        return None
    except KeyError:
        logger.error("Dataset '%s' not found in HDF5 file %s.", dataset_name, filepath.name)
        return None
    except Exception as e:
        logger.error("Failed load %s from HDF5 %s: %s", log_prefix.lower(), filepath.name, e)
        return None


def load_json_results(filepath: Path, log_prefix: str = "Benchmark item") -> Optional[Dict]:
    """
    Loads dictionary results from a JSON file if it exists.

    Args:
        filepath (Path): Path to the JSON file.
        log_prefix (str): Prefix for logging messages.

    Returns:
        Optional[Dict]: The loaded dictionary, or None if loading fails.
    """
    if not filepath.is_file():
        logger.debug("%s JSON cache file not found: %s", log_prefix, filepath.name)
        return None
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)
        if not isinstance(loaded_data, dict):
            logger.error("Invalid data type in JSON cache %s.", filepath.name)
            return None
        logger.info("Loaded %s from JSON cache: %s", log_prefix.lower(), filepath.name)
        return loaded_data
    except json.JSONDecodeError as e:
        logger.error("Corrupt JSON cache file %s: %s. Deleting.", filepath.name, e)
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return None
    except Exception as e:
        logger.error("Failed load JSON cache %s: %s", filepath.name, e)
        return None


def read_hdf5_metadata(filepath: Path, dataset_name: str = HDF5_DATASET_NAME) -> Optional[Dict[str, Any]]:
    """
    Reads metadata attributes and the original_indices dataset from an HDF5 file.

    Args:
        filepath (Path): Path to the HDF5 file.
        dataset_name (str): Name of the main dataset to read attributes from.

    Returns:
        Optional[Dict[str, Any]]: Dictionary containing metadata, including 'original_indices'
                                  if present as a dataset or attribute. Returns None if file
                                  not found or error occurs.
    """
    if not filepath.is_file():
        logger.debug("HDF5 metadata file not found: %s", filepath.name)
        return None
    metadata: Dict[str, Any] = {}
    loaded_indices: Optional[List[int]] = None
    try:
        with h5py.File(filepath, "r") as f:
            main_dset = f.get(dataset_name)
            if isinstance(main_dset, h5py.Dataset):
                metadata.update(dict(main_dset.attrs))
                metadata.pop(HDF5_INDICES_NAME, None)
            else:
                metadata.update(dict(f.attrs))
                if not metadata:
                    logger.warning("Main dataset '%s' not found and no root attributes in %s.", dataset_name, filepath.name)

            indices_dset = f.get(HDF5_INDICES_NAME)
            if isinstance(indices_dset, h5py.Dataset):
                try:
                    loaded_indices = [int(x) for x in indices_dset[:]]
                    logger.debug("Read '%s' from dataset.", HDF5_INDICES_NAME)
                except Exception as idx_err:
                    logger.error("Failed read '%s' dataset: %s. Check attrs.", HDF5_INDICES_NAME, idx_err)
            elif loaded_indices is None:
                attr_indices_val = None
                if isinstance(main_dset, h5py.Dataset) and HDF5_INDICES_NAME in main_dset.attrs:
                    attr_indices_val = main_dset.attrs[HDF5_INDICES_NAME]
                    logger.warning("Found '%s' as attribute on main dataset (old format?).", HDF5_INDICES_NAME)
                elif HDF5_INDICES_NAME in f.attrs:
                    attr_indices_val = f.attrs[HDF5_INDICES_NAME]
                    logger.warning("Found '%s' as attribute on file root.", HDF5_INDICES_NAME)

                if attr_indices_val is not None:
                    try:
                        if isinstance(attr_indices_val, np.ndarray):
                            loaded_indices = [int(x) for x in attr_indices_val]
                        elif isinstance(attr_indices_val, (list, tuple)):
                            loaded_indices = [int(x) for x in attr_indices_val]
                        elif isinstance(attr_indices_val, str):
                            loaded_indices = [int(x) for x in json.loads(attr_indices_val)]
                        else:
                            loaded_indices = [int(x) for x in list(attr_indices_val)]
                    except (TypeError, ValueError, json.JSONDecodeError) as attr_err:
                        logger.error("Could not convert '%s' attribute: %s.", HDF5_INDICES_NAME, attr_err)

            if loaded_indices is not None:
                metadata[HDF5_INDICES_NAME] = loaded_indices

    except OSError as e:
        logger.error("Error opening/reading HDF5 metadata %s: %s", filepath.name, e)
        return None
    except Exception as e:
        logger.error("Unexpected error reading HDF5 metadata %s: %s", filepath.name, e, exc_info=True)
        return None
    return metadata


def save_pickle(data: Any, filepath: Path, log_prefix: str = "Data") -> bool:
    """
    Saves data to a pickle file.

    Args:
        data (Any): The data to save.
        filepath (Path): Path to save the file.
        log_prefix (str): Prefix for logging messages.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, "wb") as f:
            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        logger.info("%s saved to %s", log_prefix, filepath.name)
        return True
    except Exception as e:
        logger.error("Failed save pickle %s: %s", filepath.name, e, exc_info=True)
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return False


def save_hdf5(data: np.ndarray, filepath: Path, dataset_name: str = "data", attributes: Optional[Dict[str, Any]] = None, log_prefix: str = "Data") -> bool:
    """
    Saves a NumPy array to an HDF5 file with attributes.

    Args:
        data (np.ndarray): The NumPy array to save.
        filepath (Path): Path to save the HDF5 file.
        dataset_name (str): Name of the dataset to create within the HDF5 file.
        attributes (Optional[Dict[str, Any]]): Dictionary of attributes to save with the dataset.
        log_prefix (str): Prefix for logging messages.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with h5py.File(filepath, "w") as f:
            dset = f.create_dataset(dataset_name, data=data, compression="gzip", compression_opts=4, shuffle=True, fletcher32=True)
            if attributes:
                serializable_attrs = json.loads(json.dumps(attributes, cls=NpEncoder))
                for k, v in serializable_attrs.items():
                    try:
                        dset.attrs[k] = v
                    except TypeError as te:
                        logger.warning("HDF5 Attr Warn %s: %s. String fallback.", k, te)
                        dset.attrs[k] = str(v)
        logger.info("%s saved to HDF5 file %s (Dataset: '%s')", log_prefix, filepath.name, dataset_name)
        return True
    except Exception as e:
        logger.error("Failed save HDF5 %s: %s", filepath.name, e, exc_info=True)
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return False


def save_json_results(results: Dict[str, Any], filepath: Path, log_prefix: str = "Benchmark item") -> bool:
    """
    Saves dictionary results to a JSON file.

    Args:
        results (Dict[str, Any]): The dictionary to save.
        filepath (Path): Path to save the JSON file.
        log_prefix (str): Prefix for logging messages.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        filepath.parent.mkdir(parents=True, exist_ok=True)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, cls=NpEncoder)
        logger.info("Saved %s JSON results to %s", log_prefix.lower(), filepath.name)
        return True
    except Exception as e:
        logger.error("Failed save JSON %s: %s", filepath.name, e, exc_info=True)
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return False


def save_stacked_features(features: np.ndarray, metadata: Dict[str, Any], cache_dir: Path, dataset_cache_id: str, feature_config: Dict[str, Any]) -> bool:
    """
    Saves stacked features and metadata to HDF5.

    Args:
        features (np.ndarray): Stacked features array.
        metadata (Dict[str, Any]): Metadata associated with the features.
        cache_dir (Path): Base caching directory.
        dataset_cache_id (str): Identifier for the dataset.
        feature_config (Dict[str, Any]): Configuration of the feature extractor.

    Returns:
        bool: True if saving was successful, False otherwise.
    """
    cache_path = get_cache_path(cache_dir, "features", dataset_cache_id, feature_config)
    log_prefix = f"Stacked features ({feature_config.get('name', '?')})"
    metadata.setdefault("num_items_processed", features.shape[0])
    metadata.setdefault("feature_ndim", features.ndim - 1 if features.ndim > 1 else features.ndim)
    metadata.setdefault("target_padded_shape", list(features.shape[1:]) if features.ndim > 1 else [])
    indices = metadata.pop(HDF5_INDICES_NAME, None)
    success = save_hdf5(features, cache_path, dataset_name=HDF5_DATASET_NAME, attributes=metadata, log_prefix=log_prefix)
    if success and indices is not None:
        try:
            with h5py.File(cache_path, "a") as f:
                indices_array = np.array(indices, dtype=np.int64)
                f.create_dataset(HDF5_INDICES_NAME, data=indices_array, compression="gzip", compression_opts=4)
            logger.debug("Appended '%s' dataset to %s", HDF5_INDICES_NAME, cache_path.name)
        except Exception as e:
            logger.error("Failed to append indices dataset to %s: %s", cache_path.name, e)
    return success



def load_stacked_features(filepath: Path) -> Optional[np.ndarray]:
    """
    Loads stacked features from HDF5 using the core load_hdf5 function.

    Args:
        filepath (Path): Path to the HDF5 file.

    Returns:
        Optional[np.ndarray]: The loaded features array, or None if loading fails.
    """
    return load_hdf5(filepath=filepath, dataset_name=HDF5_DATASET_NAME, log_prefix=f"Stacked features ({filepath.name})")


def save_pca_model(pca_model: Union[PCA, IncrementalPCA], filepath: Path) -> bool:
    """
    Saves a PCA model (standard or incremental) to a Pickle file.

    Args:
        pca_model (Union[PCA, IncrementalPCA]): The PCA model to save.
        filepath (Path): Path to save the file.

    Returns:
        bool: True if successful, False otherwise.
    """
    return save_pickle(pca_model, filepath, log_prefix=f"PCA model ({filepath.name})")


def load_pca_model(filepath: Path) -> Optional[Union[PCA, IncrementalPCA]]:
    """
    Loads a PCA model (standard or incremental) using the core load_pickle function.

    Args:
        filepath (Path): Path to the Pickle file.

    Returns:
        Optional[Union[PCA, IncrementalPCA]]: The loaded PCA model, or None if loading fails or type is invalid.
    """
    log_prefix = f"PCA model ({filepath.name})"
    model = load_pickle(filepath=filepath, log_prefix=log_prefix)
    if model is not None and not isinstance(model, (PCA, IncrementalPCA)):
        logger.warning("Loaded PCA model has invalid type (%s). Discarding.", type(model))
        try:
            filepath.unlink(missing_ok=True)
        except OSError:
            pass
        return None
    return model


def save_distance_matrix(matrix: np.ndarray, filepath: Path, feature_name: str, distance_name: str) -> bool:
    """
    Saves a distance matrix to an HDF5 file.

    Args:
        matrix (np.ndarray): The distance matrix array.
        filepath (Path): Path to save the HDF5 file.
        feature_name (str): Name of the feature used to compute distances.
        distance_name (str): Name of the distance metric.

    Returns:
        bool: True if successful, False otherwise.
    """
    log_prefix = f"Distance matrix ({distance_name} for {feature_name})"
    attributes = {"feature_name": feature_name, "distance_name": distance_name}
    return save_hdf5(matrix, filepath, dataset_name="distance_matrix", attributes=attributes, log_prefix=log_prefix)


def load_distance_matrix(filepath: Path) -> Optional[np.ndarray]:
    """
    Loads a distance matrix from HDF5 using the core load_hdf5 function.

    Args:
        filepath (Path): Path to the HDF5 file.

    Returns:
        Optional[np.ndarray]: The loaded distance matrix array, or None if loading fails.
    """
    return load_hdf5(filepath=filepath, dataset_name="distance_matrix", log_prefix=f"Distance matrix ({filepath.name})")


def load_benchmark_item_results(filepath: Path) -> Optional[Dict]:
    """
    Loads individual benchmark item JSON results using core load_json_results.

    Args:
        filepath (Path): Path to the JSON file.

    Returns:
        Optional[Dict]: The loaded results dictionary, or None if loading fails.
    """
    return load_json_results(filepath=filepath, log_prefix=f"Benchmark item result ({filepath.name})")


def save_results(results: Dict[str, Any], output_dir: Union[str, Path], filename_prefix: str):
    """
    Saves final benchmark results to JSON and a summary CSV.

    Args:
        results (Dict[str, Any]): The dictionary containing all benchmark results.
        output_dir (Union[str, Path]): Directory to save the files.
        filename_prefix (str): Prefix for the output filenames. Expected to contain dataset/run identifiers.
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    base_dataset_id_str = "unknown_dataset"
    run_id_suffix = "unknown_run"
    if filename_prefix:
        parts = filename_prefix.split("_")
        if len(parts) >= 3:
            base_dataset_id_str = parts[1]
            run_id_suffix = "_".join(parts[2:])
        elif len(parts) == 2:
            base_dataset_id_str = parts[0]
            run_id_suffix = parts[1]
        else:
            run_id_suffix = filename_prefix

    json_path = output_dir / f"bench_results_{base_dataset_id_str}_{run_id_suffix}.json"
    csv_path = output_dir / f"bench_results_summary_{base_dataset_id_str}_{run_id_suffix}.csv"

    try:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, cls=NpEncoder)
        logger.info("Final results saved to %s", json_path.name)
    except Exception as e:
        logger.error("Failed save results JSON: %s", e, exc_info=True)

    try:
        all_records = []
        is_multi_subset = False
        if results and isinstance(results, dict):
            first_val = next(iter(results.values()), None)
            if isinstance(first_val, dict) and first_val and isinstance(next(iter(first_val.values()), None), dict):
                is_multi_subset = True

        subset_results = results if is_multi_subset else {base_dataset_id_str: results}

        for subset_key, subset_data in subset_results.items():
            if not isinstance(subset_data, dict):
                continue
            for feature_key, feature_data in subset_data.items():
                if not isinstance(feature_data, dict):
                    if feature_key == "error":
                        record = {"subset": subset_key, "feature": feature_key, "metric_type": "error", "distance": None, "benchmark": "Extraction/Processing", "error": metric_data}
                        all_records.append(record)
                    continue

                for metric_type_key, metric_data in feature_data.items():
                    if not isinstance(metric_data, dict):
                        if metric_type_key == "error":
                            record = {"subset": subset_key, "feature": feature_key, "metric_type": metric_type_key, "distance": None, "benchmark": None, "error": str(metric_data)}
                            all_records.append(record)
                        continue

                    base_info = {"subset": subset_key, "feature": feature_key, "metric_type": metric_type_key}
                    if metric_type_key == "distance_based":
                        for distance_key, bench_data in metric_data.items():
                            if not isinstance(bench_data, dict):
                                continue
                            for benchmark_key, scores in bench_data.items():
                                record = base_info.copy()
                                record["distance"] = distance_key
                                record["benchmark"] = benchmark_key
                                if isinstance(scores, dict):
                                    record.update({k: v for k, v in scores.items() if isinstance(v, (str, int, float, bool, type(None)))})
                                    record.update({k: f"{v[0]:.4f} - {v[1]:.4f}" if isinstance(v, list) and len(v) == 2 and all(isinstance(x, (float, int, np.number)) for x in v) else str(v) for k, v in scores.items() if isinstance(v, (list, tuple, set))})
                                else:
                                    record["value"] = scores
                                all_records.append(record)
                    elif metric_type_key == "feature_based":
                        for benchmark_key, scores in metric_data.items():
                            record = base_info.copy()
                            record["distance"] = None
                            record["benchmark"] = benchmark_key
                            if isinstance(scores, dict):
                                record.update({k: v for k, v in scores.items() if isinstance(v, (str, int, float, bool, type(None)))})
                                record.update({k: f"{v[0]:.4f} - {v[1]:.4f}" if isinstance(v, list) and len(v) == 2 and all(isinstance(x, (float, int, np.number)) for x in v) else str(v) for k, v in scores.items() if isinstance(v, (list, tuple, set))})
                            else:
                                record["value"] = scores
                            all_records.append(record)
                    elif metric_type_key == "error":
                        record = base_info.copy()
                        record["distance"] = metric_data.get("distance")
                        record["benchmark"] = metric_data.get("benchmark", "Unknown")
                        record["error"] = metric_data.get("error", str(metric_data))
                        all_records.append(record)

        if all_records:
            df = pd.DataFrame(all_records)
            cols_order = ["subset", "feature", "metric_type", "distance", "benchmark"]
            existing_cols_ordered = [c for c in cols_order if c in df.columns]
            other_cols = sorted([c for c in df.columns if c not in existing_cols_ordered])
            if "value" in other_cols and len(other_cols) > 1:
                other_cols.remove("value")
                other_cols.append("value")
            if "error" in other_cols:
                other_cols.remove("error")
                other_cols.append("error")
            final_cols = existing_cols_ordered + other_cols
            try:
                df = df[final_cols]
            except KeyError as e:
                logger.warning("Col reorder fail CSV: %s. Cols: %s", e, list(df.columns))
            df.to_csv(csv_path, index=False, encoding="utf-8")
            logger.info("Summary results saved to %s", csv_path.name)
        else:
            logger.debug("No records for CSV summary.")
    except Exception as e:
        logger.warning("Failed create/save summary CSV: %s", e, exc_info=True)


def read_hdf5_slice(filepath: Path, start_idx: int, end_idx: int, dataset_name: str = "data") -> Optional[np.ndarray]:
    """
    Reads a slice of a dataset from an HDF5 file.

    Args:
        filepath (Path): Path to the HDF5 file.
        start_idx (int): Starting index of the slice (inclusive).
        end_idx (int): Ending index of the slice (exclusive).
        dataset_name (str): Name of the dataset within the HDF5 file.

    Returns:
        Optional[np.ndarray]: The requested slice of data, or None if loading fails.
                             Returns an empty array if start_idx >= end_idx or slice is out of bounds.
    """
    if not filepath.is_file():
        logger.error("HDF5 slice file not found: %s", filepath.name)
        return None
    try:
        with h5py.File(filepath, "r") as f:
            if dataset_name not in f:
                logger.error("Dataset '%s' not found in %s.", dataset_name, filepath.name)
                return None
            dset = f[dataset_name]
            actual_start_idx = max(0, start_idx)
            actual_end_idx = min(end_idx, dset.shape[0])
            if actual_start_idx >= actual_end_idx:
                return np.array([], dtype=dset.dtype)
            data_slice = dset[actual_start_idx:actual_end_idx]
            return data_slice
    except OSError as e:
        logger.error("Error reading slice HDF5 %s: %s", filepath.name, e)
        return None
    except Exception as e:
        logger.error("Failed read slice HDF5 %s: %s", filepath.name, e, exc_info=True)
        return Noneimport logging
import sys
from typing import Dict, Any, Optional
from pathlib import Path
import time


def setup_logging(config: Optional[Dict[str, Any]] = None):
    """
    Configures logging for the application.

    Args:
        config (Optional[Dict[str, Any]]): Logging configuration dictionary.
            Expected keys:
            - 'level' (str): Logging level (e.g., 'DEBUG', 'INFO', 'WARNING'). Defaults to 'INFO'.
            - 'format' (str): Logging format string. Defaults to a standard format.
            - 'datefmt' (str): Date format string. Defaults to ISO 8601 format.
            - 'log_file' (Optional[str]): Path to a log file. If provided, logs will also be written here.
                                           Path can be relative to project root defined in base config, or absolute.
            - 'log_dir' (Optional[str]): Directory to store log files if log_file is just a name.
    """
    if config is None:
        config = {}

    log_level = config.get("level", "INFO").upper()
    log_format = config.get("format", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    date_fmt = config.get("datefmt", "%Y-%m-%d %H:%M:%S")

    root_logger = logging.getLogger()

    if not any(isinstance(h, logging.StreamHandler) for h in root_logger.handlers):
        console_handler = logging.StreamHandler(sys.stdout)
        console_formatter = logging.Formatter(log_format, datefmt=date_fmt)
        console_handler.setFormatter(console_formatter)
        root_logger.addHandler(console_handler)

    log_file_path = config.get("log_file")
    log_dir = config.get("log_dir")

    if log_file_path:
        if log_dir:
            log_dir_path = Path(log_dir)
            log_dir_path.mkdir(parents=True, exist_ok=True)
            if not Path(log_file_path).is_absolute() and "/" not in log_file_path and "\\" not in log_file_path:
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                log_filename = f"{Path(log_file_path).stem}_{timestamp}.log"
                full_log_path = log_dir_path / log_filename
            else:
                full_log_path = Path(log_file_path)
                full_log_path.parent.mkdir(parents=True, exist_ok=True)
        else:
            full_log_path = Path(log_file_path)
            full_log_path.parent.mkdir(parents=True, exist_ok=True)

        if not any(isinstance(h, logging.FileHandler) and Path(h.baseFilename).resolve() == full_log_path.resolve() for h in root_logger.handlers):
            try:
                file_handler = logging.FileHandler(full_log_path, mode="a")
                file_formatter = logging.Formatter(log_format, datefmt=date_fmt)
                file_handler.setFormatter(file_formatter)
                root_logger.addHandler(file_handler)
                logging.info("Logging to file: %s", full_log_path)
            except Exception as e:
                logging.error("Failed to create log file handler for %s: %s", full_log_path, e, exc_info=True)
        else:
            logging.debug("File handler for %s already exists.", full_log_path)

    try:
        level = getattr(logging, log_level, logging.INFO)
        root_logger.setLevel(level)
        for handler in root_logger.handlers:
            handler.setLevel(level)
        logging.info("Logging level set to: %s", log_level)
    except AttributeError:
        logging.error("Invalid logging level: %s. Defaulting to INFO.", log_level)
        root_logger.setLevel(logging.INFO)
        for handler in root_logger.handlers:
            handler.setLevel(logging.INFO)

    logging.getLogger("matplotlib").setLevel(logging.WARNING)
    logging.getLogger("PIL").setLevel(logging.WARNING)
    logging.getLogger("h5py").setLevel(logging.WARNING)
    logging.getLogger("numexpr").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)
    logging.getLogger("transformers").setLevel(logging.WARNING)"""PyTorch specific helpers."""

import logging
import torch

logger = logging.getLogger(__name__)

_DEVICE = None


def get_device(force_cpu: bool = False) -> torch.device:
    """
    Gets the recommended device (CUDA if available, else CPU).

    Args:
        force_cpu (bool): If True, forces the use of CPU even if CUDA is available.

    Returns:
        torch.device: The selected torch device object.
    """
    global _DEVICE
    if _DEVICE is None:
        if force_cpu:
            _DEVICE = torch.device("cpu")
            logger.info("Device forced to CPU.")
        elif torch.cuda.is_available():
            _DEVICE = torch.device("cuda")
            try:
                props = torch.cuda.get_device_properties(0)
                logger.info("CUDA available. Using GPU: %s (CUDA Compute Capability %d.%d)", props.name, props.major, props.minor)
            except Exception as e:
                logger.warning("CUDA available, but failed to get device properties: %s", e)
        else:
            _DEVICE = torch.device("cpu")
            logger.info("CUDA not available. Using CPU.")
    return _DEVICE


def check_tensor(tensor: torch.Tensor, name: str = "Tensor") -> bool:
    """
    Checks a tensor for NaN or Inf values.

    Args:
        tensor (torch.Tensor): The tensor to check.
        name (str): An optional name for the tensor for logging purposes.

    Returns:
        bool: True if the tensor is valid (no NaN/Inf), False otherwise.
    """
    has_nan = torch.isnan(tensor).any().item()
    has_inf = torch.isinf(tensor).any().item()

    if has_nan:
        logger.error("%s contains NaN values!", name)
    if has_inf:
        logger.error("%s contains Inf values!", name)

    return not (has_nan or has_inf)"""
VocSim Benchmark: Utility Functions.
"""

from .config_loader import load_config
from .logging_utils import setup_logging
from .file_utils import save_results 

__all__ = [
    "load_config",
    "setup_logging",
    "save_results",
]from collections import Counter
import gc
import importlib
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type, Union

import numpy as np
import torch
import h5py
from tqdm import tqdm
from benchmarks.base import Benchmark
from benchmarks.classification import ClassificationBenchmark
from benchmarks.clustering import ClusteringPurity
from benchmarks.cscf import CSCFBenchmark
from benchmarks.f_value import FValueBenchmark
from benchmarks.perceptual import PerceptualAlignment
from benchmarks.precision import PrecisionAtK
from benchmarks.csr import ClassSeparationRatio
from benchmarks.gsr import GlobalSeparationRate  
from benchmarks.silhouette import SilhouetteBenchmark
from utils.file_utils import get_cache_path, load_distance_matrix, save_json_results, load_benchmark_item_results


logger = logging.getLogger(__name__)

HDF5_FEATURE_DATASET_NAME = "features"
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"


class BenchmarkManager:
    """
    Orchestrates benchmark execution and caching for a dataset subset.
    """

    def __init__(self, config: Dict[str, Any], device: torch.device, root_dir: Path):
        """
        Initializes the BenchmarkManager.

        Args:
            config (Dict[str, Any]): The configuration dictionary.
            device (torch.device): The device to use for benchmark computation.
            root_dir (Path): The project root directory.
        """
        self.cfg = config
        self.device = device
        self.root_dir = root_dir
        self.benchmark_configs = config.get("benchmarks", [])
        self.raw_feature_configs_map = {fc["name"]: fc for fc in config.get("feature_extractors", []) if "name" in fc}
        self.raw_distance_configs_map = {dc["name"]: dc for dc in config.get("distances", []) if "name" in dc}
        self.base_results_dir = Path(config.get("results_dir", root_dir / "results")).resolve()
        logger.info("BenchmarkManager initialized.")

    def _get_class_from_module(self, module_name: str, class_name: str) -> Type:
        """
        Dynamically imports a class from a module path.

        Args:
            module_name (str): The dotted module path (e.g., 'benchmarks.precision').
            class_name (str): The name of the class within the module.

        Returns:
            Type: The imported class object.

        Raises:
            ImportError: If the module or class cannot be found/imported.
        """
        try:
            module = importlib.import_module(module_name)
            return getattr(module, class_name)
        except Exception as e:
            logger.error("Failed load class '%s' from '%s': %s", class_name, module_name, e, exc_info=True)
            raise ImportError(f"Failed import {class_name} from {module_name}.") from e

    def _get_benchmark(self, config: Dict[str, Any]) -> Benchmark:
        """
        Instantiates a benchmark class from its configuration.

        Args:
            config (Dict[str, Any]): The configuration dictionary for the benchmark.

        Returns:
            Benchmark: An instance of the benchmark class.

        Raises:
            ValueError: If the configuration is invalid or instantiation fails.
        """
        name = config.get("name")
        if not name:
            raise ValueError("Benchmark configuration requires a 'name' field.")
        params = config.get("params", {})
        module_path = config.get("module")
        if not module_path:
            module_name_base = name.lower().replace("benchmark", "")
            module_mappings = {"fvalue": "f_value", "precisionatk": "precision", "clusteringpurity": "clustering", "cscf": "cscf", "classseparationratio": "csr", "perceptualalignment": "perceptual", "classification": "classification"}
            module_name_base = module_mappings.get(module_name_base, module_name_base)
            module_path = f"benchmarks.{module_name_base}"
            logger.debug("Inferred module path for %s: %s", name, module_path)
        try:
            target_class = self._get_class_from_module(module_path, name)
            instance = target_class(**params)
            logger.debug("Instantiated benchmark '%s'.", name)
            return instance
        except Exception as e:
            logger.error("Failed instantiate benchmark '%s' from '%s': %s", name, module_path, e, exc_info=True)
            raise ValueError(f"Failed init benchmark '{name}': {e}") from e

    def _prepare_evaluate_kwargs(
        self, base_data: Dict, item_id_map: Optional[Dict], benchmark_config: Optional[Dict] = None
    ) -> Tuple[Dict, bool]:
        """
        Prepares keyword arguments for benchmark.evaluate(), including labels derived from item_id_map if necessary.

        Args:
            base_data (Dict): Base dictionary of evaluation arguments.
            item_id_map (Optional[Dict]): Map from original item IDs to metadata/indices.
            benchmark_config (Optional[Dict]): The specific configuration for the benchmark being prepared for.

        Returns:
            Tuple[Dict, bool]: A tuple containing:
                - Dict: The prepared keyword arguments dictionary.
                - bool: True if labels were successfully prepared or already present, False otherwise.
        """
        evaluate_kwargs = base_data.copy()
        labels_prepared_successfully = True

        if "item_id_map" not in evaluate_kwargs and item_id_map:
            evaluate_kwargs["item_id_map"] = item_id_map
        elif "item_id_map" not in evaluate_kwargs:
            logger.warning("Item ID map is missing and not provided.")

        if "labels" not in evaluate_kwargs and evaluate_kwargs.get("item_id_map"):
            item_map = evaluate_kwargs["item_id_map"]
            if isinstance(item_map, dict):
                logger.debug("Auto-preparing labels list from item_id_map...")
                indices = [v.get("index") for v in item_map.values() if isinstance(v.get("index"), int)]
                if not indices:
                    logger.warning("No valid integer indices found in item_id_map.")
                    labels_prepared_successfully = False
                else:
                    num_items = max(indices) + 1
                    ordered_labels = [None] * num_items
                    labels_found_count = 0

                    label_key_to_use = "label"
                    if benchmark_config:
                        label_key_from_bench = benchmark_config.get("params", {}).get("label_source_key")
                        if label_key_from_bench:
                            label_key_to_use = label_key_from_bench
                            logger.info("Using benchmark-specified label_source_key: '%s'", label_key_to_use)
                        else:
                            if any("label" in v for v in item_map.values() if isinstance(v, dict)):
                                label_key_to_use = "label"
                                logger.debug("Using detected label key: '%s' (benchmark default)", label_key_to_use)
                            elif any("speaker" in v for v in item_map.values() if isinstance(v, dict)):
                                label_key_to_use = "speaker"
                                logger.debug("Using detected label key: '%s' (benchmark default, 'label' not found)", label_key_to_use)
                            else:
                                logger.warning("Neither benchmark key, 'label', nor 'speaker' found. Defaulting to '%s'.", label_key_to_use)
                    else:
                        if any("label" in v for v in item_map.values() if isinstance(v, dict)):
                            label_key_to_use = "label"
                        elif any("speaker" in v for v in item_map.values() if isinstance(v, dict)):
                            label_key_to_use = "speaker"
                        logger.warning("Benchmark config not passed to label prep. Guessed label key: '%s'.", label_key_to_use)

                    num_skipped_index_bounds = 0
                    num_skipped_missing_key = 0
                    num_skipped_none_value = 0
                    for item_key, item_data in item_map.items():
                        idx = item_data.get("index")
                        label_val = item_data.get(label_key_to_use)

                        if not isinstance(idx, int):
                            continue

                        if 0 <= idx < num_items:
                            if label_key_to_use not in item_data:
                                num_skipped_missing_key += 1
                                ordered_labels[idx] = None
                            elif label_val is not None:
                                ordered_labels[idx] = str(label_val)
                                labels_found_count += 1
                            else:
                                num_skipped_none_value += 1
                                ordered_labels[idx] = None
                        else:
                            num_skipped_index_bounds += 1

                    if num_skipped_index_bounds > 0:
                        logger.warning("Skipped %d items due to out-of-bounds index during label prep.", num_skipped_index_bounds)
                    if num_skipped_missing_key > 0:
                        logger.warning("Label key '%s' was missing for %d items.", label_key_to_use, num_skipped_missing_key)
                    if num_skipped_none_value > 0:
                        logger.warning("Label key '%s' had None value for %d items.", label_key_to_use, num_skipped_none_value)

                    if labels_found_count > 0:
                        evaluate_kwargs["labels"] = ordered_labels
                        if labels_found_count < len(item_map):
                            logger.debug("Label coverage partial (%d/%d items).", labels_found_count, len(item_map))
                    else:
                        logger.error("No valid labels found in item_id_map using key '%s'.", label_key_to_use)
                        labels_prepared_successfully = False
            else:
                logger.error("Cannot auto-prepare labels: item_id_map missing or invalid type.")
                labels_prepared_successfully = False
        elif "labels" in evaluate_kwargs:
            logger.debug("Using pre-provided 'labels' list.")
            if not isinstance(evaluate_kwargs["labels"], list):
                try:
                    evaluate_kwargs["labels"] = list(evaluate_kwargs["labels"])
                except TypeError:
                    logger.error("Provided labels not list-convertible (%s).", type(evaluate_kwargs["labels"]))
                    labels_prepared_successfully = False
            if labels_prepared_successfully:
                evaluate_kwargs["labels"] = [(str(label) if label is not None else None) for label in evaluate_kwargs["labels"]]
                logger.debug("Ensured pre-provided labels are list/str/None (len: %d)", len(evaluate_kwargs["labels"]))

        return evaluate_kwargs, labels_prepared_successfully

    def run_subset_benchmarks(
        self,
        subset_features_dir: Path,
        subset_dataset_obj: Any,
        item_id_map: Dict[str, Dict],
        dataset_cache_id: str,
        current_subset_key: str,
        subset_cache_dir: Path,
        feature_paths: Dict[str, Path],
        distance_paths: Dict[Tuple[str, str], Path],
    ) -> Dict[str, Any]:
        """
        Runs benchmarks using available feature and distance paths.

        Args:
            subset_features_dir (Path): Directory containing cached feature files for this subset.
            subset_dataset_obj (Any): The actual dataset subset object (e.g., Hugging Face Dataset).
            item_id_map (Dict[str, Dict]): Map from original item IDs to metadata/indices.
            dataset_cache_id (str): Unique identifier for this dataset subset/split.
            current_subset_key (str): Name of the current subset.
            subset_cache_dir (Path): Directory for storing benchmark item cache files for this subset.
            feature_paths (Dict[str, Path]): Dictionary mapping feature names to paths of cached feature files.
            distance_paths (Dict[Tuple[str, str], Path]): Dictionary mapping (feature_name, distance_name) tuples to paths of cached distance matrix files.

        Returns:
            Dict[str, Any]: A nested dictionary containing benchmark results for the subset.
        """
        logger.info("--- Starting Benchmarking for Subset: %s ---", current_subset_key)
        logger.debug("Received distance_paths dict:\n%s", distance_paths)

        subset_results: Dict[str, Dict[str, Any]] = {}
        base_evaluate_data: Dict[str, Any] = {}
        if subset_dataset_obj is not None:
            base_evaluate_data["dataset"] = subset_dataset_obj
        if not item_id_map:
            logger.warning("Item ID map missing for subset %s, benchmarks needing labels may fail.", current_subset_key)

        feature_iter = tqdm(feature_paths.items(), desc=f"Benching Features [{current_subset_key}]", leave=False)
        for feature_name, feature_hdf5_path in feature_iter:
            if not feature_hdf5_path or not feature_hdf5_path.exists():
                logger.warning("Feature path for '%s' (%s) is invalid. Skipping benchmarks.", feature_name, feature_hdf5_path)
                subset_results.setdefault(feature_name, {})["error"] = f"Input feature file missing: {feature_hdf5_path}"
                continue

            feature_iter.set_postfix_str(f"{feature_name}")
            raw_feature_conf = self.raw_feature_configs_map.get(feature_name)
            if not raw_feature_conf:
                logger.error("Config missing for available feature '%s'. Skipping benchmarks.", feature_name)
                subset_results.setdefault(feature_name, {})["error"] = f"Config missing for feature {feature_name}"
                continue
            effective_feature_config = raw_feature_conf

            logger.info("-- Benchmarking Feature: %s (using file %s) --", feature_name, feature_hdf5_path.name)
            subset_results.setdefault(feature_name, {})

            for bench_conf in self.benchmark_configs:
                benchmark_name = bench_conf.get("name")
                if not benchmark_name:
                    continue

                target_features = bench_conf.get("target_features")
                if target_features and isinstance(target_features, list) and feature_name not in target_features:
                    continue

                try:
                    benchmark = self._get_benchmark(bench_conf)
                except Exception as e:
                    logger.error("Skip bench %s: Init fail: %s", benchmark_name, e)
                    continue

                needs_features = isinstance(benchmark, ClassificationBenchmark) or \
               (isinstance(benchmark, ClusteringPurity) and not getattr(benchmark, "use_dist_matrix", False)) or \
               (isinstance(benchmark, SilhouetteBenchmark) and not getattr(benchmark, "use_distance_matrix", False))                
                needs_distances = isinstance(benchmark, (PerceptualAlignment, PrecisionAtK, FValueBenchmark, CSCFBenchmark, ClassSeparationRatio, GlobalSeparationRate)) or \
                (isinstance(benchmark, ClusteringPurity) and getattr(benchmark, "use_dist_matrix", True)) or \
                (isinstance(benchmark, SilhouetteBenchmark) and getattr(benchmark, "use_distance_matrix", True))
                if needs_distances:
                    metric_key = "distance_based"
                    subset_results[feature_name].setdefault(metric_key, {})
                    target_distances = bench_conf.get("target_distances")

                    processed_any_distance = False
                    for dist_name, dist_conf in self.raw_distance_configs_map.items():
                        if target_distances and isinstance(target_distances, list) and dist_name not in target_distances:
                            continue

                        matrix_key = (feature_name, dist_name)
                        logger.debug("Checking distance_paths for key: %s (Type: %s, %s)", matrix_key, type(matrix_key[0]), type(matrix_key[1]))

                        dist_matrix_path = distance_paths.get(matrix_key)

                        if dist_matrix_path is None:
                            found_variation = False
                            for k_feat, k_dist in distance_paths.keys():
                                if k_feat.lower() == feature_name.lower() and k_dist.lower() == dist_name.lower():
                                    logger.warning("Key %s NOT FOUND, but found case variation: (%s, %s)", matrix_key, k_feat, k_dist)
                                    found_variation = True
                                    break
                            if not found_variation:
                                logger.debug("Key %s NOT FOUND in distance_paths.", matrix_key)
                        else:
                            logger.debug("Key %s FOUND in distance_paths: %s", matrix_key, dist_matrix_path)

                        if not dist_matrix_path:
                            feature_expects_dist = raw_feature_conf.get("compute_distances_for") is None or dist_name in raw_feature_conf.get("compute_distances_for", [])

                            if feature_expects_dist:
                                logger.warning("Distance matrix path missing for %s. Cannot run '%s'.", matrix_key, benchmark_name)
                            else:
                                logger.debug("Skipping distance '%s' for feature '%s' as not specified in 'compute_distances_for'.", dist_name, feature_name)
                                continue

                            subset_results[feature_name][metric_key].setdefault(dist_name, {})[benchmark_name] = {"error": f"Required distance matrix for {dist_name} unavailable."}
                            continue
                        if not dist_matrix_path.exists():
                            logger.error("Distance matrix file missing at expected path %s. Cannot run '%s'.", dist_matrix_path, benchmark_name)
                            subset_results[feature_name][metric_key].setdefault(dist_name, {})[benchmark_name] = {"error": f"Distance matrix file missing: {dist_matrix_path.name}"}
                            continue

                        processed_any_distance = True
                        logger.debug("Prep D-Bench '%s' | Feat: %s | Dist: %s (using %s)", benchmark_name, feature_name, dist_name, dist_matrix_path.name)
                        combined_config_bench = {"feature_config": effective_feature_config, "distance_config": dist_conf, "benchmark_config": bench_conf, "name": f"{feature_name}_{dist_name}_{benchmark_name}"}

                        bench_cache_path = get_cache_path(subset_cache_dir, "bench_item", dataset_cache_id, combined_config_bench)
                        bench_res = load_benchmark_item_results(bench_cache_path)

                        if bench_res is None:
                            logger.info("Running D-Bench '%s' | Feat: %s | Dist: %s", benchmark_name, feature_name, dist_name)
                            loaded_dist_matrix = load_distance_matrix(dist_matrix_path)

                            if loaded_dist_matrix is None:
                                logger.error("Failed load dist matrix %s for %s", dist_matrix_path.name, benchmark_name)
                                bench_res = {"error": f"Dist matrix load fail: {dist_matrix_path.name}"}
                            else:
                                evaluate_kwargs, labels_ok = self._prepare_evaluate_kwargs(base_evaluate_data, item_id_map, bench_conf)
                                evaluate_kwargs["distance_matrix"] = loaded_dist_matrix
                                evaluate_kwargs["distance_matrix_path"] = dist_matrix_path
                                evaluate_kwargs["feature_config"] = effective_feature_config

                                requires_labels_or_map = any(isinstance(benchmark, cls) for cls in [PerceptualAlignment, PrecisionAtK, FValueBenchmark, CSCFBenchmark, ClassSeparationRatio]) or (isinstance(benchmark, ClusteringPurity) and getattr(benchmark, "use_dist_matrix", True))

                                run_this_benchmark = True
                                if requires_labels_or_map and not labels_ok:
                                    if not evaluate_kwargs.get("item_id_map"):
                                        logger.error("[%s] Skip %s/%s: Label prep failed and item_id_map missing.", benchmark_name, feature_name, dist_name)
                                        bench_res = {"error": "Label preparation failed and item_id_map missing."}
                                        run_this_benchmark = False
                                    else:
                                        logger.warning("[%s] Label prep failed for %s/%s, but item_id_map is present. Proceeding, benchmark must handle map.", benchmark_name, feature_name, dist_name)

                                if run_this_benchmark:
                                    try:
                                        results = benchmark.evaluate(**evaluate_kwargs)
                                        bench_res = results
                                    except Exception as e:
                                        logger.error("Error D-Bench %s (%s/%s): %s", benchmark_name, feature_name, dist_name, e, exc_info=True)
                                        bench_res = {"error": str(e)}

                                del loaded_dist_matrix
                                gc.collect()
                                if self.device.type == "cuda":
                                    torch.cuda.empty_cache()

                            save_json_results(bench_res, bench_cache_path, f"{benchmark_name} result")

                        subset_results[feature_name][metric_key].setdefault(dist_name, {})[benchmark_name] = bench_res

                    if not processed_any_distance:
                        logger.debug("No applicable/available distance matrices found for D-Bench '%s' on feature '%s'.", benchmark_name, feature_name)

                if needs_features:
                    metric_key = "feature_based"
                    subset_results[feature_name].setdefault(metric_key, {})
                    logger.debug("Prep F-Bench '%s' | Feat: %s (using %s)", benchmark_name, feature_name, feature_hdf5_path.name)
                    combined_config_bench = {"feature_config": effective_feature_config, "benchmark_config": bench_conf, "name": f"{feature_name}_{benchmark_name}"}

                    bench_cache_path = get_cache_path(subset_cache_dir, "bench_item", dataset_cache_id, combined_config_bench)
                    bench_res = load_benchmark_item_results(bench_cache_path)

                    if bench_res is None:
                        logger.info("Running F-Bench '%s' | Feature: %s", benchmark_name, feature_name)
                        evaluate_kwargs, labels_ok = self._prepare_evaluate_kwargs(base_evaluate_data, item_id_map, bench_conf)
                        evaluate_kwargs["feature_hdf5_path"] = feature_hdf5_path
                        evaluate_kwargs["feature_config"] = effective_feature_config

                        requires_labels = isinstance(benchmark, ClassificationBenchmark) or (isinstance(benchmark, ClusteringPurity) and not getattr(benchmark, "use_dist_matrix", False))

                        run_this_benchmark = True
                        if requires_labels and not labels_ok:
                            if not evaluate_kwargs.get("item_id_map"):
                                logger.error("[%s] Skip %s: Label prep failed and item_id_map missing.", benchmark_name, feature_name)
                                bench_res = {"error": "Label prep failed and item_id_map missing."}
                                run_this_benchmark = False
                            else:
                                logger.warning("[%s] Label prep failed for %s, but item_id_map is present. Proceeding, benchmark must handle map.", benchmark_name, feature_name)

                        if run_this_benchmark:
                            try:
                                results = benchmark.evaluate(**evaluate_kwargs)
                                bench_res = results
                            except FileNotFoundError:
                                logger.error("Feature file %s missing during F-Bench %s execution.", feature_hdf5_path.name, benchmark_name)
                                bench_res = {"error": f"Feature file missing: {feature_hdf5_path.name}"}
                            except Exception as e:
                                logger.error("Error F-Bench %s (%s): %s", benchmark_name, feature_name, e, exc_info=True)
                                bench_res = {"error": str(e)}
                            finally:
                                gc.collect()
                                if self.device.type == "cuda":
                                    torch.cuda.empty_cache()

                        save_json_results(bench_res, bench_cache_path, f"{benchmark_name} result")

                    subset_results[feature_name][metric_key][benchmark_name] = bench_res

                if "benchmark" in locals():
                    del benchmark
                gc.collect()

        if self.device.type == "cuda":
            torch.cuda.empty_cache()
        logger.info("--- Benchmarking Completed for Subset: %s ---", current_subset_key)
        return subset_results"""
Manages dataset loading, filtering, and metadata mapping for audio datasets.
"""

import logging
from collections import Counter
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from datasets import Audio, DatasetDict, load_dataset, load_from_disk

logger = logging.getLogger(__name__)


class DatasetManager:
    """
    Handles dataset loading, subset filtering, and metadata map construction.
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the DatasetManager with configuration settings.

        Args:
            config: Configuration dictionary containing dataset parameters.
        """
        self.cfg = config
        self.target_sr = config.get("target_sample_rate", 16000)
        self.full_dataset_obj: Optional[Any] = None
        self.base_dataset_cache_id: Optional[str] = None
        self.current_subset_key: Optional[str] = None
        self.current_dataset_cache_id: Optional[str] = None
        self.item_id_to_metadata: Dict[str, Dict] = {}
        self.current_subset_dataset: Optional[Any] = None
        self.dataset_config: Dict[str, Any] = config.get("dataset", {})

    def get_labels_for_current_dataset(self, label_source_key: Optional[str] = None) -> Optional[List[Any]]:
        """
        Extracts labels from the current_subset_dataset based on the label_source_key.

        Args:
            label_source_key (Optional[str]): The key in the dataset items to use as the label source.

        Returns:
            Optional[List[Any]]: A list of labels corresponding to the order of items
                                 in the current_subset_dataset, or None if no dataset is loaded
                                 or extraction fails.
        """
        if self.current_subset_dataset is None:
            logger.error("No current_subset_dataset selected. Call get_subset_dataset() first.")
            return None

        if label_source_key:
            actual_label_key_to_use = label_source_key
            logger.info("Using explicitly provided label_source_key: '%s' for current subset '%s'.", actual_label_key_to_use, self.current_subset_key)
        elif self.dataset_config.get("default_label_column"):
            actual_label_key_to_use = self.dataset_config.get("default_label_column")
            logger.info("Using default_label_column from dataset config: '%s' for current subset '%s'.", actual_label_key_to_use, self.current_subset_key)
        else:
            actual_label_key_to_use = "label"
            logger.info("No specific label key provided, defaulting to '%s' for current subset '%s'.", actual_label_key_to_use, self.current_subset_key)

        labels_list = []
        missing_key_count = 0
        none_value_count = 0

        try:
            for i, item in enumerate(self.current_subset_dataset):
                label_value = item.get(actual_label_key_to_use)
                if label_value is not None:
                    labels_list.append(str(label_value))
                else:
                    if actual_label_key_to_use not in item:
                        missing_key_count += 1
                    else:
                        none_value_count += 1
                    labels_list.append(None)

            if missing_key_count > 0:
                logger.warning(
                    "Label key '%s' was missing in %d items out of %d for subset '%s'." " These will be treated as None/unlabeled.",
                    actual_label_key_to_use,
                    missing_key_count,
                    len(self.current_subset_dataset),
                    self.current_subset_key,
                )
            if none_value_count > 0:
                logger.warning(
                    "Label key '%s' had a None value in %d items for subset '%s'." " These are treated as unlabeled.",
                    actual_label_key_to_use,
                    none_value_count,
                    len(self.current_subset_dataset),
                    self.current_subset_key,
                )

            if not labels_list and len(self.current_subset_dataset) > 0:
                logger.warning(
                    "Extracted an empty list of labels, but current_subset_dataset has %d items. This is unexpected if items were expected to be labeled with key '%s'.",
                    len(self.current_subset_dataset),
                    actual_label_key_to_use,
                )
            elif not labels_list:
                logger.info("Extracted an empty list of labels for an empty current_subset_dataset for key '%s'.", actual_label_key_to_use)
            else:
                logger.info("Successfully extracted %d labels using key '%s'. First few: %s", len(labels_list), actual_label_key_to_use, labels_list[:5] if len(labels_list) > 5 else labels_list)

            return labels_list

        except Exception as e:
            logger.error("Error extracting labels with key '%s' for subset '%s': %s", actual_label_key_to_use, self.current_subset_key, e, exc_info=True)
            return None

    def load_full_dataset(self) -> bool:
        """
        Loads the full dataset as specified in the configuration.

        Returns:
            True if the dataset is loaded successfully, False otherwise.
        """
        dataset_config = self.cfg.get("dataset")
        if not dataset_config:
            logger.error("Dataset configuration is missing.")
            return False

        dataset_id = dataset_config.get("id")
        split = dataset_config.get("split", "train") or "all"
        if not dataset_id:
            logger.error("Dataset configuration requires an 'id' field.")
            return False

        safe_dataset_id = Path(dataset_id).name if Path(dataset_id).is_dir() else dataset_id.replace("/", "_").replace("\\", "_").replace(":", "_")
        self.base_dataset_cache_id = f"{safe_dataset_id}_all_{split}"

        logger.info("Loading dataset '%s' (Split: %s, Cache ID: %s)", dataset_id, split, self.base_dataset_cache_id)
        try:
            load_path = Path(dataset_id)
            if load_path.is_dir():
                dataset = load_from_disk(str(load_path))
                if isinstance(dataset, DatasetDict):
                    if split not in dataset:
                        raise ValueError(f"Split '{split}' not found in DatasetDict at {load_path}")
                    dataset = dataset[split]
            else:
                dataset = load_dataset(dataset_id, name=None, split=split, trust_remote_code=True)

            if "audio" in dataset.column_names:
                if not isinstance(dataset.features["audio"], Audio):
                    dataset = dataset.cast_column("audio", Audio())
                current_sr = dataset.features["audio"].sampling_rate
                if current_sr != self.target_sr:
                    logger.info("Resampling audio to %d Hz", self.target_sr)
                    dataset = dataset.cast_column("audio", Audio(sampling_rate=self.target_sr))
                else:
                    logger.info("Audio sampling rate matches target: %d Hz", self.target_sr)
            else:
                logger.warning("Dataset lacks an 'audio' column.")

            self.full_dataset_obj = dataset
            logger.info("Dataset loaded successfully.")

            if dataset_config.get("subsets_to_run") and "subset" not in dataset.column_names:
                logger.error("Subsets requested but 'subset' column is missing.")
                self.full_dataset_obj = None
                return False
            return True

        except Exception as e:
            logger.error("Failed to load dataset '%s': %s", dataset_id, e, exc_info=True)
            self.full_dataset_obj = None
            return False

    def get_subset_dataset(self, subset_key: str) -> Optional[Tuple[Any, str]]:
        """
        Filters the full dataset to return a subset and its cache ID.

        Args:
            subset_key: Identifier for the subset to filter (e.g., 'all' for full dataset).

        Returns:
            A tuple of (subset dataset object, cache ID), or None if filtering fails.
        """
        if self.full_dataset_obj is None:
            logger.error("Full dataset not loaded.")
            return None

        self.current_subset_key = subset_key
        dataset_config = self.cfg.get("dataset", {})
        split = dataset_config.get("split", "train") or "all"
        base_id_part = self.base_dataset_cache_id.split("_all_")[0]

        if subset_key == "all":
            subset_dataset = self.full_dataset_obj
            self.current_dataset_cache_id = self.base_dataset_cache_id
            logger.info("Using full dataset for subset 'all'.")
        else:
            logger.info("Filtering dataset for subset '%s'", subset_key)
            try:
                if "subset" not in self.full_dataset_obj.column_names:
                    logger.error("Dataset lacks 'subset' column for filtering.")
                    return None
                subset_dataset = self.full_dataset_obj.filter(lambda x: x.get("subset") == subset_key, load_from_cache_file=False)
                self.current_dataset_cache_id = f"{base_id_part}_{subset_key}_{split}"
                num_samples = len(subset_dataset) if hasattr(subset_dataset, "__len__") else "iterable"
                if isinstance(num_samples, int) and num_samples == 0:
                    logger.warning("Subset '%s' contains 0 samples.", subset_key)
                    return None
                logger.info("Filtered subset '%s' with %s samples.", subset_key, num_samples)
            except Exception as e:
                logger.error("Failed to filter subset '%s': %s", subset_key, e)
                return None

        self.current_subset_dataset = subset_dataset
        self._build_item_id_map(subset_dataset)
        return subset_dataset, self.current_dataset_cache_id

    def _build_item_id_map(self, dataset: Any) -> None:
        """
        Constructs a metadata map for items in the dataset.

        Args:
            dataset: Dataset object to process.
        """
        self.item_id_to_metadata = {}
        logger.info("Building item ID map...")

        id_fields = ["original_name", "unique_id", "id", "filename", "path"]
        id_field = next(
            (field for field in id_fields if field in dataset.column_names or (field == "path" and "audio" in dataset.column_names)),
            None,
        )
        if id_field == "path" and "audio" in dataset.column_names:
            id_field = "audio.path"

        if id_field:
            logger.info("Using '%s' for item IDs.", id_field)
            try:
                ids = []
                if "." in id_field:
                    base, nested = id_field.split(".", 1)
                    ids = [str(item.get(base, {}).get(nested, i)) for i, item in enumerate(dataset)]
                else:
                    ids = [str(item.get(id_field, i)) for i, item in enumerate(dataset)]

                id_counts = Counter(ids)
                duplicates = {k: v for k, v in id_counts.items() if v > 1}
                if duplicates:
                    logger.warning("Found %d duplicate IDs. Appending suffixes.", len(duplicates))
                    final_ids = []
                    seen_counts = Counter()
                    for id_val in ids:
                        if id_counts[id_val] > 1:
                            final_ids.append(f"{id_val}_dup{seen_counts[id_val]}")
                            seen_counts[id_val] += 1
                        else:
                            final_ids.append(id_val)
                    ids = final_ids
                else:
                    logger.info("All item IDs are unique.")

                for i, (item_id, item) in enumerate(zip(ids, dataset)):
                    metadata = {
                        k: item.get(k) for k in ["label", "speaker", "subset", "original_name"] if item.get(k) is not None
                    }
                    metadata["index"] = i
                    self.item_id_to_metadata[item_id] = metadata

            except Exception as e:
                logger.error("Failed to build ID map using '%s': %s", id_field, e, exc_info=True)
                id_field = None

        if not id_field:
            logger.warning("No suitable ID field found. Using indices as IDs.")
            self.item_id_to_metadata = {
                str(i): {
                    "index": i,
                    "label": item.get("label"),
                    "speaker": item.get("speaker"),
                    "subset": item.get("subset"),
                    "original_name": item.get("original_name"),
                }
                for i, item in enumerate(dataset)
            }

        logger.info("Built item ID map for %d items.", len(self.item_id_to_metadata))

    def get_current_item_map(self) -> Dict[str, Dict]:
        """
        Retrieves the current item ID to metadata mapping.

        Returns:
            A dictionary mapping item IDs to their metadata.
        """
        return self.item_id_to_metadataimport gc
import importlib
import logging
import math
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type

import numpy as np
import torch
from tqdm.auto import tqdm
import h5py

from distances.base import DistanceCalculator
from utils.file_utils import (
    HDF5_DATASET_NAME,
    get_cache_path,
    find_cache_path,
    read_hdf5_metadata,
)
from .feature_manager import FeatureManager
from h5py import Dataset, File


logger = logging.getLogger(__name__)
HDF5_DISTANCE_DATASET_NAME = "distance_matrix"
HDF5_FEATURE_DATASET_NAME = "features"


class DistanceManager:
    """
    Manages computation and caching of pairwise distance matrices.
    Reads features chunk-by-chunk. Writes distances chunk-by-chunk.
    Uses find_cache_path for loading (loose match fallback) and get_cache_path for saving.
    """

    def __init__(self, config: Dict[str, Any], base_features_cache_dir: Path, device: torch.device):
        """
        Initializes the DistanceManager.

        Args:
            config (Dict[str, Any]): The configuration dictionary.
            base_features_cache_dir (Path): The base directory for caching features.
            device (torch.device): The device to use for computation.
        """
        self.cfg = config
        self.base_features_dir = base_features_cache_dir
        self.device = device
        self.distance_configs = config.get("distances", [])
        self._calculator_instances_cache: Dict[str, DistanceCalculator] = {}
        self.gpu_block_size = config.get("distance_gpu_block_size", 1024)
        # Ensure raw_distance_configs_map is initialized even if empty in config
        self.raw_distance_configs_map = {dc["name"]: dc for dc in config.get("distances", []) if "name" in dc}
        logger.info("Initialized DistanceManager with GPU block size (Distance Matrix Blocks): %d", self.gpu_block_size)

    def _get_class_from_module(self, module_name: str, class_name: str) -> Type:
        """
        Dynamically imports a class from a module path.

        Args:
            module_name (str): The dotted module path.
            class_name (str): The name of the class within the module.

        Returns:
            Type: The imported class object.

        Raises:
            ImportError: If the module or class cannot be found/imported.
        """
        try:
            module = importlib.import_module(module_name)
            return getattr(module, class_name)
        except Exception as e:
            logger.error("Failed load class '%s' from '%s': %s", class_name, module_name, e, exc_info=True)
            raise ImportError(f"Failed import {class_name} from {module_name}.") from e

    def _get_distance_calculator(self, config: Dict[str, Any]) -> DistanceCalculator:
        """
        Gets or instantiates a distance calculator.

        Args:
            config (Dict[str, Any]): The configuration dictionary for the distance calculator.

        Returns:
            DistanceCalculator: An instance of the distance calculator.

        Raises:
            ValueError: If the configuration is invalid.
            ImportError: If the class cannot be instantiated.
        """
        name = config.get("name")
        if not name:
            raise ValueError("Distance configuration requires a 'name' field.")
        if name in self._calculator_instances_cache:
            return self._calculator_instances_cache[name]
        params = config.get("params", {})
        dist_name_lower = name.lower()
        class_name = config.get("class")
        if not class_name:
            if dist_name_lower == "cosine":
                class_name = "CosineDistance"
            elif dist_name_lower == "euclidean":
                class_name = "EuclideanDistance"
            elif dist_name_lower == "spearman":
                class_name = "SpearmanDistance"
            else:
                class_name = f"{name.capitalize()}Distance"
        module_path = config.get("module", f"distances.{dist_name_lower}")
        try:
            calculator_class = self._get_class_from_module(module_path, class_name)
            instance = calculator_class(**params)
            self._calculator_instances_cache[name] = instance
            logger.info("Instantiated distance calculator '%s' (Config Name: %s).", class_name, name)
            return instance
        except Exception as e:
            logger.error("Failed instantiate distance calculator '%s' (Config Name: %s) from module '%s': %s", class_name, name, module_path, e, exc_info=True)
            raise ImportError(f"Failed instantiate {class_name} from {module_path}: {e}") from e

    def _clear_calculator_cache(self) -> None:
        """Clears cached distance calculator instances."""
        logger.debug("Clearing distance calculator instance cache.")
        self._calculator_instances_cache.clear()
        gc.collect()

    def _compute_distances_block_gpu_hdf5(
        self,
        calculator: DistanceCalculator,
        h5_features: "Dataset",
        distance_h5_path: Path,
        n_samples: int,
        distance_dataset_name: str = HDF5_DISTANCE_DATASET_NAME,
    ) -> bool:
        """
        Computes pairwise distances chunk-by-chunk on GPU using data from HDF5 and saves to a NEW HDF5 file.

        Args:
            calculator (DistanceCalculator): The distance calculator instance.
            h5_features (Dataset): The h5py Dataset object containing features.
            distance_h5_path (Path): Path to save the new distance matrix HDF5 file.
            n_samples (int): The total number of samples (rows) in the feature dataset.
            distance_dataset_name (str): Name of the dataset to create in the distance HDF5 file.

        Returns:
            bool: True if the distance matrix was computed and saved successfully, False otherwise.
        """
        calc_name = calculator.__class__.__name__
        feature_h5_path_str = h5_features.file.filename
        if n_samples <= 0:
            logger.warning("[%s] n_samples is 0, skipping.", calc_name)
            try:
                distance_h5_path.parent.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            try:
                with h5py.File(distance_h5_path, "w") as f_dist:
                    f_dist.create_dataset(distance_dataset_name, shape=(0, 0), dtype=np.float32)
                logger.info("Created empty distance file: %s", distance_h5_path.name)
                return True
            except Exception as e:
                logger.error("Failed write empty distance matrix: %s", e)
                return False
        if n_samples == 1:
            logger.info("[%s] n_samples is 1.", calc_name)
            try:
                distance_h5_path.parent.mkdir(parents=True, exist_ok=True)
            except Exception:
                pass
            try:
                with h5py.File(distance_h5_path, "w") as f_dist:
                    f_dist.create_dataset(distance_dataset_name, shape=(1, 1), dtype=np.float32, data=[[0.0]])
                return True
            except Exception as e:
                logger.error("Failed write single-sample distance matrix: %s", e)
                return False

        logger.info("Starting HDF5->HDF5 Block Distance Calc (%s) for N=%d, Block=%d", calc_name, n_samples, self.gpu_block_size)
        start_time = time.time()
        block_size = self.gpu_block_size
        num_blocks = math.ceil(n_samples / block_size)
        try:
            distance_h5_path.parent.mkdir(parents=True, exist_ok=True)
            with h5py.File(distance_h5_path, "w") as f_dist:
                if h5_features.shape[0] != n_samples:
                    logger.error("Feature count mismatch (%d vs %d)", h5_features.shape[0], n_samples)
                    return False
                h5_distances = f_dist.create_dataset(distance_dataset_name, shape=(n_samples, n_samples), dtype=np.float32, compression="gzip", compression_opts=4, shuffle=True, fletcher32=True)
                total_block_pairs = num_blocks * (num_blocks + 1) // 2
                pbar = tqdm(total=total_block_pairs, desc=f"Dist Blocks [{calc_name}]", leave=False)
                for i in range(num_blocks):
                    start_i = i * block_size
                    end_i = min((i + 1) * block_size, n_samples)
                    try:
                        block_i_cpu = h5_features[start_i:end_i]
                    except Exception as read_err:
                        logger.error("Read block i=%d fail: %s", i, read_err)
                        pbar.update(num_blocks - i)
                        continue
                    block_i_flat = block_i_cpu.reshape(block_i_cpu.shape[0], -1).astype(np.float32)
                    block_i_gpu = torch.from_numpy(block_i_flat).to(self.device)
                    del block_i_cpu, block_i_flat
                    gc.collect()
                    for j in range(i, num_blocks):
                        start_j = j * block_size
                        end_j = min((j + 1) * block_size, n_samples)
                        try:
                            try:
                                block_j_cpu = h5_features[start_j:end_j]
                            except Exception as read_err:
                                logger.error("Read block j=%d fail: %s", j, read_err)
                                pbar.update(1)
                                continue
                            block_j_flat = block_j_cpu.reshape(block_j_cpu.shape[0], -1).astype(np.float32)
                            block_j_gpu = torch.from_numpy(block_j_flat).to(self.device)
                            del block_j_cpu, block_j_flat
                            gc.collect()
                            dist_ij_gpu = calculator.compute_pairwise(block_i_gpu, block_j_gpu)
                            if dist_ij_gpu is None:
                                raise RuntimeError(f"Calculator {calc_name} returned None for block ({i},{j})")
                            dist_block = dist_ij_gpu.cpu().numpy().astype(np.float32)
                            h5_distances[start_i:end_i, start_j:end_j] = dist_block
                            if i != j:
                                h5_distances[start_j:end_j, start_i:end_i] = dist_block.T
                            del dist_ij_gpu, block_j_gpu, dist_block
                            if self.device.type == "cuda":
                                torch.cuda.empty_cache()
                        except Exception as e_inner:
                            logger.error("Error block (%d vs %d) (%s): %s", i, j, calc_name, e_inner, exc_info=True)
                        finally:
                            if "block_j_gpu" in locals() and block_j_gpu is not None:
                                del block_j_gpu
                            gc.collect()
                        pbar.update(1)
                    del block_i_gpu
                    gc.collect()
                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()
                pbar.close()
                logger.debug("Zeroing diagonal...")
                diag_chunk_size = min(n_samples, max(block_size * 4, 4096))
                for k in range(0, n_samples, diag_chunk_size):
                    end_k = min(k + diag_chunk_size, n_samples)
                    if k >= end_k:
                        continue
                    try:
                        block_data = h5_distances[k:end_k, k:end_k]
                        np.fill_diagonal(block_data, 0.0)
                        h5_distances[k:end_k, k:end_k] = block_data
                    except Exception as diag_err:
                        logger.error("Error zeroing diag %d:%d: %s", k, end_k, diag_err)
                logger.debug("Diagonal zeroing done.")
        except Exception as e:
            logger.error(
                "HDF5 distance calculation failed (%s) for %s -> %s: %s",
                calc_name,
                feature_h5_path_str,
                distance_h5_path,
                e,
                exc_info=True,
            )
            try:
                if distance_h5_path.exists():
                    distance_h5_path.unlink()
                    logger.info("Deleted incomplete distance file: %s", distance_h5_path.name)
            except OSError as unlink_err:
                logger.error("Failed delete incomplete dist file: %s", unlink_err)
            return False
        elapsed = time.time() - start_time
        logger.info("HDF5 Block Distance Calc (%s) done in %.2fs. Output: %s", calc_name, elapsed, distance_h5_path.name)
        return True

    def process_subset_distances(
        self,
        dataset_cache_id: str,
        current_subset_key: str,
        feature_manager: "FeatureManager",
        subset_features_dir: Path,
        item_id_map: Dict[str, Dict],
        feature_paths: Dict[str, Path],
        run_steps: List[str],
    ) -> Dict[Tuple[str, str], Path]:
        """
        Computes or finds existing distance matrices for the given features.

        Args:
            dataset_cache_id (str): Unique identifier for this dataset subset/split.
            current_subset_key (str): Name of the current subset.
            feature_manager (FeatureManager): The FeatureManager instance.
            subset_features_dir (Path): Directory containing cached feature files for this subset.
            item_id_map (Dict[str, Dict]): Map from original item IDs to metadata/indices.
            feature_paths (Dict[str, Path]): Dictionary mapping feature names to paths of cached feature files.
            run_steps (List[str]): List of steps being run in the pipeline.

        Returns:
            Dict[Tuple[str, str], Path]: A dictionary mapping (feature_name, distance_name) tuples to
                                        paths of distance matrix files that were successfully processed or found.
        """
        logger.info("--- Distance Computation (Steps: %s) for Subset: %s ---", run_steps, current_subset_key)
        distance_file_paths: Dict[Tuple[str, str], Path] = {}
        n_samples_expected = len(item_id_map)
        if n_samples_expected == 0:
            logger.warning("No items in %s. Skip dist.", current_subset_key)
            return distance_file_paths
        self._clear_calculator_cache()

        compute_distances_step = "distances" in run_steps

        feature_iter = tqdm(feature_paths.items(), desc=f"Distances [{current_subset_key}]", leave=False)
        for feature_name, feature_path in feature_iter:
            feature_iter.set_postfix_str(f"{feature_name}")

            if not feature_path or not feature_path.exists():
                logger.warning("Input feature file missing for '%s' at %s. Skipping distance calculation.", feature_name, feature_path)
                continue

            raw_feature_conf = feature_manager.all_feature_configs_map.get(feature_name)
            if not raw_feature_conf:
                logger.error("Raw feature config missing for %s. Skip dist.", feature_name)
                continue
            effective_config = feature_manager._get_effective_feature_config(raw_feature_conf, current_subset_key)

            allowed_dists = effective_config.get("compute_distances_for")
            if allowed_dists is not None and not isinstance(allowed_dists, list):
                logger.warning("Invalid 'compute_distances_for'. Using all.")
                allowed_dists = None

            distances_to_process = [dc for dc in self.distance_configs if dc.get("name") and (allowed_dists is None or dc.get("name") in allowed_dists)]
            if not distances_to_process:
                continue

            feature_metadata = read_hdf5_metadata(feature_path, HDF5_FEATURE_DATASET_NAME)
            if not feature_metadata:
                logger.error("Metadata missing for feature file '%s'. Skip dist.", feature_path.name)
                continue

            is_base_feature = not effective_config.get("base_extractor")
            item_count_attr = "num_items_processed" if is_base_feature else "num_items"
            n_samples_feat = feature_metadata.get(item_count_attr)

            if n_samples_feat is None:
                fallback_attr = "num_items" if is_base_feature else "num_items_processed"
                n_samples_feat = feature_metadata.get(fallback_attr)
                if n_samples_feat is not None:
                    logger.warning("Primary item count attr '%s' missing for '%s'. Using fallback '%s'.", item_count_attr, feature_path.name, fallback_attr)
                else:
                    logger.error("Item count missing (checked '%s', '%s') in metadata for '%s'. Skip dist.", item_count_attr, fallback_attr, feature_path.name)
                    continue

            n_samples_feat = int(n_samples_feat)
            if n_samples_feat <= 0:
                logger.warning("Feature '%s' has %d samples. Skipping distances.", feature_name, n_samples_feat)
                continue

            dist_iter = tqdm(distances_to_process, desc=f"Distances for {feature_name}", leave=False)
            for dist_conf in dist_iter:
                dist_name = dist_conf.get("name")
                if not dist_name:
                    continue
                dist_iter.set_postfix_str(f"{dist_name}")
                matrix_key = (feature_name, dist_name)

                combined_config_for_path = {"feature_config": effective_config, "distance_config": dist_conf, "name": f"{feature_name}_{dist_name}"}
                found_path = find_cache_path(subset_features_dir, f"distances_{dist_name}", dataset_cache_id, combined_config_for_path)

                cache_status = "MISSING"
                if found_path and found_path.is_file():
                    logger.debug("Found potential match for dist '%s/%s': %s", dist_name, feature_name, found_path.name)
                    try:
                        with h5py.File(found_path, "r") as f_dist_check:
                            if HDF5_DISTANCE_DATASET_NAME in f_dist_check and f_dist_check[HDF5_DISTANCE_DATASET_NAME].shape == (n_samples_feat, n_samples_feat):
                                cache_status = "VALID"
                            else:
                                cache_status = f"INVALID_SHAPE/DATASET ({found_path.name})"
                    except Exception as e:
                        cache_status = f"INVALID_READ ({found_path.name})"
                        logger.warning("Error reading cache %s: %s", found_path.name, e)

                if cache_status == "VALID" and found_path:
                    logger.info("Using valid cache for dist '%s/%s': %s", dist_name, feature_name, found_path.name)
                    distance_file_paths[matrix_key] = found_path
                elif compute_distances_step:
                    logger.info("Computing distance matrix for '%s' on '%s' (Cache Status: %s).", dist_name, feature_name, cache_status)

                    expected_save_path = get_cache_path(subset_features_dir, f"distances_{dist_name}", dataset_cache_id, combined_config_for_path)

                    try:
                        with h5py.File(feature_path, "r") as f_feat:
                            if HDF5_FEATURE_DATASET_NAME not in f_feat:
                                logger.error("Feature dataset missing in %s. Cannot compute distance.", feature_path.name)
                                continue
                            h5_features_dataset = f_feat[HDF5_FEATURE_DATASET_NAME]
                            if h5_features_dataset.shape[0] != n_samples_feat:
                                logger.error("Feature file shape mismatch during compute for %s. Skip.", feature_path.name)
                                continue

                            calculator = self._get_distance_calculator(dist_conf)
                            success = self._compute_distances_block_gpu_hdf5(
                                calculator=calculator,
                                h5_features=h5_features_dataset,
                                distance_h5_path=expected_save_path,
                                n_samples=n_samples_feat,
                                distance_dataset_name=HDF5_DISTANCE_DATASET_NAME,
                            )

                        if success:
                            distance_file_paths[matrix_key] = expected_save_path
                            try:
                                with h5py.File(expected_save_path, "a") as f_dist_meta:
                                    if HDF5_DISTANCE_DATASET_NAME in f_dist_meta:
                                        dset = f_dist_meta[HDF5_DISTANCE_DATASET_NAME]
                                        dist_meta = {"feature_name": feature_name, "distance_name": dist_name, "n_samples": n_samples_feat, "creation_time_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
                                        for k, v in dist_meta.items():
                                            dset.attrs[k] = str(v)
                            except Exception as meta_e:
                                logger.error("Failed write metadata to %s: %s", expected_save_path, meta_e)
                            logger.info("Distance matrix computed and saved for '%s/%s'.", dist_name, feature_name)
                        else:
                            logger.error("Distance calculation failed for %s.", matrix_key)

                    except Exception as compute_err:
                        logger.error("Error during distance computation for %s: %s", matrix_key, compute_err, exc_info=True)
                    finally:
                        gc.collect()
                        if self.device.type == "cuda":
                            torch.cuda.empty_cache()
                else:
                    logger.info("Cache MISS for dist '%s/%s' and 'distances' step skipped. Cannot compute.", dist_name, feature_name)

            gc.collect()
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

        self._clear_calculator_cache()
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
        logger.info("--- Distance Computation Completed for Subset: %s ---", current_subset_key)
        return distance_file_pathsimport gc
import importlib
import json
import logging
from itertools import chain, islice
from pathlib import Path
from typing import Any, Dict, Iterator, List, Optional, Set, Tuple, Type, Union, Callable

import h5py
import numpy as np
import torch
from sklearn.decomposition import PCA, IncrementalPCA
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import joblib
from datasets import Dataset, IterableDataset

from features.base import FeatureExtractor
from utils.feature_utils import apply_averaging
from utils.file_utils import (
    NpEncoder,
    get_cache_path,
    load_hdf5,
    load_pca_model,
    read_hdf5_metadata,
    save_pca_model,
)

logger = logging.getLogger(__name__)

HDF5_DATASET_NAME = "features"
HDF5_INDICES_NAME = "original_indices"


class FeatureManager:
    """
    Manages feature extraction, averaging/flattening, and PCA.
    """

    def __init__(
        self,
        config: Dict[str, Any],
        base_features_cache_dir: Path,
        device: torch.device,
        base_models_dir: Path,
    ):
        """
        Initializes the FeatureManager.

        Args:
            config (Dict[str, Any]): The configuration dictionary.
            base_features_cache_dir (Path): The base directory for caching features.
            device (torch.device): The device to use for computation.
            base_models_dir (Path): The base directory for caching models (like PCA).
        """
        self.cfg = config
        self.base_features_dir = base_features_cache_dir
        self.base_models_dir = base_models_dir
        self.device = device
        self.all_feature_configs = config.get("feature_extractors", [])
        self.all_feature_configs_map = {fc["name"]: fc for fc in self.all_feature_configs if "name" in fc}
        self.whisperseg_param_map = self._create_subset_to_params_map(config, "whisperseg_subset_params")
        self.pca_model_cache: Dict[str, Union[IncrementalPCA, PCA]] = {}
        self._extractor_instances_cache: Dict[str, FeatureExtractor] = {}
        self.processed_feature_metadata: Dict[str, Dict[str, Any]] = {}
        self.default_pca_load_chunks = config.get("pca_load_chunks", 0)
        logger.info(
            "FeatureManager initialized (HDF5, Single-Scan, Default PCA Load Chunks: %d).", self.default_pca_load_chunks
        )
        logger.info("Features Cache Dir: %s", self.base_features_dir)
        logger.info("Models (PCA) Dir: %s", self.base_models_dir)

    def _create_subset_to_params_map(self, config: Dict[str, Any], map_key: str) -> Dict[str, Dict[str, Any]]:
        """
        Creates a map from subset key to dynamic parameters for specific extractors.

        Args:
            config (Dict[str, Any]): The full configuration dictionary.
            map_key (str): The key in the config containing the subset parameter map.

        Returns:
            Dict[str, Dict[str, Any]]: A dictionary mapping subset names to parameter dictionaries.
        """
        subset_param_map = {}
        category_config = config.get(map_key, {})
        if not isinstance(category_config, dict):
            logger.warning("'%s' in config is not a dictionary.", map_key)
            return subset_param_map
        for subset_name, subset_data in category_config.items():
            if isinstance(subset_data, dict) and "params" in subset_data:
                if isinstance(subset_data["params"], dict):
                    subset_param_map[subset_name] = subset_data["params"].copy()
                else:
                    logger.warning("Params for '%s' in '%s' is not a dict.", subset_name, map_key)
            else:
                logger.debug("Invalid format or missing 'params' for '%s' in '%s'. Skipping.", subset_name, map_key)
        if subset_param_map:
            logger.info("Created dynamic param map for '%s': %s", map_key, list(subset_param_map.keys()))
        return subset_param_map

    def _get_class_from_module(self, module_name: str, class_name: str) -> Type:
        """
        Dynamically imports a class from a module path.

        Args:
            module_name (str): The dotted module path.
            class_name (str): The name of the class within the module.

        Returns:
            Type: The imported class object.

        Raises:
            ImportError: If the module or class cannot be found/imported.
        """
        try:
            module = importlib.import_module(module_name)
            return getattr(module, class_name)
        except ModuleNotFoundError:
            logger.error("Module not found: %s", module_name)
            raise
        except AttributeError:
            logger.error("Class '%s' not found in module '%s'", class_name, module_name)
            raise
        except Exception as e:
            logger.error("Failed load class '%s' from '%s': %s", class_name, module_name, e, exc_info=True)
            raise ImportError(f"Failed import {class_name} from {module_name}.") from e

    def _get_feature_extractor(self, config: Dict[str, Any]) -> Optional[FeatureExtractor]:
        """
        Gets or instantiates a feature extractor.

        Args:
            config (Dict[str, Any]): The configuration dictionary for the feature extractor.

        Returns:
            Optional[FeatureExtractor]: An instance of the FeatureExtractor class, or None if instantiation fails.
        """
        extractor_name = config.get("name")
        if not extractor_name:
            logger.error("Feature config needs 'name'.")
            return None

        if extractor_name in self._extractor_instances_cache:
            return self._extractor_instances_cache[extractor_name]

        class_name = config.get("class", extractor_name)
        module_path = config.get("module")
        if not module_path:
            module_name_guess = class_name.replace("Extractor", "").lower()
            if "whisperseg" in module_name_guess:
                module_path = "features.whisperseg.extractor"
            elif "paperautoencoder" in module_name_guess:
                module_path = "reproducibility.features.autoencoder"
            elif "papervae" in module_name_guess:
                module_path = "reproducibility.features.vae"
            else:
                module_path = f"features.{module_name_guess}"
            logger.debug("Inferred module path for %s: %s", class_name, module_path)

        try:
            extractor_class = self._get_class_from_module(module_path, class_name)
            params = config.get("params", {}).copy()
            params["device"] = self.device
            if "Paper" in class_name:
                params["base_models_dir"] = self.base_models_dir

            instance = extractor_class(**params)
            self._extractor_instances_cache[extractor_name] = instance
            logger.info("Instantiated extractor '%s' (Config: %s).", class_name, extractor_name)
            return instance
        except Exception as e:
            logger.error("Failed instantiate extractor '%s' (Config: %s): %s", class_name, extractor_name, e, exc_info=True)
            return None

    def _clear_extractor_cache(self) -> None:
        """Clears the cache of instantiated feature extractors."""
        logger.debug("Clearing feature extractor instance cache...")
        for instance in list(self._extractor_instances_cache.values()):
            del instance
        self._extractor_instances_cache.clear()
        gc.collect()
        if self.device.type == "cuda":
            try:
                torch.cuda.empty_cache()
                logger.debug("Cleared CUDA cache.")
            except Exception as cuda_err:
                logger.warning("Could not clear CUDA cache: %s", cuda_err)

    def _get_effective_feature_config(self, feature_config: Dict[str, Any], subset_key: Optional[str]) -> Dict[str, Any]:
        """
        Applies dynamic subset parameters to a feature configuration if configured.

        Args:
            feature_config (Dict[str, Any]): The base feature configuration.
            subset_key (Optional[str]): The key of the current dataset subset.

        Returns:
            Dict[str, Any]: The feature configuration with dynamic parameters applied.
        """
        effective_config = feature_config.copy()
        is_whisperseg = "whisperseg" in effective_config.get("module", "").lower() or "WhisperSeg" in effective_config.get("name", "")

        if is_whisperseg and subset_key:
            subset_params = self.whisperseg_param_map.get(subset_key)
            if subset_params:
                if "params" not in effective_config:
                    effective_config["params"] = {}
                effective_config["params"] = {**effective_config.get("params", {}), **subset_params}
                logger.debug("Applied dynamic params for '%s' on subset '%s': %s", effective_config.get("name"), subset_key, subset_params)
            elif self.whisperseg_param_map:
                logger.debug("No dynamic params found for subset '%s'. Using defaults for '%s'.", subset_key, effective_config.get("name"))
        return effective_config

    def get_feature_file_path(self, feature_name: str, dataset_cache_id: str, subset_features_dir: Optional[Path] = None) -> Optional[Path]:
        """
        Determines the expected path for a cached feature file (base, intermediate, or final PCA).

        Args:
            feature_name (str): The name of the feature configuration.
            dataset_cache_id (str): Unique identifier for the dataset subset/split.
            subset_features_dir (Optional[Path]): The directory specific to the current subset's features.

        Returns:
            Optional[Path]: The expected Path object, or None if the feature configuration is not found.
        """
        raw_config = self.all_feature_configs_map.get(feature_name)
        if not raw_config:
            logger.error("Configuration for feature '%s' not found.", feature_name)
            return None

        is_base = not raw_config.get("base_extractor")
        needs_intermediate = raw_config.get("averaging") is not None
        needs_pca = raw_config.get("pca") is not None

        final_config = raw_config

        final_prefix = "features"
        if needs_intermediate and not needs_pca:
            final_prefix = "intermediate"

        cache_dir_to_use = subset_features_dir if subset_features_dir else self.base_features_dir

        return get_cache_path(cache_dir=cache_dir_to_use, prefix=final_prefix, dataset_cache_id=dataset_cache_id, config_dict=final_config, extra_suffix=None)

    def _extract_and_save_features_hdf5(
        self,
        extractor: FeatureExtractor,
        dataset: Union[Dataset, IterableDataset],
        cache_path: Path,
        feature_config: Dict[str, Any],
        item_id_map: Optional[Dict] = None,
        batch_size: int = 32,
    ) -> Optional[Dict[str, Any]]:
        """
        Extracts features using a two-pass scan-then-write process and saves to HDF5.

        Args:
            extractor (FeatureExtractor): The feature extractor instance.
            dataset (Union[Dataset, IterableDataset]): The dataset to extract from.
            cache_path (Path): Path to save the new feature HDF5 file.
            feature_config (Dict[str, Any]): The configuration dictionary for the feature extractor.
            item_id_map (Optional[Dict]): Map from item IDs to metadata, used to get original indices.
            batch_size (int): Batch size for processing during extraction and writing.

        Returns:
            Optional[Dict[str, Any]]: Metadata dictionary of the saved file, or None on failure.
        """
        feature_name = feature_config.get("name", "unknown_feature")
        extractor_name = extractor.__class__.__name__
        logger.info("Starting HDF5 extraction for '%s' using '%s' to %s.", feature_name, extractor_name, cache_path.name)
        params = feature_config.get("params", {})

        # --- Pass 1: Scan for valid items and determine maximum feature shape ---
        logger.info("Step 1: Scanning dataset for valid items and max feature shape...")
        max_dims: List[int] = []
        final_ndim: Optional[int] = None
        final_dtype: Optional[np.dtype] = None
        valid_item_original_indices: List[int] = []
        valid_item_internal_indices: List[int] = []
        processed_indices_set: Set[int] = set()
        skipped_scan_count = 0
        is_iterable = not hasattr(dataset, "__len__")
        total_items_scan = None if is_iterable else len(dataset)

        loader_scan = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=lambda x: x)
        pbar_scan = tqdm(loader_scan, desc=f"Scan [{feature_name}]", total=total_items_scan, leave=False)
        global_idx_scan = 0

        for batch in pbar_scan:
            for item in batch:
                current_internal_idx = global_idx_scan
                audio_info = item.get("audio")
                is_valid_item = isinstance(audio_info, dict) and audio_info.get("array") is not None and "sampling_rate" in audio_info

                if is_valid_item:
                    try:
                        features = extractor.extract(audio_data=audio_info["array"], sample_rate=audio_info["sampling_rate"], **params)
                        features_np = features.cpu().numpy() if isinstance(features, torch.Tensor) else np.array(features)

                        if features_np.size == 0 or np.isnan(features_np).any() or np.isinf(features_np).any():
                            is_valid_item = False
                        else:
                            if final_ndim is None:
                                final_ndim = features_np.ndim
                                final_dtype = np.float32
                                max_dims = list(features_np.shape)
                            elif features_np.ndim != final_ndim:
                                logger.warning("Inconsistent feature dimensionality at item %d. Expected %dD, got %dD. Skipping.", current_internal_idx, final_ndim, features_np.ndim)
                                is_valid_item = False
                            else:
                                for i in range(final_ndim):
                                    max_dims[i] = max(max_dims[i], features_np.shape[i])
                    except Exception as e:
                        logger.warning("Error extracting feature for item %d during scan: %s. Skipping.", current_internal_idx, e)
                        is_valid_item = False

                if is_valid_item:
                    valid_item_original_indices.append(current_internal_idx)
                    valid_item_internal_indices.append(current_internal_idx)
                    processed_indices_set.add(current_internal_idx)
                else:
                    skipped_scan_count += 1
                global_idx_scan += 1
            pbar_scan.set_postfix({"Valid": len(valid_item_internal_indices), "Skipped": skipped_scan_count, "MaxShape": str(tuple(max_dims))})
        pbar_scan.close()

        processed_count = len(valid_item_internal_indices)
        if processed_count == 0:
            logger.error("No valid audio items found for '%s'. Aborting.", feature_name)
            return None
        max_shape = tuple(max_dims)
        logger.info("Scan complete. Found %d valid items. Max feature shape: %s", processed_count, max_shape)

        # --- Pass 2: Create HDF5 file and write features ---
        metadata_for_attrs = {
            "feature_config_name": feature_name,
            "num_items_processed": processed_count,
            "num_items_skipped": skipped_scan_count,
            "target_padded_shape": list(max_shape),
            "feature_ndim": final_ndim,
            "config": json.loads(json.dumps(feature_config, cls=NpEncoder)),
        }
        final_returned_metadata = metadata_for_attrs.copy()
        final_returned_metadata[HDF5_INDICES_NAME] = valid_item_original_indices

        try:
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            with h5py.File(cache_path, "w") as h5_file:
                h5_dataset = h5_file.create_dataset(
                    HDF5_DATASET_NAME, shape=(processed_count, *max_shape), dtype=final_dtype,
                    compression="gzip", compression_opts=4, shuffle=True, fletcher32=True)
                for k, v in metadata_for_attrs.items():
                    h5_dataset.attrs[k] = json.dumps(v, cls=NpEncoder)
                h5_file.create_dataset(HDF5_INDICES_NAME, data=np.array(valid_item_original_indices, dtype=np.int64), compression="gzip", compression_opts=4)

                logger.info("Step 2: Writing %d features to HDF5 file '%s'...", processed_count, cache_path.name)
                write_idx = 0
                error_occurred_write = False
                loader_write = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=lambda x: x)
                pbar_write = tqdm(loader_write, desc=f"Write [{feature_name}]", total=int(total_items_scan / batch_size), leave=False)
                global_idx_write = 0
                for batch in pbar_write:
                    features_to_write_list = []
                    for item in batch:
                        current_internal_idx = global_idx_write
                        global_idx_write += 1
                        if current_internal_idx not in processed_indices_set:
                            continue
                        try:
                            features = extractor.extract(audio_data=item["audio"]["array"], sample_rate=item["audio"]["sampling_rate"], **params)
                            features_np = features.cpu().numpy() if isinstance(features, torch.Tensor) else np.array(features)
                            pad_width = [(0, max_shape[i] - features_np.shape[i]) for i in range(final_ndim)]
                            padded_feature = np.pad(features_np, pad_width, mode="constant", constant_values=0.0) if any(p[1] > 0 for p in pad_width) else features_np
                            features_to_write_list.append(padded_feature)
                        except Exception as e:
                            logger.error("Processing item internal_idx %d failed during write pass: %s", current_internal_idx, e)
                            error_occurred_write = True
                    if features_to_write_list:
                        batch_array = np.stack(features_to_write_list).astype(final_dtype)
                        h5_dataset[write_idx : write_idx + len(features_to_write_list)] = batch_array
                        write_idx += len(features_to_write_list)
                    pbar_write.set_postfix({"Written": write_idx, "TotalValid": processed_count})
                pbar_write.close()

                if write_idx != processed_count or error_occurred_write:
                    raise RuntimeError(f"Write failed. Written: {write_idx}/{processed_count}. Errors: {error_occurred_write}")

        except Exception as e:
            logger.error("HDF5 extraction/write failed for '%s': %s", feature_name, e, exc_info=True)
            if cache_path.exists(): cache_path.unlink(missing_ok=True)
            return None

        logger.info("Successfully wrote %d features to HDF5: %s", write_idx, cache_path)
        return final_returned_metadata

    def _process_intermediate_features(
        self,
        base_feature_path: Path,
        intermediate_feature_path: Path,
        feature_config: Dict[str, Any],
        chunk_size: int = 1024,
    ) -> Optional[Dict[str, Any]]:
        """
        Processes base features (averaging/flattening) -> intermediate HDF5.

        Args:
            base_feature_path (Path): Path to READ base feature HDF5 file.
            intermediate_feature_path (Path): Path to SAVE intermediate HDF5 file.
            feature_config (Dict[str, Any]): Effective configuration for the intermediate step.
            chunk_size (int): Size of chunks to read from the base HDF5 file.

        Returns:
            Optional[Dict[str, Any]]: Metadata of the intermediate file, or None on failure.
        """
        feature_name = feature_config.get("name", "unknown_intermediate")
        averaging_method = feature_config.get("averaging")
        logger.info("Processing intermediate features for '%s' (Method: %s) -> %s", feature_name, averaging_method, intermediate_feature_path.name)

        if not base_feature_path.exists():
            logger.error("Base feature file not found: %s. Cannot create intermediate.", base_feature_path)
            return None

        base_metadata = read_hdf5_metadata(base_feature_path, HDF5_DATASET_NAME)
        if base_metadata is None:
            logger.error("Could not read metadata from base HDF5: %s. Cannot create intermediate.", base_feature_path)
            return None
        base_original_indices = load_hdf5(base_feature_path, HDF5_INDICES_NAME, "Base Indices")

        item_count_attr = "num_items_processed"
        n_items = base_metadata.get(item_count_attr)
        if n_items is None:
            try:
                with h5py.File(base_feature_path, "r") as f_check:
                    n_items = f_check[HDF5_DATASET_NAME].shape[0]
                logger.warning("Base metadata missing 'num_items_processed'. Used shape[0]: %d", n_items)
            except Exception:
                logger.error("Could not determine item count from base file: %s.", base_feature_path)
                return None
        n_items = int(n_items)
        if n_items == 0:
            logger.warning("Base file has 0 items: %s. Skipping intermediate creation.", base_feature_path)
            return None

        intermediate_shape_per_item: Optional[Tuple[int, ...]] = None
        final_dtype = np.float32
        write_errors = 0
        successful_indices: List[int] = []
        h5_intermediate_file = None
        h5_base_file = None

        try:
            h5_base_file = h5py.File(base_feature_path, "r")
            if HDF5_DATASET_NAME not in h5_base_file:
                raise ValueError(f"Dataset '{HDF5_DATASET_NAME}' missing in {base_feature_path}")
            base_dset = h5_base_file[HDF5_DATASET_NAME]
            if base_dset.shape[0] != n_items:
                logger.warning("Base shape %d != meta count %d. Using shape[0].", base_dset.shape[0], n_items)
                n_items = base_dset.shape[0]
            if n_items == 0:
                logger.warning("Base file confirmed 0 items. Skipping intermediate creation.")
                return None

            first_item = base_dset[0]
            processed_first_item = apply_averaging(first_item, averaging_method)
            if processed_first_item is None:
                raise ValueError("Averaging returned None for first item.")
            if processed_first_item.ndim != 1:
                raise ValueError(f"Intermediate shape must be 1D after processing, got {processed_first_item.ndim}D.")
            intermediate_shape_per_item = processed_first_item.shape
            logger.info("Intermediate feature dimension for '%s': %d", feature_name, intermediate_shape_per_item[0])

            intermediate_feature_path.parent.mkdir(parents=True, exist_ok=True)
            h5_intermediate_file = h5py.File(intermediate_feature_path, "w")
            intermediate_dset = None

            pbar = tqdm(range(0, n_items, chunk_size), desc=f"Intermediate [{feature_name}]", leave=False)
            all_processed_batches = []
            items_processed_count = 0

            for i in pbar:
                start_idx = i
                end_idx = min(i + chunk_size, n_items)
                if start_idx >= end_idx:
                    continue
                try:
                    chunk_data = base_dset[start_idx:end_idx]
                    current_chunk_processed_items = []
                    current_chunk_success_indices = []

                    for j in range(chunk_data.shape[0]):
                        item_idx_in_base = start_idx + j
                        processed_item = apply_averaging(chunk_data[j], averaging_method)
                        if processed_item is not None and processed_item.size > 0 and not np.isnan(processed_item).any() and not np.isinf(processed_item).any():
                            if processed_item.shape != intermediate_shape_per_item:
                                logger.warning("Item %d processed shape %s != expected %s. Skipping.", item_idx_in_base, processed_item.shape, intermediate_shape_per_item)
                                write_errors += 1
                            else:
                                current_chunk_processed_items.append(processed_item.astype(final_dtype))
                                if base_original_indices is not None:
                                    current_chunk_success_indices.append(base_original_indices[item_idx_in_base])
                                else:
                                    current_chunk_success_indices.append(item_idx_in_base)

                        else:
                            logger.warning("Skipping item %d due to processing error/invalid output (avg='%s').", item_idx_in_base, averaging_method)
                            write_errors += 1

                    if current_chunk_processed_items:
                        batch_array = np.stack(current_chunk_processed_items)
                        all_processed_batches.append(batch_array)
                        successful_indices.extend(current_chunk_success_indices)
                        items_processed_count += batch_array.shape[0]

                    del chunk_data, current_chunk_processed_items, current_chunk_success_indices
                    gc.collect()

                except Exception as chunk_err:
                    logger.error("Error processing intermediate chunk %d-%d: %s", start_idx, end_idx, chunk_err, exc_info=True)
                    write_errors += end_idx - start_idx
            pbar.close()

            if not all_processed_batches:
                logger.error("No items processed successfully for intermediate feature '%s'.", feature_name)
                raise RuntimeError("Intermediate processing failed for all items.")

            final_intermediate_data = np.concatenate(all_processed_batches, axis=0)
            if final_intermediate_data.shape[0] != items_processed_count:
                raise RuntimeError("Mismatch between processed count and concatenated array size.")

            intermediate_dset = h5_intermediate_file.create_dataset(
                HDF5_DATASET_NAME,
                data=final_intermediate_data,
                dtype=final_dtype,
                compression="gzip",
                compression_opts=4,
                shuffle=True,
                fletcher32=True,
            )
            logger.info("Written %d items to intermediate dataset.", final_intermediate_data.shape[0])

            intermediate_metadata_attrs = {
                "feature_config_name": feature_name,
                "base_feature_path": str(base_feature_path),
                "averaging_method": averaging_method,
                "num_items": items_processed_count,
                "feature_dim": list(intermediate_shape_per_item),
                "feature_ndim": 1,
                "num_processing_errors": write_errors,
            }
            serializable_attrs = json.loads(json.dumps(intermediate_metadata_attrs, cls=NpEncoder))
            for k, v in serializable_attrs.items():
                try:
                    intermediate_dset.attrs[k] = v
                except TypeError:
                    intermediate_dset.attrs[k] = str(v)

            if successful_indices:
                indices_array = np.array(successful_indices, dtype=np.int64)
                if indices_array.shape[0] == items_processed_count:
                    h5_intermediate_file.create_dataset(HDF5_INDICES_NAME, data=indices_array, compression="gzip", compression_opts=4)
                    logger.info("Saved corresponding original indices (%d items).", len(successful_indices))
                else:
                    logger.error("Mismatch between successful indices count and items written. Indices not saved.")
                    intermediate_metadata_attrs["indices_save_error"] = "Count mismatch"

            final_intermediate_metadata = intermediate_metadata_attrs.copy()
            if successful_indices and "indices_save_error" not in final_intermediate_metadata:
                final_intermediate_metadata[HDF5_INDICES_NAME] = successful_indices
            else:
                final_intermediate_metadata[HDF5_INDICES_NAME] = None
            return final_intermediate_metadata

        except Exception as e:
            logger.error("Failed processing intermediate features for '%s': %s", feature_name, e, exc_info=True)
            try:
                intermediate_feature_path.unlink(missing_ok=True)
            except OSError:
                pass
            return None
        finally:
            if h5_base_file:
                h5_base_file.close()
            if h5_intermediate_file:
                h5_intermediate_file.close()
            gc.collect()

    def _process_pca_features(
        self,
        intermediate_feature_path: Path,
        pca_feature_path: Path,
        feature_config: Dict[str, Any],
        dataset_cache_id: str,
        subset_features_dir: Path,
        chunk_size: int = 1024,
    ) -> Optional[Dict[str, Any]]:
        """
        Applies PCA (standard or incremental) based on config.

        Args:
            intermediate_feature_path (Path): Path to READ intermediate feature HDF5 file.
            pca_feature_path (Path): Path to SAVE final PCA feature HDF5 file.
            feature_config (Dict[str, Any]): Effective configuration for the PCA step.
            dataset_cache_id (str): Unique identifier for the dataset subset/split.
            subset_features_dir (Path): Directory for saving PCA model cache files.
            chunk_size (int): Size of chunks to read from the intermediate HDF5 file.

        Returns:
            Optional[Dict[str, Any]]: Metadata of the final PCA feature file, or None on failure.
        """
        feature_name = feature_config.get("name", "unknown_pca")
        pca_components = feature_config.get("pca")
        if not pca_components:
            logger.error("PCA components not specified for '%s'.", feature_name)
            return None
        pca_components = int(pca_components)
        pca_load_chunks = feature_config.get("pca_load_chunks", self.default_pca_load_chunks)
        pca_model_base_dir = self.base_models_dir / "pca_models"
        pca_model_base_dir.mkdir(parents=True, exist_ok=True)

        logger.info("Processing PCA for '%s' (Load Chunks Config: %s) -> %s", feature_name, pca_load_chunks, pca_feature_path.name)

        if not intermediate_feature_path.exists():
            logger.error("Intermediate file not found: %s. Cannot apply PCA.", intermediate_feature_path)
            return None

        intermediate_metadata = read_hdf5_metadata(intermediate_feature_path, HDF5_DATASET_NAME)
        if not intermediate_metadata:
            logger.error("Could not read metadata from intermediate file: %s", intermediate_feature_path)
            return None

        intermediate_original_indices = load_hdf5(intermediate_feature_path, HDF5_INDICES_NAME, "Intermediate Indices")

        n_items = intermediate_metadata.get("num_items")
        if n_items is None:
            logger.error("Item count missing from intermediate metadata: %s", intermediate_feature_path)
            return None
        n_items = int(n_items)
        if n_items == 0:
            logger.warning("Intermediate file has 0 items. Skipping PCA processing.")
            return None

        pca_model: Optional[Union[PCA, IncrementalPCA]] = None
        h5_pca_file = None
        h5_intermediate_file_fit = None
        h5_intermediate_file_transform = None
        final_shape_per_item: Tuple[int, ...] = (pca_components,)
        final_dtype = np.float32
        write_errors = 0
        intermediate_dim = intermediate_metadata.get("feature_dim", [0])[0]

        try:
            pca_model_path = get_cache_path(pca_model_base_dir, "pca_model", dataset_cache_id, feature_config)

            if str(pca_model_path) in self.pca_model_cache:
                pca_model = self.pca_model_cache[str(pca_model_path)]
                logger.info("Using cached PCA model instance for '%s'.", feature_name)
            elif pca_model_path.exists():
                pca_model = load_pca_model(pca_model_path)
                if pca_model:
                    logger.info("Loaded existing PCA model file for '%s' from %s", feature_name, pca_model_path.name)
                    self.pca_model_cache[str(pca_model_path)] = pca_model
                else:
                    logger.warning("Failed to load PCA model from %s. Will refit.", pca_model_path.name)
                    try:
                        pca_model_path.unlink(missing_ok=True)
                    except OSError:
                        pass

            if pca_model is None:
                logger.info("Fitting PCA model for '%s'...", feature_name)
                h5_intermediate_file_fit = h5py.File(intermediate_feature_path, "r")
                intermediate_dset_fit = h5_intermediate_file_fit[HDF5_DATASET_NAME]

                if pca_components > intermediate_dim:
                    logger.warning("Requested PCA components (%d) > intermediate dim (%d). Using %d.", pca_components, intermediate_dim, intermediate_dim)
                    pca_components = intermediate_dim
                    final_shape_per_item = (pca_components,)

                if pca_load_chunks == -1:
                    logger.info("Using standard PCA (loading all %d intermediate items)...", n_items)
                    all_data = None
                    try:
                        all_data = intermediate_dset_fit[:].astype(np.float32)
                        logger.info("Fitting standard PCA (n=%d) on data shape %s...", pca_components, all_data.shape)
                        pca_model = PCA(n_components=pca_components)
                        pca_model.fit(all_data)
                        explained_var = np.sum(pca_model.explained_variance_ratio_) if hasattr(pca_model, "explained_variance_ratio_") else np.nan
                        logger.info("Standard PCA fit complete. Explained variance: %.4f", explained_var)
                    except MemoryError:
                        logger.error(
                            "MemoryError loading all %d items (%d dims) for standard PCA. Try setting 'pca_load_chunks: 0' for '%s'.",
                            n_items,
                            intermediate_dim,
                            feature_name,
                        )
                        raise
                    except Exception as std_pca_err:
                        logger.error("Standard PCA fitting failed: %s", std_pca_err, exc_info=True)
                        raise
                    finally:
                        if all_data is not None:
                            del all_data
                        gc.collect()
                else:
                    pca_model = IncrementalPCA(n_components=pca_components, batch_size=chunk_size)
                    num_chunks_to_fit = float("inf") if pca_load_chunks == 0 else pca_load_chunks
                    fit_desc = "all chunks" if num_chunks_to_fit == float("inf") else f"first {int(num_chunks_to_fit)} chunk(s)"
                    logger.info("Using IncrementalPCA (n=%d, fitting on %s)...", pca_components, fit_desc)
                    pbar_fit = tqdm(range(0, n_items, chunk_size), desc=f"Fit IncPCA [{feature_name}]", leave=False)
                    chunks_fitted = 0
                    total_samples_fitted = 0
                    for i in pbar_fit:
                        if chunks_fitted >= num_chunks_to_fit:
                            break
                        start_idx, end_idx = i, min(i + chunk_size, n_items)
                        if start_idx >= end_idx:
                            continue
                        try:
                            chunk_data = intermediate_dset_fit[start_idx:end_idx].astype(np.float32)
                            if chunk_data.shape[0] > 0:
                                pca_model.partial_fit(chunk_data)
                                chunks_fitted += 1
                                total_samples_fitted += chunk_data.shape[0]
                                pbar_fit.set_postfix({"Fitted Chunks": chunks_fitted, "Samples": total_samples_fitted})
                            del chunk_data
                            gc.collect()
                        except Exception as inc_fit_err:
                            logger.error("IncrementalPCA fit failed chunk %d-%d: %s", start_idx, end_idx, inc_fit_err, exc_info=True)
                            raise
                    pbar_fit.close()
                    if chunks_fitted == 0:
                        raise RuntimeError("IncrementalPCA fitted 0 chunks.")
                    fitted_components_inc = getattr(pca_model, "n_components_", 0)
                    explained_var_inc = np.sum(pca_model.explained_variance_ratio_) if hasattr(pca_model, "explained_variance_ratio_") and pca_model.explained_variance_ratio_ is not None else np.nan
                    logger.info("IncrementalPCA fit complete. Fitted %d components on %d samples. Explained var: %.4f", fitted_components_inc, total_samples_fitted, explained_var_inc)

                h5_intermediate_file_fit.close()
                h5_intermediate_file_fit = None
                if pca_model:
                    save_pca_model(pca_model, pca_model_path)
                    logger.info("PCA model fitted and saved to %s", pca_model_path.name)
                    self.pca_model_cache[str(pca_model_path)] = pca_model
                else:
                    raise RuntimeError("PCA fitting finished without a model.")
            else:
                loaded_components = getattr(pca_model, "n_components_", 0)
                if loaded_components > intermediate_dim:
                    logger.error("Loaded PCA model has %d components, but intermediate data only has %d dimensions. Cannot use this model.", loaded_components, intermediate_dim)
                    return None
                if loaded_components < pca_components:
                    logger.warning("Loaded PCA model has %d components, less than requested %d. Final features will have %d dimensions.", loaded_components, pca_components, loaded_components)
                    final_shape_per_item = (loaded_components,)
                elif loaded_components > pca_components:
                    logger.info("Loaded PCA model has %d components. Will use only the first %d as requested.", loaded_components, pca_components)

            logger.info("Applying PCA transform (Output Dim: %d) and writing to: %s", final_shape_per_item[0], pca_feature_path.name)
            pca_feature_path.parent.mkdir(parents=True, exist_ok=True)
            h5_intermediate_file_transform = h5py.File(intermediate_feature_path, "r")
            intermediate_dset_transform = h5_intermediate_file_transform[HDF5_DATASET_NAME]
            h5_pca_file = h5py.File(pca_feature_path, "w")
            pca_dset = h5_pca_file.create_dataset(
                HDF5_DATASET_NAME,
                shape=(n_items, *final_shape_per_item),
                dtype=final_dtype,
                compression="gzip",
                compression_opts=4,
                shuffle=True,
                fletcher32=True,
            )

            explained_var_ratio = pca_model.explained_variance_ratio_[:final_shape_per_item[0]] if hasattr(pca_model, "explained_variance_ratio_") else None
            pca_metadata_attrs = {
                "feature_config_name": feature_name,
                "source_feature_path": str(intermediate_feature_path),
                "pca_model_path": str(pca_model_path),
                "pca_components_requested": int(pca_components),
                "pca_components_actual": final_shape_per_item[0],
                "pca_load_chunks_config": pca_load_chunks,
                "num_items": n_items,
                "feature_dim": list(final_shape_per_item),
                "feature_ndim": 1,
                "explained_variance_ratio_sum": float(np.sum(explained_var_ratio)) if explained_var_ratio is not None else None,
            }
            serializable_attrs = json.loads(json.dumps(pca_metadata_attrs, cls=NpEncoder))
            for k, v in serializable_attrs.items():
                try:
                    pca_dset.attrs[k] = v
                except TypeError:
                    pca_dset.attrs[k] = str(v)

            if intermediate_original_indices is not None:
                indices_array = np.array(intermediate_original_indices, dtype=np.int64)
                if indices_array.shape[0] == n_items:
                    h5_pca_file.create_dataset(HDF5_INDICES_NAME, data=indices_array, compression="gzip", compression_opts=4)
                else:
                    logger.warning("Indices length mismatch (%d) vs item count (%d). Indices not saved.", indices_array.shape[0], n_items)
                    pca_metadata_attrs["indices_save_error"] = "Count mismatch"

            pbar_transform = tqdm(range(0, n_items, chunk_size), desc=f"Transform PCA [{feature_name}]", leave=False)
            write_idx = 0
            for i in pbar_transform:
                start_idx, end_idx = i, min(i + chunk_size, n_items)
                if start_idx >= end_idx:
                    continue
                try:
                    chunk_data = intermediate_dset_transform[start_idx:end_idx].astype(np.float32)
                    transformed_chunk = pca_model.transform(chunk_data)
                    if transformed_chunk.shape[1] > final_shape_per_item[0]:
                        transformed_chunk = transformed_chunk[:, :final_shape_per_item[0]]

                    transformed_chunk = transformed_chunk.astype(final_dtype)

                    current_chunk_size = transformed_chunk.shape[0]
                    actual_end_write_idx = write_idx + current_chunk_size
                    expected_slice_shape = (current_chunk_size, *final_shape_per_item)

                    if transformed_chunk.shape != expected_slice_shape:
                        raise ValueError(f"Transformed chunk shape {transformed_chunk.shape} mismatch expected {expected_slice_shape}")
                    if actual_end_write_idx > n_items:
                        raise IndexError(f"PCA write index {actual_end_write_idx} exceeds total {n_items}")

                    pca_dset[write_idx:actual_end_write_idx] = transformed_chunk
                    write_idx = actual_end_write_idx

                except Exception as chunk_pca_err:
                    logger.error("Error transforming/writing PCA chunk %d-%d: %s", start_idx, end_idx, chunk_pca_err, exc_info=True)
                    write_errors += end_idx - start_idx
            pbar_transform.close()

            if write_idx != n_items or write_errors > 0:
                raise RuntimeError(f"PCA feature write failed. Written: {write_idx}/{n_items}. Errors: {write_errors}.")

            final_pca_metadata = pca_metadata_attrs.copy()
            if intermediate_original_indices is not None and "indices_save_error" not in final_pca_metadata:
                final_pca_metadata[HDF5_INDICES_NAME] = intermediate_original_indices
            else:
                final_pca_metadata[HDF5_INDICES_NAME] = None
            return final_pca_metadata

        except Exception as e:
            logger.error("Failed PCA processing for '%s': %s", feature_name, e, exc_info=True)
            try:
                pca_feature_path.unlink(missing_ok=True)
            except OSError:
                pass
            return None
        finally:
            if h5_intermediate_file_fit:
                h5_intermediate_file_fit.close()
            if h5_intermediate_file_transform:
                h5_intermediate_file_transform.close()
            if h5_pca_file:
                h5_pca_file.close()
            if "pca_model" in locals() and pca_model is not None:
                del pca_model
            gc.collect()

    def _check_cache(
        self,
        prefix: str,
        cache_dir: Path,
        dataset_cache_id: str,
        effective_config: Dict[str, Any],
        expected_item_count: Optional[int] = None,
        check_attr: str = "num_items",
    ) -> Tuple[Optional[Path], Optional[Dict[str, Any]]]:
        """
        Checks cache using the exact expected path, validates metadata.

        Args:
            prefix (str): Prefix for the cache file (e.g., 'features', 'intermediate').
            cache_dir (Path): The directory where the cache file is expected.
            dataset_cache_id (str): Unique identifier for the dataset.
            effective_config (Dict[str, Any]): The effective feature configuration used for path generation.
            expected_item_count (Optional[int]): The expected number of items in the HDF5 file.
            check_attr (str): The name of the attribute in the HDF5 file to check the item count against.

        Returns:
            Tuple[Optional[Path], Optional[Dict[str, Any]]]: A tuple containing the path to the valid cache file
                                                            and its metadata, or (None, None) if cache is missing,
                                                            invalid, or does not match expected count.
        """
        feature_name = effective_config.get("name", "unknown")
        expected_path = self.get_feature_file_path(feature_name, dataset_cache_id, cache_dir)
        if expected_path is None:
            logger.warning("Could not determine expected cache path for '%s'.", feature_name)
            return None, None

        file_type = prefix.split("_")[0].upper()

        if not expected_path.is_file():
            logger.debug("%s cache MISS for '%s': %s", file_type, feature_name, expected_path.name)
            return None, None

        logger.debug("Found potential %s match for '%s': %s", file_type, feature_name, expected_path.name)
        metadata = read_hdf5_metadata(expected_path, HDF5_DATASET_NAME)

        if not metadata:
            logger.warning("Could not read metadata from %s cache %s. Discarding.", file_type, expected_path.name)
            return None, None

        if expected_item_count is not None and expected_item_count >= 0:
            item_count_from_meta = metadata.get(check_attr)
            if item_count_from_meta is None:
                logger.warning("%s cache %s for '%s' missing item count attr '%s'. Discarding.", file_type, expected_path.name, feature_name, check_attr)
                return None, None
            try:
                item_count = int(item_count_from_meta)
                if item_count != expected_item_count:
                    logger.warning(
                        "%s cache %s for '%s' has mismatched count (%d vs %d). Discarding.",
                        file_type,
                        expected_path.name,
                        feature_name,
                        item_count,
                        expected_item_count,
                    )
                    return None, None
            except (ValueError, TypeError):
                logger.warning("Invalid item count attribute '%s' value '%s' in %s. Discarding.", check_attr, item_count_from_meta, expected_path.name)
                return None, None

        indices = load_hdf5(expected_path, HDF5_INDICES_NAME, f"{feature_name} indices")
        if indices is not None:
            metadata[HDF5_INDICES_NAME] = indices

        logger.info("Using valid %s cache for '%s': %s", file_type, feature_name, expected_path.name)
        return expected_path, metadata

    def process_subset_features(
        self,
        subset_dataset_obj: Any,
        dataset_cache_id: str,
        current_subset_key: str,
        subset_features_dir: Path,
        item_id_map: Optional[Dict],
        run_steps: List[str],
    ) -> Dict[str, Path]:
        """
        Processes all configured features for a given dataset subset.

        Args:
            subset_dataset_obj (Any): The actual dataset subset object.
            dataset_cache_id (str): Unique identifier for the dataset subset/split.
            current_subset_key (str): Name of the current subset.
            subset_features_dir (Path): Directory for storing feature cache files for this subset.
            item_id_map (Optional[Dict]): Map from original item IDs to metadata.
            run_steps (List[str]): List of steps being run in the pipeline.

        Returns:
            Dict[str, Path]: A dictionary mapping feature names to paths of processed feature files.
                             Only includes features successfully processed or found in cache.
        """
        logger.info("--- Feature Processing (Steps: %s) for Subset: %s ---", run_steps, current_subset_key)
        final_feature_paths: Dict[str, Path] = {}
        available_base_paths: Dict[str, Path] = {}
        available_intermediate_paths: Dict[str, Path] = {}

        self.pca_model_cache.clear()
        self._clear_extractor_cache()
        self.processed_feature_metadata.clear()
        total_items_in_subset: Optional[int] = None
        if hasattr(subset_dataset_obj, "__len__"):
            try:
                total_items_in_subset = len(subset_dataset_obj)
            except TypeError:
                logger.warning("Dataset object claims no length.")

        compute_features_step = "features" in run_steps

        subset_features_dir.mkdir(parents=True, exist_ok=True)

        configs_to_process = self.all_feature_configs

        logger.info("--- Feature Pass 1: Base Features ---")
        for feature_config in configs_to_process:
            feature_name = feature_config.get("name")
            if not feature_name or feature_config.get("base_extractor"):
                continue

            effective_config = self._get_effective_feature_config(feature_config, current_subset_key)
            path_to_use, cached_metadata = self._check_cache("features", subset_features_dir, dataset_cache_id, effective_config, total_items_in_subset, check_attr="num_items_processed")

            if path_to_use and cached_metadata:
                available_base_paths[feature_name] = path_to_use
                self.processed_feature_metadata[feature_name] = cached_metadata
                if effective_config.get("benchmark_this", True) and not effective_config.get("averaging") and not effective_config.get("pca"):
                    final_feature_paths[feature_name] = path_to_use
            elif compute_features_step:
                logger.info("Base cache MISS for '%s'. Computing...", feature_name)
                expected_save_path = get_cache_path(subset_features_dir, "features", dataset_cache_id, effective_config)
                extractor = self._get_feature_extractor(effective_config)
                if extractor:
                    computed_metadata = self._extract_and_save_features_hdf5(
                        extractor=extractor,
                        dataset=subset_dataset_obj,
                        cache_path=expected_save_path,
                        feature_config=effective_config,
                        item_id_map=item_id_map,
                        batch_size=self.cfg.get("extraction_batch_size", 32),
                    )
                    if feature_name in self._extractor_instances_cache:
                        del self._extractor_instances_cache[feature_name]
                    del extractor
                    gc.collect()

                    if computed_metadata:
                        available_base_paths[feature_name] = expected_save_path
                        self.processed_feature_metadata[feature_name] = computed_metadata
                        if effective_config.get("benchmark_this", True) and not effective_config.get("averaging") and not effective_config.get("pca"):
                            final_feature_paths[feature_name] = expected_save_path
                        logger.info("Base features computed for '%s'.", feature_name)
                    else:
                        logger.error("Base computation FAILED for '%s'.", feature_name)
                else:
                    logger.error("Could not get extractor for base feature '%s'.", feature_name)
            else:
                logger.info("Base cache MISS for '%s' and 'features' step skipped.", feature_name)

        logger.info("--- Feature Pass 2: Intermediate Features ---")
        for feature_config in configs_to_process:
            feature_name = feature_config.get("name")
            base_feature_name = feature_config.get("base_extractor")
            needs_intermediate = feature_config.get("averaging") is not None
            if not base_feature_name or not needs_intermediate:
                continue

            effective_config = self._get_effective_feature_config(feature_config, current_subset_key)
            
            # For intermediate, we expect the count to match the base feature's count.
            base_meta = self.processed_feature_metadata.get(base_feature_name)
            expected_count = int(base_meta.get("num_items_processed", -1)) if base_meta else -1
            
            path_to_use, cached_metadata = self._check_cache("intermediate", subset_features_dir, dataset_cache_id, effective_config, expected_count, check_attr="num_items")

            if path_to_use and cached_metadata:
                available_intermediate_paths[feature_name] = path_to_use
                self.processed_feature_metadata[feature_name] = cached_metadata
                if effective_config.get("benchmark_this", True) and not effective_config.get("pca"):
                    final_feature_paths[feature_name] = path_to_use
            elif compute_features_step:
                base_feature_path = available_base_paths.get(base_feature_name)
                if not base_feature_path:
                    logger.error("Cannot compute intermediate '%s': Base feature '%s' not available.", feature_name, base_feature_name)
                    continue
                logger.info("Intermediate cache MISS for '%s'. Computing...", feature_name)
                expected_inter_save_path = get_cache_path(subset_features_dir, "intermediate", dataset_cache_id, effective_config)
                inter_meta = self._process_intermediate_features(base_feature_path=base_feature_path, intermediate_feature_path=expected_inter_save_path, feature_config=effective_config, chunk_size=self.cfg.get("extraction_batch_size", 1024))
                if inter_meta:
                    available_intermediate_paths[feature_name] = expected_inter_save_path
                    self.processed_feature_metadata[feature_name] = inter_meta
                    if effective_config.get("benchmark_this", True) and not effective_config.get("pca"):
                        final_feature_paths[feature_name] = expected_inter_save_path
                    logger.info("Intermediate features computed for '%s'.", feature_name)
                else:
                    logger.error("Intermediate computation FAILED for '%s'.", feature_name)
            else:
                logger.info("Intermediate cache MISS for '%s' and 'features' step skipped.", feature_name)

        logger.info("--- Feature Pass 3: PCA Features ---")
        for feature_config in configs_to_process:
            feature_name = feature_config.get("name")
            base_feature_name = feature_config.get("base_extractor")
            needs_pca = feature_config.get("pca") is not None
            if not base_feature_name or not needs_pca:
                continue

            effective_config = self._get_effective_feature_config(feature_config, current_subset_key)
            path_to_use, cached_metadata = self._check_cache("features", subset_features_dir, dataset_cache_id, effective_config, -1, check_attr="num_items") # Count check is tricky here

            if path_to_use and cached_metadata:
                self.processed_feature_metadata[feature_name] = cached_metadata
                if effective_config.get("benchmark_this", True):
                    final_feature_paths[feature_name] = path_to_use
            elif compute_features_step:
                # Determine the name of the feature that serves as input to PCA
                pca_input_feature_name = base_feature_name
                
                pca_input_path = available_intermediate_paths.get(pca_input_feature_name)
                if not pca_input_path:
                    pca_input_path = available_base_paths.get(pca_input_feature_name)

                if not pca_input_path:
                    logger.error("Cannot compute PCA '%s': Input feature '%s' (base or intermediate) not available.", feature_name, pca_input_feature_name)
                    continue

                logger.info("PCA cache MISS for '%s'. Computing...", feature_name)
                expected_pca_save_path = get_cache_path(subset_features_dir, "features", dataset_cache_id, effective_config)
                pca_meta = self._process_pca_features(
                    intermediate_feature_path=pca_input_path,
                    pca_feature_path=expected_pca_save_path,
                    feature_config=effective_config,
                    dataset_cache_id=dataset_cache_id,
                    subset_features_dir=subset_features_dir,
                    chunk_size=self.cfg.get("extraction_batch_size", 1024),
                )
                if pca_meta:
                    self.processed_feature_metadata[feature_name] = pca_meta
                    if effective_config.get("benchmark_this", True):
                        final_feature_paths[feature_name] = expected_pca_save_path
                    logger.info("PCA features computed for '%s'.", feature_name)
                else:
                    logger.error("PCA computation FAILED for '%s'.", feature_name)
            else:
                logger.info("PCA cache MISS for '%s' and 'features' step skipped.", feature_name)

        self.pca_model_cache.clear()
        self._clear_extractor_cache()
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
        logger.info("--- Feature Processing Completed for Subset: %s ---", current_subset_key)
        return final_feature_pathsimport functools
import gc
import importlib
import logging
import sys
import time
from pathlib import Path
from typing import (Any, Callable, Dict, List, Optional, Tuple, Type, Union)

import torch
from torch.utils.data import DataLoader, IterableDataset

from trainers.base import Trainer
from utils.logging_utils import setup_logging
from utils.torch_utils import get_device

from reproducibility.models.autoencoder import Autoencoder, AudioConfig
from reproducibility.trainers.autoencoder import ae_collate_fn
from reproducibility.models.vae import VariationalAutoencoder
from reproducibility.trainers.vae import vae_collate_fn as top_level_vae_collate_fn
from vocsim.managers.dataset_manager import DatasetManager


logger = logging.getLogger(__name__)


class TrainerManager:
    """Handles trainer and model instantiation, dataloader creation, and training execution."""

    def __init__(self, config: Dict[str, Any], models_dir: Path, device: torch.device):
        """
        Initializes the TrainerManager.

        Args:
            config (Dict[str, Any]): The configuration dictionary.
            models_dir (Path): The base directory for saving trained models.
            device (torch.device): The device to use for training.
        """
        self.cfg = config
        self.models_dir = models_dir
        self.device = device
        self.training_jobs = config.get("train", [])
        self.trained_model_paths: Dict[str, Path] = {}

    def _get_class_from_module(self, module_name: str, class_name: str) -> Type:
        """
        Dynamically imports a class from a module path.

        Args:
            module_name (str): The dotted module path.
            class_name (str): The name of the class within the module.

        Returns:
            Type: The imported class object.

        Raises:
            ImportError: If the module or class cannot be found/imported.
        """
        try:
            module = importlib.import_module(module_name)
            return getattr(module, class_name)
        except Exception as e:
            logger.error("Failed import: %s from %s.", class_name, module_name, exc_info=True)
            raise ImportError(f"Failed import {class_name} from {module_name}.") from e

    def _instantiate_model(self, model_config: Dict[str, Any]) -> torch.nn.Module:
        """
        Instantiates the torch.nn.Module based on model configuration.

        Args:
            model_config (Dict[str, Any]): Configuration for the model.

        Returns:
            torch.nn.Module: An instance of the model.

        Raises:
            ValueError: If model config is invalid or instantiation fails.
        """
        model_name = model_config.get("name")
        model_module_path = model_config.get("module")
        model_params = model_config.get("params", {})
        if not model_name or not model_module_path:
            raise ValueError("Model config requires 'name' and 'module'.")
        logger.info("Instantiating model: %s", model_name)
        ModelClass = self._get_class_from_module(model_module_path, model_name)
        instance = None
        if ModelClass is Autoencoder:
            if AudioConfig is None:
                raise ImportError("AudioConfig not available for Autoencoder.")
            ae_audio_cfg_dict = model_params.get("audio_config")
            ae_dims = model_params.get("dimensions")
            ae_bneck = model_params.get("bottleneck_dim")
            if not all([ae_audio_cfg_dict, ae_dims, ae_bneck is not None]):
                raise ValueError("Missing 'audio_config', 'dimensions', or 'bottleneck_dim' for Autoencoder model params.")
            try:
                ae_audio_cfg = AudioConfig(**ae_audio_cfg_dict)
                instance = Autoencoder(config=ae_audio_cfg, max_spec_width=ae_dims["max_spec_width"], bottleneck_dim=ae_bneck)
            except Exception as ae_init_err:
                logger.error("Error initializing Autoencoder: %s", ae_init_err, exc_info=True)
                raise
        elif ModelClass is VariationalAutoencoder:
            vae_z_dim = model_params.get("z_dim")
            vae_precision = model_params.get("model_precision", 10.0)
            vae_lr = model_params.get("learning_rate", 1e-3)
            if vae_z_dim is None:
                raise ValueError("Missing 'z_dim' for VariationalAutoencoder model params.")
            try:
                instance = VariationalAutoencoder(z_dim=vae_z_dim, model_precision=vae_precision, device_name=self.device.type, lr=vae_lr)
            except Exception as vae_init_err:
                logger.error("Error initializing VariationalAutoencoder: %s", vae_init_err, exc_info=True)
                raise
        else:
            try:
                instance = ModelClass(**model_params)
            except Exception as gen_init_err:
                logger.error("Error initializing model %s: %s", model_name, gen_init_err, exc_info=True)
                raise
        if instance is None:
            raise RuntimeError(f"Model instantiation failed for {model_name}")
        logger.info("Model '%s' instantiated successfully.", model_name)
        return instance

    def _get_trainer(self, trainer_config: Dict[str, Any], model: torch.nn.Module, training_scope: str) -> Trainer:
        """
        Instantiates a Trainer, adjusting the output directory based on the training scope.

        Args:
            trainer_config (Dict[str, Any]): Configuration for the trainer.
            model (torch.nn.Module): The model instance to train.
            training_scope (str): The scope of training (e.g., dataset subset name).

        Returns:
            Trainer: An instance of the Trainer.

        Raises:
            ValueError: If trainer config is invalid.
            ImportError: If trainer class cannot be instantiated.
        """
        trainer_name = trainer_config.get("name")
        trainer_module_path = trainer_config.get("module")
        trainer_params = trainer_config.get("params", {})
        if not trainer_name or not trainer_module_path:
            raise ValueError("Trainer config requires 'name' and 'module'.")
        logger.info("Instantiating trainer: %s (Scope: %s)", trainer_name, training_scope)
        TrainerClass = self._get_class_from_module(trainer_module_path, trainer_name)
        scoped_trainer_name = f"{trainer_name}_{training_scope}"
        trainer_output_dir = self.models_dir / scoped_trainer_name
        instance = TrainerClass(model=model, config=trainer_params, output_dir=trainer_output_dir, device=self.device.type)
        logger.info("Trainer '%s' instantiated. Output: %s", scoped_trainer_name, trainer_output_dir)
        return instance

    def _get_collate_fn(self, trainer: Trainer) -> Optional[Callable]:
        """
        Determines the appropriate collate_fn based on the trainer type.

        Args:
            trainer (Trainer): The trainer instance.

        Returns:
            Optional[Callable]: The collate function, or None if a default collate should be used or an error occurs.
        """
        collate_fn = None
        trainer_class_name = trainer.__class__.__name__
        if trainer_class_name == "PaperVAETrainer":
            logger.info("Using VAE-specific collate function.")
            if top_level_vae_collate_fn is None:
                logger.error("VAE collate function not imported.")
                return None
            try:
                if not all(hasattr(trainer, attr) for attr in ["target_sr", "n_fft", "hop_length", "win_length", "window_fn_str", "spec_height", "spec_width", "window_samples", "hop_samples"]):
                    logger.error("Trainer '%s' missing frontend parameters for collate_fn.", trainer_class_name)
                    return None
                collate_fn = functools.partial(top_level_vae_collate_fn, target_sr=trainer.target_sr, n_fft=trainer.n_fft, hop_length=trainer.hop_length, win_length=trainer.win_length, window_fn_str=trainer.window_fn_str, spec_height=trainer.spec_height, spec_width=trainer.spec_width, window_samples=trainer.window_samples, hop_samples=trainer.hop_samples)
            except AttributeError as e:
                logger.error("VAE trainer missing param for collate: %s", e, exc_info=True)
                return None
        elif trainer_class_name == "PaperAutoencoderTrainer":
            logger.info("Using AE-specific collate function (padding).")
            if ae_collate_fn is None:
                logger.error("AE collate function not imported.")
                return None
            collate_fn = ae_collate_fn
        else:
            logger.debug("Using default DataLoader collate for %s.", trainer_class_name)
        return collate_fn

    def run_training_job(self, job_config: Dict[str, Any], dataset: Any, training_scope: str):
        """
        Instantiates model/trainer, creates dataloaders, runs training for a single job.

        Args:
            job_config (Dict[str, Any]): Configuration for the training job.
            dataset (Any): The dataset object for training.
            training_scope (str): The scope of training (e.g., dataset subset name).
        """
        trainer_config = job_config.get("trainer")
        model_config = job_config.get("model")
        if not trainer_config:
            logger.error("Missing 'trainer' section. Skip.")
            return
        if not model_config:
            model_config = trainer_config.get("model")
            logger.warning("Using nested 'model' config. Prefer top-level.")
        if not model_config:
            logger.error("Missing 'model' section. Skip.")
            return

        base_trainer_name = trainer_config.get("name", "unknown_trainer")
        scoped_trainer_name = f"{base_trainer_name}_{training_scope}"
        logger.info("--- Starting Training Job: %s ---", scoped_trainer_name)
        start_time = time.time()
        model, trainer, train_loader, val_loader = None, None, None, None
        try:
            model = self._instantiate_model(model_config)
            trainer = self._get_trainer(trainer_config, model, training_scope)
            batch_size = job_config.get("batch_size", 64)
            num_workers = 0 if sys.platform == "win32" else job_config.get("num_workers", 0)
            pin_memory = job_config.get("pin_memory", True) and self.device.type == "cuda"
            train_dataset = dataset
            val_dataset = None
            collate_fn = self._get_collate_fn(trainer)
            if collate_fn is None and trainer.__class__.__name__ in ["PaperVAETrainer", "PaperAutoencoderTrainer"]:
                logger.error("Missing collate for %s. Abort.", trainer.__class__.__name__)
                return
            is_iterable = isinstance(train_dataset, IterableDataset)
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=not is_iterable, num_workers=num_workers, pin_memory=pin_memory, collate_fn=collate_fn, drop_last=True)
            val_loader = None
            logger.info("DataLoaders created (Batch: %d, Workers: %d).", batch_size, num_workers)
            trainer.train(train_loader, val_loader)
            model_save_path = trainer.output_dir / "final_model.pt"
            if model_save_path.exists():
                self.trained_model_paths[scoped_trainer_name] = model_save_path
                logger.info("Stored path for '%s': %s", scoped_trainer_name, model_save_path)
            else:
                logger.warning("Final model not found for '%s' in %s", scoped_trainer_name, trainer.output_dir)
        except (ImportError, ValueError, FileNotFoundError) as job_err:
            logger.error("Config/Init error job '%s': %s", scoped_trainer_name, job_err, exc_info=True)
        except Exception as e:
            logger.error("Runtime error job '%s': %s", scoped_trainer_name, e, exc_info=True)
        finally:
            del model, trainer, train_loader, val_loader
            gc.collect()
            if self.device.type == "cuda":
                torch.cuda.empty_cache()
            elapsed = time.time() - start_time
            logger.info("Job '%s' finished in %.2fs.", scoped_trainer_name, elapsed)

    def run_all_training_jobs(self, dataset_manager: DatasetManager):
        """
        Iterates through all training jobs configured and executes them.

        Args:
            dataset_manager (DatasetManager): The dataset manager instance.
        """
        if not self.training_jobs:
            logger.info("No training jobs defined.")
            return
        logger.info("Found %d training job(s).", len(self.training_jobs))
        for i, job_config in enumerate(self.training_jobs):
            trainer_name_cfg = job_config.get("trainer", {}).get("name", f"Job_{i+1}")
            training_scope = job_config.get("train_on_subset", "all")
            logger.info("--- Processing Training Job %d/%d (%s, Scope: %s) ---", i + 1, len(self.training_jobs), trainer_name_cfg, training_scope)
            target_dataset_obj = None
            if training_scope == "all":
                target_dataset_obj = dataset_manager.full_dataset_obj
                if target_dataset_obj is None:
                    logger.error("Full dataset not loaded. Skip job %s.", trainer_name_cfg)
                    continue
                logger.info("Job targets full dataset.")
            else:
                subset_info = dataset_manager.get_subset_dataset(training_scope)
                if subset_info:
                    target_dataset_obj, _ = subset_info
                    logger.info("Job targets subset: '%s'", training_scope)
                else:
                    logger.error("Subset '%s' not found/loaded. Skip job %s.", training_scope, trainer_name_cfg)
                    continue
            self.run_training_job(job_config, target_dataset_obj, training_scope)
        logger.info("--- All Configured Training Jobs Processed ---")

    def get_trained_model_paths(self) -> Dict[str, Path]:
        """
        Returns a dictionary mapping SCOPED trainer names to their final saved model paths.

        Returns:
            Dict[str, Path]: Mapping of scoped trainer names to file paths.
        """
        return self.trained_model_pathsfrom .trainer_manager import TrainerManager
from .dataset_manager import DatasetManager
from .feature_manager import FeatureManager
from .distance_manager import DistanceManager
from .benchmark_manager import BenchmarkManager

__all__ = [
    "DatasetManager",
    "FeatureManager",
    "DistanceManager",
    "BenchmarkManager",
    "TrainerManager"
]import datetime
import gc
import logging
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple, Optional, Callable
import os


import torch
import pandas as pd
import numpy as np
import json


from vocsim.managers import (
    BenchmarkManager,
    DatasetManager,
    DistanceManager,
    FeatureManager,
    TrainerManager,
)

from utils.config_loader import load_config
from utils.file_utils import get_cache_path, find_cache_path, save_results, _get_safe_path_part
from utils.logging_utils import setup_logging
from utils.torch_utils import get_device

logger = logging.getLogger(__name__)


class NpEncoder(json.JSONEncoder):
    """Helper JSON Encoder class for NumPy types, Path, Tensors, etc."""

    def default(self, obj):
        """
        Encodes various object types into JSON-serializable formats.

        Args:
            obj: The object to encode.

        Returns:
            JSON-serializable representation of the object.
        """
        if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
            if np.isnan(obj):
                return None
            if np.isinf(obj):
                return None
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return [self.default(el) for el in obj.tolist()]
        elif isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, torch.Tensor):
            return self.default(obj.detach().cpu().numpy())
        elif isinstance(obj, (datetime.date, datetime.datetime)):
            return obj.isoformat()
        try:
            return json.JSONEncoder.default(self, obj)
        except TypeError:
            logger.warning("NpEncoder encountered unhandled type %s, converting to string.", type(obj))
            return str(obj)


class PipelineRunner:
    """
    Manages the execution of the VocSim benchmark pipeline based on specified steps.
    """

    def __init__(self, config: Dict[str, Any]):
        """
        Initializes the PipelineRunner with configuration and manager instances.

        Args:
            config: Configuration dictionary for the pipeline.
        """
        self.cfg = config
        project_root_str = config.get("project_root", ".")
        self.root_dir = Path(project_root_str).resolve()

        cfg_results_dir = config.get("results_dir")
        self.base_results_dir = Path(cfg_results_dir).resolve() if cfg_results_dir and Path(cfg_results_dir).is_absolute() else (self.root_dir / (cfg_results_dir or "results")).resolve()

        cfg_features_dir = config.get("features_dir")
        self.base_features_dir = Path(cfg_features_dir).resolve() if cfg_features_dir and Path(cfg_features_dir).is_absolute() else (self.root_dir / (cfg_features_dir or "features_cache")).resolve()

        cfg_models_dir = config.get("models_dir")
        self.base_models_dir = Path(cfg_models_dir).resolve() if cfg_models_dir and Path(cfg_models_dir).is_absolute() else (self.root_dir / (cfg_models_dir or "models")).resolve()

        self.device = get_device(config.get("force_cpu", False))
        self._setup_base_directories()

        self.dataset_manager = DatasetManager(config)
        self.feature_manager = FeatureManager(config, self.base_features_dir, self.device, self.base_models_dir)
        self.distance_manager = DistanceManager(config, self.base_features_dir, self.device)
        self.benchmark_manager = BenchmarkManager(config, self.device, self.root_dir)
        self.trainer_manager = TrainerManager(config, self.base_models_dir, self.device)
        logger.info("PipelineRunner initialized.")

    def _setup_base_directories(self) -> None:
        """
        Creates base directories for results, features, and models if they do not exist.
        """
        self.base_results_dir.mkdir(parents=True, exist_ok=True)
        self.base_features_dir.mkdir(parents=True, exist_ok=True)
        self.base_models_dir.mkdir(parents=True, exist_ok=True)
        logger.info("Results directory: %s", self.base_results_dir)
        logger.info("Features cache directory: %s", self.base_features_dir)
        logger.info("Models directory: %s", self.base_models_dir)

    def _run_subset_loop(self, steps: List[str]) -> None:
        """
        Internal method to loop through dataset subsets and run selected evaluation steps.

        Args:
            steps (List[str]): The list of pipeline steps requested.
        """
        overall_results: Dict[str, Dict[str, Any]] = {}
        run_id_template = self.cfg.get("run_id", "run_${now:%Y%m%d_%H%M%S}")
        run_id = datetime.datetime.now().strftime(run_id_template.replace("${now:%Y%m%d_%H%M%S}", "%Y%m%d_%H%M%S"))
        logger.info("Using Run ID: %s", run_id)

        needs_eval = any(step in steps for step in ["features", "distances", "benchmarks"])
        if needs_eval and not self.dataset_manager.full_dataset_obj:
            if not self.dataset_manager.load_full_dataset():
                logger.error("Failed to load dataset for evaluation steps.")
                return
            logger.info("Full dataset loaded for evaluation steps.")

        subsets_to_run_config = self.cfg.get("dataset", {}).get("subsets_to_run")
        if subsets_to_run_config is None:
            top_level_subset = self.cfg.get("dataset", {}).get("subset")
            subsets_to_run = [top_level_subset] if top_level_subset else ["all"]
        elif isinstance(subsets_to_run_config, str):
            subsets_to_run = [subsets_to_run_config]
        elif isinstance(subsets_to_run_config, list):
            subsets_to_run = subsets_to_run_config
        else:
            logger.warning("Invalid 'subsets_to_run' type: %s. Defaulting to ['all'].", type(subsets_to_run_config))
            subsets_to_run = ["all"]

        log_subset_msg = "'all' (full dataset)" if subsets_to_run == ["all"] else f"subsets: {subsets_to_run}"
        logger.info("Processing %s for steps: %s", log_subset_msg, steps)

        for subset_key in subsets_to_run:
            start_time = time.time()
            logger.info("\n===== Processing Subset: %s =====", subset_key)

            subset_features_dir = self.base_features_dir / _get_safe_path_part(subset_key)
            subset_bench_cache_dir = subset_features_dir / "benchmark_cache"
            subset_results_dir = self.base_results_dir / _get_safe_path_part(subset_key)
            for directory in (subset_features_dir, subset_bench_cache_dir, subset_results_dir):
                directory.mkdir(parents=True, exist_ok=True)
            logger.debug("Dirs - Feat: %s, BenchCache: %s, Results: %s", subset_features_dir, subset_bench_cache_dir, subset_results_dir)

            subset_dataset: Optional[Any] = None
            dataset_cache_id: Optional[str] = None
            item_id_map: Dict[str, Dict] = {}
            if needs_eval:
                subset_info = self.dataset_manager.get_subset_dataset(subset_key)
                if subset_info is None:
                    logger.warning("Skipping subset '%s' due to dataset loading/filtering failure.", subset_key)
                    continue
                subset_dataset, dataset_cache_id = subset_info
                item_id_map = self.dataset_manager.get_current_item_map()
                if not item_id_map:
                    logger.warning("Empty item ID map for subset '%s'. Benchmarks requiring labels might fail.", subset_key)
            else:
                dataset_id_for_filename = _get_safe_path_part(Path(self.cfg.get("dataset", {}).get("id", "unknown_dataset")).name)
                split_name = self.cfg.get("dataset", {}).get("split", "train")
                dataset_cache_id = f"{dataset_id_for_filename}_{_get_safe_path_part(subset_key)}_{split_name}"
                logger.info("No evaluation steps requested. Constructed approximate dataset_cache_id: %s", dataset_cache_id)

            feature_paths: Dict[str, Path] = {}
            distance_paths: Dict[Tuple[str, str], Path] = {}
            subset_results: Dict[str, Any] = {}

            try:
                if "features" in steps:
                    logger.info("--- Running Feature Processing for Subset: %s ---", subset_key)
                    feature_paths = self.feature_manager.process_subset_features(
                        subset_dataset_obj=subset_dataset,
                        dataset_cache_id=dataset_cache_id,
                        current_subset_key=subset_key,
                        subset_features_dir=subset_features_dir,
                        item_id_map=item_id_map,
                        run_steps=steps,
                    )
                    if not feature_paths:
                        logger.warning("Feature processing yielded no valid paths for subset '%s'. Subsequent steps may fail if cache is missing.", subset_key)
                elif any(step in steps for step in ["distances", "benchmarks"]):
                    logger.info("--- Locating Existing Feature Paths Only for Subset: %s ---", subset_key)
                    for feat_conf in self.cfg.get("feature_extractors", []):
                        feat_name = feat_conf.get("name")
                        if not feat_name or not feat_conf.get("benchmark_this", True):
                            continue
                        expected_path = self.feature_manager.get_feature_file_path(feat_name, dataset_cache_id, subset_features_dir)
                        if expected_path and expected_path.is_file():
                            feature_paths[feat_name] = expected_path
                        else:
                            eff_conf = self.feature_manager._get_effective_feature_config(feat_conf, subset_key)
                            path_to_use = find_cache_path(
                                cache_dir=subset_features_dir, prefix="features", dataset_cache_id=dataset_cache_id, config_dict=eff_conf, extra_suffix=None
                            )
                            if path_to_use and path_to_use.is_file():
                                logger.debug("Found feature '%s' using find_cache_path: %s", feat_name, path_to_use.name)
                                feature_paths[feat_name] = path_to_use
                            else:
                                logger.debug("No cached feature file found (using find_cache_path or expected path) for '%s' when feature step was skipped.", feat_name)
                    if not feature_paths:
                        logger.warning("Feature step skipped and no cached feature files found. Subsequent steps will likely fail.")

                if "distances" in steps:
                    if not feature_paths:
                        logger.warning("Skipping distances step for subset '%s' as no feature paths are available.", subset_key)
                    elif not self.cfg.get("distances"):
                        logger.info("No distance metrics configured, skipping distance computation for '%s'.", subset_key)
                    else:
                        logger.info("--- Running Distance Computation for Subset: %s ---", subset_key)
                        distance_paths = self.distance_manager.process_subset_distances(
                            dataset_cache_id=dataset_cache_id,
                            current_subset_key=subset_key,
                            feature_manager=self.feature_manager,
                            subset_features_dir=subset_features_dir,
                            item_id_map=item_id_map,
                            feature_paths=feature_paths,
                            run_steps=steps,
                        )
                        if not distance_paths:
                            logger.warning("Distance processing yielded no valid paths for subset '%s'. Subsequent steps may fail if cache is missing.", subset_key)
                elif "benchmarks" in steps and self.cfg.get("distances"):
                    logger.info("--- Locating Existing Distance Paths Only for Subset: %s ---", subset_key)
                    for f_name in feature_paths.keys():
                        raw_feat_conf = self.feature_manager.all_feature_configs_map.get(f_name)
                        if not raw_feat_conf:
                            continue
                        eff_feat_conf = self.feature_manager._get_effective_feature_config(raw_feat_conf, subset_key)
                        allowed_dists = eff_feat_conf.get("compute_distances_for")

                        for dist_conf in self.cfg.get("distances", []):
                            d_name = dist_conf.get("name")
                            if not d_name or (allowed_dists is not None and d_name not in allowed_dists):
                                continue
                            combined_config_for_path_check = {"feature_config": eff_feat_conf, "distance_config": dist_conf, "name": f"{f_name}_{d_name}"}
                            expected_path = get_cache_path(
                                cache_dir=subset_features_dir, prefix=f"distances_{d_name}", dataset_cache_id=dataset_cache_id, config_dict=combined_config_for_path_check, extra_suffix=None
                            )
                            if expected_path and expected_path.is_file():
                                distance_paths[(f_name, d_name)] = expected_path
                            else:
                                combined_config = {"feature_config": eff_feat_conf, "distance_config": dist_conf, "name": f"{f_name}_{d_name}"}
                                found_dist_path = find_cache_path(
                                    cache_dir=subset_features_dir, prefix=f"distances_{d_name}", dataset_cache_id=dataset_cache_id, config_dict=combined_config
                                )
                                if found_dist_path and found_dist_path.is_file():
                                    logger.debug("Found distance '%s/%s' using find_cache_path: %s", f_name, d_name, found_dist_path.name)
                                    distance_paths[(f_name, d_name)] = found_dist_path
                                else:
                                    logger.debug("No cached distance file found (using find_cache_path or expected path) for '%s/%s' when distance step was skipped.", f_name, d_name)

                if "benchmarks" in steps:
                    if not self.cfg.get("benchmarks"):
                        logger.info("No benchmarks configured, skipping benchmark run for '%s'.", subset_key)
                    elif not feature_paths:
                        logger.warning("Skipping benchmarks step for subset '%s' as no feature paths are available (step skipped or compute failed).", subset_key)
                    else:
                        logger.info("--- Running Benchmarks for Subset: %s ---", subset_key)
                        subset_results = self.benchmark_manager.run_subset_benchmarks(
                            subset_features_dir=subset_features_dir,
                            subset_dataset_obj=subset_dataset,
                            item_id_map=item_id_map,
                            dataset_cache_id=dataset_cache_id,
                            current_subset_key=subset_key,
                            subset_cache_dir=subset_bench_cache_dir,
                            feature_paths=feature_paths,
                            distance_paths=distance_paths,
                        )
                        overall_results[subset_key] = subset_results

                        if subset_results:
                            dataset_id_for_filename = _get_safe_path_part(Path(self.cfg.get("dataset", {}).get("id", "unknown_dataset")).name)
                            filename_prefix = f"{_get_safe_path_part(subset_key)}_{dataset_id_for_filename}_{run_id}_results"
                            save_results(subset_results, subset_results_dir, filename_prefix)
                else:
                    logger.info("Skipping benchmark step for subset '%s'.", subset_key)

            except Exception as e:
                logger.error("Error processing subset '%s': %s", subset_key, e, exc_info=True)
            finally:
                if subset_dataset is not None:
                    del subset_dataset
                if "item_id_map" in locals():
                    del item_id_map
                if "feature_paths" in locals():
                    del feature_paths
                if "distance_paths" in locals():
                    del distance_paths
                if "subset_results" in locals():
                    del subset_results
                gc.collect()
                if torch.cuda.is_available():
                    try:
                        torch.cuda.empty_cache()
                    except Exception as cuda_err:
                        logger.warning("CUDA cache clear failed: %s", cuda_err)
                logger.info("Memory cleared after processing subset '%s'.", subset_key)

            elapsed = time.time() - start_time
            logger.info("Subset '%s' completed in %.2fs.", subset_key, elapsed)
            logger.info("===== Subset %s Complete =====", subset_key)

        if len(subsets_to_run) > 1 and overall_results:
            logger.info("Saving combined results for all processed subsets.")
            dataset_id_for_filename = _get_safe_path_part(Path(self.cfg.get("dataset", {}).get("id", "unknown_dataset")).name)
            combined_prefix = f"COMBINED_{dataset_id_for_filename}_{run_id}"
            save_results(overall_results, self.base_results_dir, combined_prefix)
        elif overall_results:
            logger.info("Single subset processed. Results saved under subset name (see above).")
        else:
            logger.info("No overall benchmark results generated.")

    def run(self, steps: List[str]) -> None:
        """
        Executes the specified pipeline steps based on the provided list.

        Args:
            steps: List of steps to run (e.g., ["train", "features", "distances", "benchmarks"]).
                   Steps are executed in a fixed logical order if present.
        """
        logger.info("--- Executing Pipeline Stages: %s ---", steps)

        if "train" in steps:
            if self.cfg.get("train"):
                logger.info("--- Stage: Training ---")
                if not self.dataset_manager.full_dataset_obj:
                    if not self.dataset_manager.load_full_dataset():
                        logger.error("Failed to load dataset for training stage. Aborting.")
                        return

                if not self.trainer_manager.run_all_training_jobs(self.dataset_manager):
                    logger.error("Training stage encountered errors.")
                else:
                    logger.info("--- Training Stage Complete ---")
            else:
                logger.info("Skipping training stage: No 'train' jobs defined in config.")
        else:
            logger.info("Skipping training stage (not requested).")

        evaluation_steps_requested = [s for s in steps if s in ["features", "distances", "benchmarks"]]
        if evaluation_steps_requested:
            logger.info("--- Starting Evaluation Stages: %s ---", evaluation_steps_requested)
            self._run_subset_loop(steps)
            logger.info("--- Evaluation Stages Complete ---")
        else:
            logger.info("Skipping all evaluation stages (features, distances, benchmarks) as none were requested.")

        logger.info("--- Pipeline Runner Finished ---")"""
VocSim Benchmark: Core API and Orchestration Module.
This package contains the central runner and logic for executing
benchmarks within the VocSim framework.
"""
__version__ = '0.0.1'

from .runner import PipelineRunner

__all__ = [
    "PipelineRunner",
]